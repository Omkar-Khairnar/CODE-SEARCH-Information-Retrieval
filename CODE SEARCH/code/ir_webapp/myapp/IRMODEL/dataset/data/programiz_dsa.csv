Question Title,Question Description,Answer Code
"Stack Data Structure and Implementation in Python, Java and C/C++","A stack is a linear data structure that follows the principle of Last In First Out (LIFO). This means the last element inserted inside the stack is removed first. You can think of the stack data structure as the pile of plates on top of another. Here, you can: Put a new plate on top
	Remove the top plate Put a new plate on top Remove the top plate And, if you want the plate at the bottom, you must first remove all the plates on top. This is exactly how the stack data structure works. In programming terms, putting an item on top of the stack is called push and removing an item is called pop. In the above image, although item 3 was kept last, it was removed first. This is exactly how the LIFO (Last In First Out) Principle works. We can implement a stack in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same.  There are some basic operations that allow us to perform different actions on a stack. Push: Add an element to the top of a stack
	Pop: Remove an element from the top of a stack
	IsEmpty: Check if the stack is empty
	IsFull: Check if the stack is full
	Peek: Get the value of the top element without removing it Push: Add an element to the top of a stack Pop: Remove an element from the top of a stack IsEmpty: Check if the stack is empty IsFull: Check if the stack is full Peek: Get the value of the top element without removing it The operations work as follows: A pointer called TOP is used to keep track of the top element in the stack. When initializing the stack, we set its value to -1 so that we can check if the stack is empty by comparing TOP == -1. On pushing an element, we increase the value of TOP and place the new element in the position pointed to by TOP. On popping an element, we return the element pointed to by TOP and reduce its value. Before pushing, we check if the stack is already full Before popping, we check if the stack is already empty The most common stack implementation is using arrays, but it can also be implemented using lists. For the array-based implementation of a stack, the push and pop operations take constant time, i.e. O(1). Although stack is a simple data structure to implement, it is very powerful. The most common uses of a stack are: To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order.
	In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form.
	In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed. To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order. In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form. In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed.","# Stack implementation in python


# Creating a stack
def create_stack():
    stack = []
    return stack


# Creating an empty stack
def check_empty(stack):
    return len(stack) == 0


# Adding items into the stack
def push(stack, item):
    stack.append(item)
    print(""pushed item: "" + item)


# Removing an element from the stack
def pop(stack):
    if (check_empty(stack)):
        return ""stack is empty""

    return stack.pop()


stack = create_stack()
push(stack, str(1))
push(stack, str(2))
push(stack, str(3))
push(stack, str(4))
print(""popped item: "" + pop(stack))
print(""stack after popping an element: "" + str(stack))
"
"Stack Data Structure and Implementation in Python, Java and C/C++","A stack is a linear data structure that follows the principle of Last In First Out (LIFO). This means the last element inserted inside the stack is removed first. You can think of the stack data structure as the pile of plates on top of another. Here, you can: Put a new plate on top
	Remove the top plate Put a new plate on top Remove the top plate And, if you want the plate at the bottom, you must first remove all the plates on top. This is exactly how the stack data structure works. In programming terms, putting an item on top of the stack is called push and removing an item is called pop. In the above image, although item 3 was kept last, it was removed first. This is exactly how the LIFO (Last In First Out) Principle works. We can implement a stack in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same.  There are some basic operations that allow us to perform different actions on a stack. Push: Add an element to the top of a stack
	Pop: Remove an element from the top of a stack
	IsEmpty: Check if the stack is empty
	IsFull: Check if the stack is full
	Peek: Get the value of the top element without removing it Push: Add an element to the top of a stack Pop: Remove an element from the top of a stack IsEmpty: Check if the stack is empty IsFull: Check if the stack is full Peek: Get the value of the top element without removing it The operations work as follows: A pointer called TOP is used to keep track of the top element in the stack. When initializing the stack, we set its value to -1 so that we can check if the stack is empty by comparing TOP == -1. On pushing an element, we increase the value of TOP and place the new element in the position pointed to by TOP. On popping an element, we return the element pointed to by TOP and reduce its value. Before pushing, we check if the stack is already full Before popping, we check if the stack is already empty The most common stack implementation is using arrays, but it can also be implemented using lists. For the array-based implementation of a stack, the push and pop operations take constant time, i.e. O(1). Although stack is a simple data structure to implement, it is very powerful. The most common uses of a stack are: To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order.
	In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form.
	In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed. To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order. In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form. In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed.","// Stack implementation in Java

class Stack {
  private int arr[];
  private int top;
  private int capacity;

  // Creating a stack
  Stack(int size) {
    arr = new int[size];
    capacity = size;
    top = -1;
  }

  // Add elements into stack
  public void push(int x) {
    if (isFull()) {
      System.out.println(""OverFlow\nProgram Terminated\n"");
      System.exit(1);
    }

    System.out.println(""Inserting "" + x);
    arr[++top] = x;
  }

  // Remove element from stack
  public int pop() {
    if (isEmpty()) {
      System.out.println(""STACK EMPTY"");
      System.exit(1);
    }
    return arr[top--];
  }

  // Utility function to return the size of the stack
  public int size() {
    return top + 1;
  }

  // Check if the stack is empty
  public Boolean isEmpty() {
    return top == -1;
  }

  // Check if the stack is full
  public Boolean isFull() {
    return top == capacity - 1;
  }

  public void printStack() {
    for (int i = 0; i <= top; i++) {
      System.out.println(arr[i]);
    }
  }

  public static void main(String[] args) {
    Stack stack = new Stack(5);

    stack.push(1);
    stack.push(2);
    stack.push(3);
    stack.push(4);

    stack.pop();
    System.out.println(""\nAfter popping out"");

    stack.printStack();

  }
}"
"Stack Data Structure and Implementation in Python, Java and C/C++","A stack is a linear data structure that follows the principle of Last In First Out (LIFO). This means the last element inserted inside the stack is removed first. You can think of the stack data structure as the pile of plates on top of another. Here, you can: Put a new plate on top
	Remove the top plate Put a new plate on top Remove the top plate And, if you want the plate at the bottom, you must first remove all the plates on top. This is exactly how the stack data structure works. In programming terms, putting an item on top of the stack is called push and removing an item is called pop. In the above image, although item 3 was kept last, it was removed first. This is exactly how the LIFO (Last In First Out) Principle works. We can implement a stack in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same.  There are some basic operations that allow us to perform different actions on a stack. Push: Add an element to the top of a stack
	Pop: Remove an element from the top of a stack
	IsEmpty: Check if the stack is empty
	IsFull: Check if the stack is full
	Peek: Get the value of the top element without removing it Push: Add an element to the top of a stack Pop: Remove an element from the top of a stack IsEmpty: Check if the stack is empty IsFull: Check if the stack is full Peek: Get the value of the top element without removing it The operations work as follows: A pointer called TOP is used to keep track of the top element in the stack. When initializing the stack, we set its value to -1 so that we can check if the stack is empty by comparing TOP == -1. On pushing an element, we increase the value of TOP and place the new element in the position pointed to by TOP. On popping an element, we return the element pointed to by TOP and reduce its value. Before pushing, we check if the stack is already full Before popping, we check if the stack is already empty The most common stack implementation is using arrays, but it can also be implemented using lists. For the array-based implementation of a stack, the push and pop operations take constant time, i.e. O(1). Although stack is a simple data structure to implement, it is very powerful. The most common uses of a stack are: To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order.
	In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form.
	In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed. To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order. In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form. In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed.","// Stack implementation in C

#include <stdio.h>
#include <stdlib.h>

#define MAX 10

int count = 0;

// Creating a stack
struct stack {
  int items[MAX];
  int top;
};
typedef struct stack st;

void createEmptyStack(st *s) {
  s->top = -1;
}

// Check if the stack is full
int isfull(st *s) {
  if (s->top == MAX - 1)
    return 1;
  else
    return 0;
}

// Check if the stack is empty
int isempty(st *s) {
  if (s->top == -1)
    return 1;
  else
    return 0;
}

// Add elements into stack
void push(st *s, int newitem) {
  if (isfull(s)) {
    printf(""STACK FULL"");
  } else {
    s->top++;
    s->items[s->top] = newitem;
  }
  count++;
}

// Remove element from stack
void pop(st *s) {
  if (isempty(s)) {
    printf(""\n STACK EMPTY \n"");
  } else {
    printf(""Item popped= %d"", s->items[s->top]);
    s->top--;
  }
  count--;
  printf(""\n"");
}

// Print elements of stack
void printStack(st *s) {
  printf(""Stack: "");
  for (int i = 0; i < count; i++) {
    printf(""%d "", s->items[i]);
  }
  printf(""\n"");
}

// Driver code
int main() {
  int ch;
  st *s = (st *)malloc(sizeof(st));

  createEmptyStack(s);

  push(s, 1);
  push(s, 2);
  push(s, 3);
  push(s, 4);

  printStack(s);

  pop(s);

  printf(""\nAfter popping out\n"");
  printStack(s);
}"
"Stack Data Structure and Implementation in Python, Java and C/C++","A stack is a linear data structure that follows the principle of Last In First Out (LIFO). This means the last element inserted inside the stack is removed first. You can think of the stack data structure as the pile of plates on top of another. Here, you can: Put a new plate on top
	Remove the top plate Put a new plate on top Remove the top plate And, if you want the plate at the bottom, you must first remove all the plates on top. This is exactly how the stack data structure works. In programming terms, putting an item on top of the stack is called push and removing an item is called pop. In the above image, although item 3 was kept last, it was removed first. This is exactly how the LIFO (Last In First Out) Principle works. We can implement a stack in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same.  There are some basic operations that allow us to perform different actions on a stack. Push: Add an element to the top of a stack
	Pop: Remove an element from the top of a stack
	IsEmpty: Check if the stack is empty
	IsFull: Check if the stack is full
	Peek: Get the value of the top element without removing it Push: Add an element to the top of a stack Pop: Remove an element from the top of a stack IsEmpty: Check if the stack is empty IsFull: Check if the stack is full Peek: Get the value of the top element without removing it The operations work as follows: A pointer called TOP is used to keep track of the top element in the stack. When initializing the stack, we set its value to -1 so that we can check if the stack is empty by comparing TOP == -1. On pushing an element, we increase the value of TOP and place the new element in the position pointed to by TOP. On popping an element, we return the element pointed to by TOP and reduce its value. Before pushing, we check if the stack is already full Before popping, we check if the stack is already empty The most common stack implementation is using arrays, but it can also be implemented using lists. For the array-based implementation of a stack, the push and pop operations take constant time, i.e. O(1). Although stack is a simple data structure to implement, it is very powerful. The most common uses of a stack are: To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order.
	In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form.
	In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed. To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order. In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form. In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed.","// Stack implementation in C++

#include <stdlib.h>
#include <iostream>

using namespace std;

#define MAX 10
int size = 0;

// Creating a stack
struct stack {
  int items[MAX];
  int top;
};
typedef struct stack st;

void createEmptyStack(st *s) {
  s->top = -1;
}

// Check if the stack is full
int isfull(st *s) {
  if (s->top == MAX - 1)
    return 1;
  else
    return 0;
}

// Check if the stack is empty
int isempty(st *s) {
  if (s->top == -1)
    return 1;
  else
    return 0;
}

// Add elements into stack
void push(st *s, int newitem) {
  if (isfull(s)) {
    cout << ""STACK FULL"";
  } else {
    s->top++;
    s->items[s->top] = newitem;
  }
  size++;
}

// Remove element from stack
void pop(st *s) {
  if (isempty(s)) {
    cout << ""\n STACK EMPTY \n"";
  } else {
    cout << ""Item popped= "" << s->items[s->top];
    s->top--;
  }
  size--;
  cout << endl;
}

// Print elements of stack
void printStack(st *s) {
  printf(""Stack: "");
  for (int i = 0; i < size; i++) {
    cout << s->items[i] << "" "";
  }
  cout << endl;
}

// Driver code
int main() {
  int ch;
  st *s = (st *)malloc(sizeof(st));

  createEmptyStack(s);

  push(s, 1);
  push(s, 2);
  push(s, 3);
  push(s, 4);

  printStack(s);

  pop(s);

  cout << ""\nAfter popping out\n"";
  printStack(s);
}"
"Queue Data Structure and Implementation in Java, Python and C/C++","A queue is a useful data structure in programming. It is similar to the ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket. Queue follows the First In First Out (FIFO) rule - the item that goes in first is the item that comes out first. In the above image, since 1 was kept in the queue before 2, it is the first to be removed from the queue as well. It follows the FIFO rule. In programming terms, putting items in the queue is called enqueue, and removing items from the queue is called dequeue. We can implement the queue in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same. A queue is an object (an abstract data structure - ADT) that allows the following operations: Enqueue: Add an element to the end of the queue
	Dequeue: Remove an element from the front of the queue
	IsEmpty: Check if the queue is empty
	IsFull: Check if the queue is full
	Peek: Get the value of the front of the queue without removing it Enqueue: Add an element to the end of the queue Dequeue: Remove an element from the front of the queue IsEmpty: Check if the queue is empty IsFull: Check if the queue is full Peek: Get the value of the front of the queue without removing it Queue operations work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last element of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last element of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set the value of FRONT to 0
	increase the REAR index by 1
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set the value of FRONT to 0 increase the REAR index by 1 add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  We usually use arrays to implement queues in Java and C/++. In the case of Python, we use lists. As you can see in the image below, after a bit of enqueuing and dequeuing, the size of the queue has been reduced. And we can only add indexes 0 and 1 only when the queue is reset (when all the elements have been dequeued). After REAR reaches the last index, if we can store extra elements in the empty spaces (0 and 1), we can make use of the empty spaces. This is implemented by a modified queue called the circular queue. The complexity of enqueue and dequeue operations in a queue using an array is O(1). If you use pop(N) in python code, then the complexity might be O(n) depending on the position of the item to be popped. CPU scheduling, Disk Scheduling
	When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc
	Handling of interrupts in real-time systems.
	Call Center phone systems use Queues to hold people calling them in order. CPU scheduling, Disk Scheduling When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc Handling of interrupts in real-time systems. Call Center phone systems use Queues to hold people calling them in order. Types of Queue
	Circular Queue
	Deque Data Structure
	Priority Queue Types of Queue Circular Queue Deque Data Structure Priority Queue","# Queue implementation in Python

class Queue:

    def __init__(self):
        self.queue = []

    # Add an element
    def enqueue(self, item):
        self.queue.append(item)

    # Remove an element
    def dequeue(self):
        if len(self.queue) < 1:
            return None
        return self.queue.pop(0)

    # Display  the queue
    def display(self):
        print(self.queue)

    def size(self):
        return len(self.queue)


q = Queue()
q.enqueue(1)
q.enqueue(2)
q.enqueue(3)
q.enqueue(4)
q.enqueue(5)

q.display()

q.dequeue()

print(""After removing an element"")
q.display()
"
"Queue Data Structure and Implementation in Java, Python and C/C++","A queue is a useful data structure in programming. It is similar to the ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket. Queue follows the First In First Out (FIFO) rule - the item that goes in first is the item that comes out first. In the above image, since 1 was kept in the queue before 2, it is the first to be removed from the queue as well. It follows the FIFO rule. In programming terms, putting items in the queue is called enqueue, and removing items from the queue is called dequeue. We can implement the queue in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same. A queue is an object (an abstract data structure - ADT) that allows the following operations: Enqueue: Add an element to the end of the queue
	Dequeue: Remove an element from the front of the queue
	IsEmpty: Check if the queue is empty
	IsFull: Check if the queue is full
	Peek: Get the value of the front of the queue without removing it Enqueue: Add an element to the end of the queue Dequeue: Remove an element from the front of the queue IsEmpty: Check if the queue is empty IsFull: Check if the queue is full Peek: Get the value of the front of the queue without removing it Queue operations work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last element of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last element of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set the value of FRONT to 0
	increase the REAR index by 1
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set the value of FRONT to 0 increase the REAR index by 1 add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  We usually use arrays to implement queues in Java and C/++. In the case of Python, we use lists. As you can see in the image below, after a bit of enqueuing and dequeuing, the size of the queue has been reduced. And we can only add indexes 0 and 1 only when the queue is reset (when all the elements have been dequeued). After REAR reaches the last index, if we can store extra elements in the empty spaces (0 and 1), we can make use of the empty spaces. This is implemented by a modified queue called the circular queue. The complexity of enqueue and dequeue operations in a queue using an array is O(1). If you use pop(N) in python code, then the complexity might be O(n) depending on the position of the item to be popped. CPU scheduling, Disk Scheduling
	When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc
	Handling of interrupts in real-time systems.
	Call Center phone systems use Queues to hold people calling them in order. CPU scheduling, Disk Scheduling When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc Handling of interrupts in real-time systems. Call Center phone systems use Queues to hold people calling them in order. Types of Queue
	Circular Queue
	Deque Data Structure
	Priority Queue Types of Queue Circular Queue Deque Data Structure Priority Queue","// Queue implementation in Java

public class Queue {
  int SIZE = 5;
  int items[] = new int[SIZE];
  int front, rear;

  Queue() {
    front = -1;
    rear = -1;
  }

  boolean isFull() {
    if (front == 0 && rear == SIZE - 1) {
      return true;
    }
    return false;
  }

  boolean isEmpty() {
    if (front == -1)
      return true;
    else
      return false;
  }

  void enQueue(int element) {
    if (isFull()) {
      System.out.println(""Queue is full"");
    } else {
      if (front == -1)
        front = 0;
      rear++;
      items[rear] = element;
      System.out.println(""Inserted "" + element);
    }
  }

  int deQueue() {
    int element;
    if (isEmpty()) {
      System.out.println(""Queue is empty"");
      return (-1);
    } else {
      element = items[front];
      if (front >= rear) {
        front = -1;
        rear = -1;
      } /* Q has only one element, so we reset the queue after deleting it. */
      else {
        front++;
      }
      System.out.println(""Deleted -> "" + element);
      return (element);
    }
  }

  void display() {
    /* Function to display elements of Queue */
    int i;
    if (isEmpty()) {
      System.out.println(""Empty Queue"");
    } else {
      System.out.println(""\nFront index-> "" + front);
      System.out.println(""Items -> "");
      for (i = front; i <= rear; i++)
        System.out.print(items[i] + ""  "");

      System.out.println(""\nRear index-> "" + rear);
    }
  }

  public static void main(String[] args) {
    Queue q = new Queue();

    // deQueue is not possible on empty queue
    q.deQueue();

    // enQueue 5 elements
    q.enQueue(1);
    q.enQueue(2);
    q.enQueue(3);
    q.enQueue(4);
    q.enQueue(5);

    // 6th element can't be added to because the queue is full
    q.enQueue(6);

    q.display();

    // deQueue removes element entered first i.e. 1
    q.deQueue();

    // Now we have just 4 elements
    q.display();

  }
}"
"Queue Data Structure and Implementation in Java, Python and C/C++","A queue is a useful data structure in programming. It is similar to the ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket. Queue follows the First In First Out (FIFO) rule - the item that goes in first is the item that comes out first. In the above image, since 1 was kept in the queue before 2, it is the first to be removed from the queue as well. It follows the FIFO rule. In programming terms, putting items in the queue is called enqueue, and removing items from the queue is called dequeue. We can implement the queue in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same. A queue is an object (an abstract data structure - ADT) that allows the following operations: Enqueue: Add an element to the end of the queue
	Dequeue: Remove an element from the front of the queue
	IsEmpty: Check if the queue is empty
	IsFull: Check if the queue is full
	Peek: Get the value of the front of the queue without removing it Enqueue: Add an element to the end of the queue Dequeue: Remove an element from the front of the queue IsEmpty: Check if the queue is empty IsFull: Check if the queue is full Peek: Get the value of the front of the queue without removing it Queue operations work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last element of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last element of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set the value of FRONT to 0
	increase the REAR index by 1
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set the value of FRONT to 0 increase the REAR index by 1 add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  We usually use arrays to implement queues in Java and C/++. In the case of Python, we use lists. As you can see in the image below, after a bit of enqueuing and dequeuing, the size of the queue has been reduced. And we can only add indexes 0 and 1 only when the queue is reset (when all the elements have been dequeued). After REAR reaches the last index, if we can store extra elements in the empty spaces (0 and 1), we can make use of the empty spaces. This is implemented by a modified queue called the circular queue. The complexity of enqueue and dequeue operations in a queue using an array is O(1). If you use pop(N) in python code, then the complexity might be O(n) depending on the position of the item to be popped. CPU scheduling, Disk Scheduling
	When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc
	Handling of interrupts in real-time systems.
	Call Center phone systems use Queues to hold people calling them in order. CPU scheduling, Disk Scheduling When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc Handling of interrupts in real-time systems. Call Center phone systems use Queues to hold people calling them in order. Types of Queue
	Circular Queue
	Deque Data Structure
	Priority Queue Types of Queue Circular Queue Deque Data Structure Priority Queue","// Queue implementation in C

#include <stdio.h>
#define SIZE 5

void enQueue(int);
void deQueue();
void display();

int items[SIZE], front = -1, rear = -1;

int main() {
  //deQueue is not possible on empty queue
  deQueue();

  //enQueue 5 elements
  enQueue(1);
  enQueue(2);
  enQueue(3);
  enQueue(4);
  enQueue(5);

  // 6th element can't be added to because the queue is full
  enQueue(6);

  display();

  //deQueue removes element entered first i.e. 1
  deQueue();

  //Now we have just 4 elements
  display();

  return 0;
}

void enQueue(int value) {
  if (rear == SIZE - 1)
    printf(""\nQueue is Full!!"");
  else {
    if (front == -1)
      front = 0;
    rear++;
    items[rear] = value;
    printf(""\nInserted -> %d"", value);
  }
}

void deQueue() {
  if (front == -1)
    printf(""\nQueue is Empty!!"");
  else {
    printf(""\nDeleted : %d"", items[front]);
    front++;
    if (front > rear)
      front = rear = -1;
  }
}

// Function to print the queue
void display() {
  if (rear == -1)
    printf(""\nQueue is Empty!!!"");
  else {
    int i;
    printf(""\nQueue elements are:\n"");
    for (i = front; i <= rear; i++)
      printf(""%d  "", items[i]);
  }
  printf(""\n"");
}"
"Queue Data Structure and Implementation in Java, Python and C/C++","A queue is a useful data structure in programming. It is similar to the ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket. Queue follows the First In First Out (FIFO) rule - the item that goes in first is the item that comes out first. In the above image, since 1 was kept in the queue before 2, it is the first to be removed from the queue as well. It follows the FIFO rule. In programming terms, putting items in the queue is called enqueue, and removing items from the queue is called dequeue. We can implement the queue in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same. A queue is an object (an abstract data structure - ADT) that allows the following operations: Enqueue: Add an element to the end of the queue
	Dequeue: Remove an element from the front of the queue
	IsEmpty: Check if the queue is empty
	IsFull: Check if the queue is full
	Peek: Get the value of the front of the queue without removing it Enqueue: Add an element to the end of the queue Dequeue: Remove an element from the front of the queue IsEmpty: Check if the queue is empty IsFull: Check if the queue is full Peek: Get the value of the front of the queue without removing it Queue operations work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last element of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last element of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set the value of FRONT to 0
	increase the REAR index by 1
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set the value of FRONT to 0 increase the REAR index by 1 add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  We usually use arrays to implement queues in Java and C/++. In the case of Python, we use lists. As you can see in the image below, after a bit of enqueuing and dequeuing, the size of the queue has been reduced. And we can only add indexes 0 and 1 only when the queue is reset (when all the elements have been dequeued). After REAR reaches the last index, if we can store extra elements in the empty spaces (0 and 1), we can make use of the empty spaces. This is implemented by a modified queue called the circular queue. The complexity of enqueue and dequeue operations in a queue using an array is O(1). If you use pop(N) in python code, then the complexity might be O(n) depending on the position of the item to be popped. CPU scheduling, Disk Scheduling
	When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc
	Handling of interrupts in real-time systems.
	Call Center phone systems use Queues to hold people calling them in order. CPU scheduling, Disk Scheduling When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc Handling of interrupts in real-time systems. Call Center phone systems use Queues to hold people calling them in order. Types of Queue
	Circular Queue
	Deque Data Structure
	Priority Queue Types of Queue Circular Queue Deque Data Structure Priority Queue","// Queue implementation in C++

#include <iostream>
#define SIZE 5

using namespace std;

class Queue {
   private:
  int items[SIZE], front, rear;

   public:
  Queue() {
    front = -1;
    rear = -1;
  }

  bool isFull() {
    if (front == 0 && rear == SIZE - 1) {
      return true;
    }
    return false;
  }

  bool isEmpty() {
    if (front == -1)
      return true;
    else
      return false;
  }

  void enQueue(int element) {
    if (isFull()) {
      cout << ""Queue is full"";
    } else {
      if (front == -1) front = 0;
      rear++;
      items[rear] = element;
      cout << endl
         << ""Inserted "" << element << endl;
    }
  }

  int deQueue() {
    int element;
    if (isEmpty()) {
      cout << ""Queue is empty"" << endl;
      return (-1);
    } else {
      element = items[front];
      if (front >= rear) {
        front = -1;
        rear = -1;
      } /* Q has only one element, so we reset the queue after deleting it. */
      else {
        front++;
      }
      cout << endl
         << ""Deleted -> "" << element << endl;
      return (element);
    }
  }

  void display() {
    /* Function to display elements of Queue */
    int i;
    if (isEmpty()) {
      cout << endl
         << ""Empty Queue"" << endl;
    } else {
      cout << endl
         << ""Front index-> "" << front;
      cout << endl
         << ""Items -> "";
      for (i = front; i <= rear; i++)
        cout << items[i] << ""  "";
      cout << endl
         << ""Rear index-> "" << rear << endl;
    }
  }
};

int main() {
  Queue q;

  //deQueue is not possible on empty queue
  q.deQueue();

  //enQueue 5 elements
  q.enQueue(1);
  q.enQueue(2);
  q.enQueue(3);
  q.enQueue(4);
  q.enQueue(5);

  // 6th element can't be added to because the queue is full
  q.enQueue(6);

  q.display();

  //deQueue removes element entered first i.e. 1
  q.deQueue();

  //Now we have just 4 elements
  q.display();

  return 0;
}"
Circular Queue Data Structure,"A circular queue is the extended version of a regular queue where the last element is connected to the first element. Thus forming a circle-like structure. The circular queue solves the major limitation of the normal queue. In a normal queue, after a bit of insertion and deletion, there will be non-usable empty space. Here, indexes 0 and 1 can only be used after resetting the queue (deletion of all elements). This reduces the actual size of the queue. Circular Queue works by the process of circular increment i.e. when we try to increment the pointer and we reach the end of the queue, we start from the beginning of the queue. Here, the circular increment is performed by modulo division with the queue size. That is, The circular queue work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last elements of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last elements of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set value of FRONT to 0
	circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue)
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set value of FRONT to 0 circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue) add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	circularly increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT circularly increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  However, the check for full queue has a new additional case: Case 1: FRONT = 0 && REAR == SIZE - 1
	Case 2: FRONT = REAR + 1 Case 1: FRONT = 0 && REAR == SIZE - 1 Case 2: FRONT = REAR + 1 The second case happens when REAR starts from 0 due to circular increment and when its value is just 1 less than FRONT, the queue is full. The most common queue implementation is using arrays, but it can also be implemented using lists. The complexity of the enqueue and dequeue operations of a circular queue is O(1) for (array implementations). CPU scheduling
	Memory management
	Traffic Management CPU scheduling Memory management Traffic Management","# Circular Queue implementation in Python


class MyCircularQueue():

    def __init__(self, k):
        self.k = k
        self.queue = [None] * k
        self.head = self.tail = -1

    # Insert an element into the circular queue
    def enqueue(self, data):

        if ((self.tail + 1) % self.k == self.head):
            print(""The circular queue is full\n"")

        elif (self.head == -1):
            self.head = 0
            self.tail = 0
            self.queue[self.tail] = data
        else:
            self.tail = (self.tail + 1) % self.k
            self.queue[self.tail] = data

    # Delete an element from the circular queue
    def dequeue(self):
        if (self.head == -1):
            print(""The circular queue is empty\n"")

        elif (self.head == self.tail):
            temp = self.queue[self.head]
            self.head = -1
            self.tail = -1
            return temp
        else:
            temp = self.queue[self.head]
            self.head = (self.head + 1) % self.k
            return temp

    def printCQueue(self):
        if(self.head == -1):
            print(""No element in the circular queue"")

        elif (self.tail >= self.head):
            for i in range(self.head, self.tail + 1):
                print(self.queue[i], end="" "")
            print()
        else:
            for i in range(self.head, self.k):
                print(self.queue[i], end="" "")
            for i in range(0, self.tail + 1):
                print(self.queue[i], end="" "")
            print()


# Your MyCircularQueue object will be instantiated and called as such:
obj = MyCircularQueue(5)
obj.enqueue(1)
obj.enqueue(2)
obj.enqueue(3)
obj.enqueue(4)
obj.enqueue(5)
print(""Initial queue"")
obj.printCQueue()

obj.dequeue()
print(""After removing an element from the queue"")
obj.printCQueue()
"
Circular Queue Data Structure,"A circular queue is the extended version of a regular queue where the last element is connected to the first element. Thus forming a circle-like structure. The circular queue solves the major limitation of the normal queue. In a normal queue, after a bit of insertion and deletion, there will be non-usable empty space. Here, indexes 0 and 1 can only be used after resetting the queue (deletion of all elements). This reduces the actual size of the queue. Circular Queue works by the process of circular increment i.e. when we try to increment the pointer and we reach the end of the queue, we start from the beginning of the queue. Here, the circular increment is performed by modulo division with the queue size. That is, The circular queue work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last elements of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last elements of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set value of FRONT to 0
	circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue)
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set value of FRONT to 0 circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue) add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	circularly increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT circularly increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  However, the check for full queue has a new additional case: Case 1: FRONT = 0 && REAR == SIZE - 1
	Case 2: FRONT = REAR + 1 Case 1: FRONT = 0 && REAR == SIZE - 1 Case 2: FRONT = REAR + 1 The second case happens when REAR starts from 0 due to circular increment and when its value is just 1 less than FRONT, the queue is full. The most common queue implementation is using arrays, but it can also be implemented using lists. The complexity of the enqueue and dequeue operations of a circular queue is O(1) for (array implementations). CPU scheduling
	Memory management
	Traffic Management CPU scheduling Memory management Traffic Management","// Circular Queue implementation in Java

public class CQueue {
  int SIZE = 5; // Size of Circular Queue
  int front, rear;
  int items[] = new int[SIZE];

  CQueue() {
    front = -1;
    rear = -1;
  }

  // Check if the queue is full
  boolean isFull() {
    if (front == 0 && rear == SIZE - 1) {
      return true;
    }
    if (front == rear + 1) {
      return true;
    }
    return false;
  }

  // Check if the queue is empty
  boolean isEmpty() {
    if (front == -1)
      return true;
    else
      return false;
  }

  // Adding an element
  void enQueue(int element) {
    if (isFull()) {
      System.out.println(""Queue is full"");
    } else {
      if (front == -1)
        front = 0;
      rear = (rear + 1) % SIZE;
      items[rear] = element;
      System.out.println(""Inserted "" + element);
    }
  }

  // Removing an element
  int deQueue() {
    int element;
    if (isEmpty()) {
      System.out.println(""Queue is empty"");
      return (-1);
    } else {
      element = items[front];
      if (front == rear) {
        front = -1;
        rear = -1;
      } /* Q has only one element, so we reset the queue after deleting it. */
      else {
        front = (front + 1) % SIZE;
      }
      return (element);
    }
  }

  void display() {
    /* Function to display status of Circular Queue */
    int i;
    if (isEmpty()) {
      System.out.println(""Empty Queue"");
    } else {
      System.out.println(""Front -> "" + front);
      System.out.println(""Items -> "");
      for (i = front; i != rear; i = (i + 1) % SIZE)
        System.out.print(items[i] + "" "");
      System.out.println(items[i]);
      System.out.println(""Rear -> "" + rear);
    }
  }

  public static void main(String[] args) {

    CQueue q = new CQueue();

    // Fails because front = -1
    q.deQueue();

    q.enQueue(1);
    q.enQueue(2);
    q.enQueue(3);
    q.enQueue(4);
    q.enQueue(5);

    // Fails to enqueue because front == 0 && rear == SIZE - 1
    q.enQueue(6);

    q.display();

    int elem = q.deQueue();

    if (elem != -1) {
      System.out.println(""Deleted Element is "" + elem);
    }
    q.display();

    q.enQueue(7);

    q.display();

    // Fails to enqueue because front == rear + 1
    q.enQueue(8);
  }

}"
Circular Queue Data Structure,"A circular queue is the extended version of a regular queue where the last element is connected to the first element. Thus forming a circle-like structure. The circular queue solves the major limitation of the normal queue. In a normal queue, after a bit of insertion and deletion, there will be non-usable empty space. Here, indexes 0 and 1 can only be used after resetting the queue (deletion of all elements). This reduces the actual size of the queue. Circular Queue works by the process of circular increment i.e. when we try to increment the pointer and we reach the end of the queue, we start from the beginning of the queue. Here, the circular increment is performed by modulo division with the queue size. That is, The circular queue work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last elements of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last elements of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set value of FRONT to 0
	circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue)
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set value of FRONT to 0 circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue) add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	circularly increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT circularly increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  However, the check for full queue has a new additional case: Case 1: FRONT = 0 && REAR == SIZE - 1
	Case 2: FRONT = REAR + 1 Case 1: FRONT = 0 && REAR == SIZE - 1 Case 2: FRONT = REAR + 1 The second case happens when REAR starts from 0 due to circular increment and when its value is just 1 less than FRONT, the queue is full. The most common queue implementation is using arrays, but it can also be implemented using lists. The complexity of the enqueue and dequeue operations of a circular queue is O(1) for (array implementations). CPU scheduling
	Memory management
	Traffic Management CPU scheduling Memory management Traffic Management","// Circular Queue implementation in C

#include <stdio.h>

#define SIZE 5

int items[SIZE];
int front = -1, rear = -1;

// Check if the queue is full
int isFull() {
  if ((front == rear + 1) || (front == 0 && rear == SIZE - 1)) return 1;
  return 0;
}

// Check if the queue is empty
int isEmpty() {
  if (front == -1) return 1;
  return 0;
}

// Adding an element
void enQueue(int element) {
  if (isFull())
    printf(""\n Queue is full!! \n"");
  else {
    if (front == -1) front = 0;
    rear = (rear + 1) % SIZE;
    items[rear] = element;
    printf(""\n Inserted -> %d"", element);
  }
}

// Removing an element
int deQueue() {
  int element;
  if (isEmpty()) {
    printf(""\n Queue is empty !! \n"");
    return (-1);
  } else {
    element = items[front];
    if (front == rear) {
      front = -1;
      rear = -1;
    } 
    // Q has only one element, so we reset the 
    // queue after dequeing it. ?
    else {
      front = (front + 1) % SIZE;
    }
    printf(""\n Deleted element -> %d \n"", element);
    return (element);
  }
}

// Display the queue
void display() {
  int i;
  if (isEmpty())
    printf("" \n Empty Queue\n"");
  else {
    printf(""\n Front -> %d "", front);
    printf(""\n Items -> "");
    for (i = front; i != rear; i = (i + 1) % SIZE) {
      printf(""%d "", items[i]);
    }
    printf(""%d "", items[i]);
    printf(""\n Rear -> %d \n"", rear);
  }
}

int main() {
  // Fails because front = -1
  deQueue();

  enQueue(1);
  enQueue(2);
  enQueue(3);
  enQueue(4);
  enQueue(5);

  // Fails to enqueue because front == 0 && rear == SIZE - 1
  enQueue(6);

  display();
  deQueue();

  display();

  enQueue(7);
  display();

  // Fails to enqueue because front == rear + 1
  enQueue(8);

  return 0;
}"
Circular Queue Data Structure,"A circular queue is the extended version of a regular queue where the last element is connected to the first element. Thus forming a circle-like structure. The circular queue solves the major limitation of the normal queue. In a normal queue, after a bit of insertion and deletion, there will be non-usable empty space. Here, indexes 0 and 1 can only be used after resetting the queue (deletion of all elements). This reduces the actual size of the queue. Circular Queue works by the process of circular increment i.e. when we try to increment the pointer and we reach the end of the queue, we start from the beginning of the queue. Here, the circular increment is performed by modulo division with the queue size. That is, The circular queue work as follows: two pointers FRONT and REAR
	FRONT track the first element of the queue
	REAR track the last elements of the queue
	initially, set value of FRONT and REAR to -1 two pointers FRONT and REAR FRONT track the first element of the queue REAR track the last elements of the queue initially, set value of FRONT and REAR to -1 check if the queue is full
	for the first element, set value of FRONT to 0
	circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue)
	add the new element in the position pointed to by REAR check if the queue is full for the first element, set value of FRONT to 0 circularly increase the REAR index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue) add the new element in the position pointed to by REAR check if the queue is empty
	return the value pointed by FRONT
	circularly increase the FRONT index by 1
	for the last element, reset the values of FRONT and REAR to -1 check if the queue is empty return the value pointed by FRONT circularly increase the FRONT index by 1 for the last element, reset the values of FRONT and REAR to -1  However, the check for full queue has a new additional case: Case 1: FRONT = 0 && REAR == SIZE - 1
	Case 2: FRONT = REAR + 1 Case 1: FRONT = 0 && REAR == SIZE - 1 Case 2: FRONT = REAR + 1 The second case happens when REAR starts from 0 due to circular increment and when its value is just 1 less than FRONT, the queue is full. The most common queue implementation is using arrays, but it can also be implemented using lists. The complexity of the enqueue and dequeue operations of a circular queue is O(1) for (array implementations). CPU scheduling
	Memory management
	Traffic Management CPU scheduling Memory management Traffic Management","// Circular Queue implementation in C++

#include <iostream>
#define SIZE 5 /* Size of Circular Queue */

using namespace std;

class Queue {
   private:
  int items[SIZE], front, rear;

   public:
  Queue() {
    front = -1;
    rear = -1;
  }
  // Check if the queue is full
  bool isFull() {
    if (front == 0 && rear == SIZE - 1) {
      return true;
    }
    if (front == rear + 1) {
      return true;
    }
    return false;
  }
  // Check if the queue is empty
  bool isEmpty() {
    if (front == -1)
      return true;
    else
      return false;
  }
  // Adding an element
  void enQueue(int element) {
    if (isFull()) {
      cout << ""Queue is full"";
    } else {
      if (front == -1) front = 0;
      rear = (rear + 1) % SIZE;
      items[rear] = element;
      cout << endl
         << ""Inserted "" << element << endl;
    }
  }
  // Removing an element
  int deQueue() {
    int element;
    if (isEmpty()) {
      cout << ""Queue is empty"" << endl;
      return (-1);
    } else {
      element = items[front];
      if (front == rear) {
        front = -1;
        rear = -1;
      }
      // Q has only one element,
      // so we reset the queue after deleting it.
      else {
        front = (front + 1) % SIZE;
      }
      return (element);
    }
  }

  void display() {
    // Function to display status of Circular Queue
    int i;
    if (isEmpty()) {
      cout << endl
         << ""Empty Queue"" << endl;
    } else {
      cout << ""Front -> "" << front;
      cout << endl
         << ""Items -> "";
      for (i = front; i != rear; i = (i + 1) % SIZE)
        cout << items[i];
      cout << items[i];
      cout << endl
         << ""Rear -> "" << rear;
    }
  }
};

int main() {
  Queue q;

  // Fails because front = -1
  q.deQueue();

  q.enQueue(1);
  q.enQueue(2);
  q.enQueue(3);
  q.enQueue(4);
  q.enQueue(5);

  // Fails to enqueue because front == 0 && rear == SIZE - 1
  q.enQueue(6);

  q.display();

  int elem = q.deQueue();

  if (elem != -1)
    cout << endl
       << ""Deleted Element is "" << elem;

  q.display();

  q.enQueue(7);

  q.display();

  // Fails to enqueue because front == rear + 1
  q.enQueue(8);

  return 0;
}"
Priority Queue Data Structure,"A priority queue is a special type of queue in which each element is associated with a priority value. And, elements are served on the basis of their priority. That is, higher priority elements are served first. However, if elements with the same priority occur, they are served according to their order in the queue. Assigning Priority Value Generally, the value of the element itself is considered for assigning the priority. For example, The element with the highest value is considered the highest priority element. However, in other cases, we can assume the element with the lowest value as the highest priority element. We can also set priorities according to our needs. In a queue, the first-in-first-out rule is implemented whereas, in a priority queue, the values are removed on the basis of priority. The element with the highest priority is removed first. Priority queue can be implemented using an array, a linked list, a heap data structure, or a binary search tree. Among these data structures, heap data structure provides an efficient implementation of priority queues.  Hence, we will be using the heap data structure to implement the priority queue in this tutorial. A max-heap is implemented in the following operations. If you want to learn more about it, please visit max-heap and min-heap. A comparative analysis of different implementations of priority queue is given below. Basic operations of a priority queue are inserting, removing, and peeking elements. Before studying the priority queue, please refer to the heap data structure for a better understanding of binary heap as it is used to implement the priority queue in this article. Inserting an element into a priority queue (max-heap) is done by the following steps. Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue
		
	
	Heapify the tree.
		
			Heapify after insertion Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue Heapify the tree.
		
			Heapify after insertion Algorithm for insertion of an element into priority queue (max-heap) For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Deleting an element from a priority queue (max-heap) is done as follows: Select the element to be deleted.
		
			Select the element to be deleted
		
	
	Swap it with the last element.
		
			Swap with the last leaf node element
		
	
	Remove the last element.
		
			Remove the last element leaf
		
	
	Heapify the tree.
		
			Heapify the priority queue Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last leaf node element Remove the last element.
		
			Remove the last element leaf Heapify the tree.
		
			Heapify the priority queue Algorithm for deletion of an element in the priority queue (max-heap) For Min Heap, the above algorithm is modified so that the both childNodes are smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum value after removing it from Min Heap. Some of the applications of a priority queue are: Dijkstra's algorithm
	for implementing stack
	for load balancing and interrupt handling in an operating system
	for data compression in Huffman code Dijkstra's algorithm for implementing stack for load balancing and interrupt handling in an operating system for data compression in Huffman code","# Priority Queue implementation in Python


# Function to heapify the tree
def heapify(arr, n, i):
    # Find the largest among root, left child and right child
    largest = i
    l = 2 * i + 1
    r = 2 * i + 2

    if l < n and arr[i] < arr[l]:
        largest = l

    if r < n and arr[largest] < arr[r]:
        largest = r

    # Swap and continue heapifying if root is not largest
    if largest != i:
        arr[i], arr[largest] = arr[largest], arr[i]
        heapify(arr, n, largest)


# Function to insert an element into the tree
def insert(array, newNum):
    size = len(array)
    if size == 0:
        array.append(newNum)
    else:
        array.append(newNum)
        for i in range((size // 2) - 1, -1, -1):
            heapify(array, size, i)


# Function to delete an element from the tree
def deleteNode(array, num):
    size = len(array)
    i = 0
    for i in range(0, size):
        if num == array[i]:
            break

    array[i], array[size - 1] = array[size - 1], array[i]

    array.remove(size - 1)

    for i in range((len(array) // 2) - 1, -1, -1):
        heapify(array, len(array), i)


arr = []

insert(arr, 3)
insert(arr, 4)
insert(arr, 9)
insert(arr, 5)
insert(arr, 2)

print (""Max-Heap array: "" + str(arr))

deleteNode(arr, 4)
print(""After deleting an element: "" + str(arr))"
Priority Queue Data Structure,"A priority queue is a special type of queue in which each element is associated with a priority value. And, elements are served on the basis of their priority. That is, higher priority elements are served first. However, if elements with the same priority occur, they are served according to their order in the queue. Assigning Priority Value Generally, the value of the element itself is considered for assigning the priority. For example, The element with the highest value is considered the highest priority element. However, in other cases, we can assume the element with the lowest value as the highest priority element. We can also set priorities according to our needs. In a queue, the first-in-first-out rule is implemented whereas, in a priority queue, the values are removed on the basis of priority. The element with the highest priority is removed first. Priority queue can be implemented using an array, a linked list, a heap data structure, or a binary search tree. Among these data structures, heap data structure provides an efficient implementation of priority queues.  Hence, we will be using the heap data structure to implement the priority queue in this tutorial. A max-heap is implemented in the following operations. If you want to learn more about it, please visit max-heap and min-heap. A comparative analysis of different implementations of priority queue is given below. Basic operations of a priority queue are inserting, removing, and peeking elements. Before studying the priority queue, please refer to the heap data structure for a better understanding of binary heap as it is used to implement the priority queue in this article. Inserting an element into a priority queue (max-heap) is done by the following steps. Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue
		
	
	Heapify the tree.
		
			Heapify after insertion Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue Heapify the tree.
		
			Heapify after insertion Algorithm for insertion of an element into priority queue (max-heap) For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Deleting an element from a priority queue (max-heap) is done as follows: Select the element to be deleted.
		
			Select the element to be deleted
		
	
	Swap it with the last element.
		
			Swap with the last leaf node element
		
	
	Remove the last element.
		
			Remove the last element leaf
		
	
	Heapify the tree.
		
			Heapify the priority queue Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last leaf node element Remove the last element.
		
			Remove the last element leaf Heapify the tree.
		
			Heapify the priority queue Algorithm for deletion of an element in the priority queue (max-heap) For Min Heap, the above algorithm is modified so that the both childNodes are smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum value after removing it from Min Heap. Some of the applications of a priority queue are: Dijkstra's algorithm
	for implementing stack
	for load balancing and interrupt handling in an operating system
	for data compression in Huffman code Dijkstra's algorithm for implementing stack for load balancing and interrupt handling in an operating system for data compression in Huffman code","// Priority Queue implementation in Java

import java.util.ArrayList;

class Heap {
  // Function to heapify the tree
  void heapify(ArrayList<Integer> hT, int i) {
    int size = hT.size();
    // Find the largest among root, left child and right child
    int largest = i;
    int l = 2 * i + 1;
    int r = 2 * i + 2;
    if (l < size && hT.get(l) > hT.get(largest))
      largest = l;
    if (r < size && hT.get(r) > hT.get(largest))
      largest = r;

    // Swap and continue heapifying if root is not largest
    if (largest != i) {
      int temp = hT.get(largest);
      hT.set(largest, hT.get(i));
      hT.set(i, temp);

      heapify(hT, largest);
    }
  }

  // Function to insert an element into the tree
  void insert(ArrayList<Integer> hT, int newNum) {
    int size = hT.size();
    if (size == 0) {
      hT.add(newNum);
    } else {
      hT.add(newNum);
      for (int i = size / 2 - 1; i >= 0; i--) {
        heapify(hT, i);
      }
    }
  }

  // Function to delete an element from the tree
  void deleteNode(ArrayList<Integer> hT, int num) {
    int size = hT.size();
    int i;
    for (i = 0; i < size; i++) {
      if (num == hT.get(i))
        break;
    }

    int temp = hT.get(i);
    hT.set(i, hT.get(size - 1));
    hT.set(size - 1, temp);

    hT.remove(size - 1);
    for (int j = size / 2 - 1; j >= 0; j--) {
      heapify(hT, j);
    }
  }

  // Print the tree
  void printArray(ArrayList<Integer> array, int size) {
    for (Integer i : array) {
      System.out.print(i + "" "");
    }
    System.out.println();
  }

  // Driver code
  public static void main(String args[]) {

    ArrayList<Integer> array = new ArrayList<Integer>();
    int size = array.size();

    Heap h = new Heap();
    h.insert(array, 3);
    h.insert(array, 4);
    h.insert(array, 9);
    h.insert(array, 5);
    h.insert(array, 2);

    System.out.println(""Max-Heap array: "");
    h.printArray(array, size);

    h.deleteNode(array, 4);
    System.out.println(""After deleting an element: "");
    h.printArray(array, size);
  }
}"
Priority Queue Data Structure,"A priority queue is a special type of queue in which each element is associated with a priority value. And, elements are served on the basis of their priority. That is, higher priority elements are served first. However, if elements with the same priority occur, they are served according to their order in the queue. Assigning Priority Value Generally, the value of the element itself is considered for assigning the priority. For example, The element with the highest value is considered the highest priority element. However, in other cases, we can assume the element with the lowest value as the highest priority element. We can also set priorities according to our needs. In a queue, the first-in-first-out rule is implemented whereas, in a priority queue, the values are removed on the basis of priority. The element with the highest priority is removed first. Priority queue can be implemented using an array, a linked list, a heap data structure, or a binary search tree. Among these data structures, heap data structure provides an efficient implementation of priority queues.  Hence, we will be using the heap data structure to implement the priority queue in this tutorial. A max-heap is implemented in the following operations. If you want to learn more about it, please visit max-heap and min-heap. A comparative analysis of different implementations of priority queue is given below. Basic operations of a priority queue are inserting, removing, and peeking elements. Before studying the priority queue, please refer to the heap data structure for a better understanding of binary heap as it is used to implement the priority queue in this article. Inserting an element into a priority queue (max-heap) is done by the following steps. Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue
		
	
	Heapify the tree.
		
			Heapify after insertion Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue Heapify the tree.
		
			Heapify after insertion Algorithm for insertion of an element into priority queue (max-heap) For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Deleting an element from a priority queue (max-heap) is done as follows: Select the element to be deleted.
		
			Select the element to be deleted
		
	
	Swap it with the last element.
		
			Swap with the last leaf node element
		
	
	Remove the last element.
		
			Remove the last element leaf
		
	
	Heapify the tree.
		
			Heapify the priority queue Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last leaf node element Remove the last element.
		
			Remove the last element leaf Heapify the tree.
		
			Heapify the priority queue Algorithm for deletion of an element in the priority queue (max-heap) For Min Heap, the above algorithm is modified so that the both childNodes are smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum value after removing it from Min Heap. Some of the applications of a priority queue are: Dijkstra's algorithm
	for implementing stack
	for load balancing and interrupt handling in an operating system
	for data compression in Huffman code Dijkstra's algorithm for implementing stack for load balancing and interrupt handling in an operating system for data compression in Huffman code","// Priority Queue implementation in C

#include <stdio.h>
int size = 0;
void swap(int *a, int *b) {
  int temp = *b;
  *b = *a;
  *a = temp;
}

// Function to heapify the tree
void heapify(int array[], int size, int i) {
  if (size == 1) {
    printf(""Single element in the heap"");
  } else {
    // Find the largest among root, left child and right child
    int largest = i;
    int l = 2 * i + 1;
    int r = 2 * i + 2;
    if (l < size && array[l] > array[largest])
      largest = l;
    if (r < size && array[r] > array[largest])
      largest = r;

    // Swap and continue heapifying if root is not largest
    if (largest != i) {
      swap(&array[i], &array[largest]);
      heapify(array, size, largest);
    }
  }
}

// Function to insert an element into the tree
void insert(int array[], int newNum) {
  if (size == 0) {
    array[0] = newNum;
    size += 1;
  } else {
    array[size] = newNum;
    size += 1;
    for (int i = size / 2 - 1; i >= 0; i--) {
      heapify(array, size, i);
    }
  }
}

// Function to delete an element from the tree
void deleteRoot(int array[], int num) {
  int i;
  for (i = 0; i < size; i++) {
    if (num == array[i])
      break;
  }

  swap(&array[i], &array[size - 1]);
  size -= 1;
  for (int i = size / 2 - 1; i >= 0; i--) {
    heapify(array, size, i);
  }
}

// Print the array
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i)
    printf(""%d "", array[i]);
  printf(""\n"");
}

// Driver code
int main() {
  int array[10];

  insert(array, 3);
  insert(array, 4);
  insert(array, 9);
  insert(array, 5);
  insert(array, 2);

  printf(""Max-Heap array: "");
  printArray(array, size);

  deleteRoot(array, 4);

  printf(""After deleting an element: "");

  printArray(array, size);
}"
Priority Queue Data Structure,"A priority queue is a special type of queue in which each element is associated with a priority value. And, elements are served on the basis of their priority. That is, higher priority elements are served first. However, if elements with the same priority occur, they are served according to their order in the queue. Assigning Priority Value Generally, the value of the element itself is considered for assigning the priority. For example, The element with the highest value is considered the highest priority element. However, in other cases, we can assume the element with the lowest value as the highest priority element. We can also set priorities according to our needs. In a queue, the first-in-first-out rule is implemented whereas, in a priority queue, the values are removed on the basis of priority. The element with the highest priority is removed first. Priority queue can be implemented using an array, a linked list, a heap data structure, or a binary search tree. Among these data structures, heap data structure provides an efficient implementation of priority queues.  Hence, we will be using the heap data structure to implement the priority queue in this tutorial. A max-heap is implemented in the following operations. If you want to learn more about it, please visit max-heap and min-heap. A comparative analysis of different implementations of priority queue is given below. Basic operations of a priority queue are inserting, removing, and peeking elements. Before studying the priority queue, please refer to the heap data structure for a better understanding of binary heap as it is used to implement the priority queue in this article. Inserting an element into a priority queue (max-heap) is done by the following steps. Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue
		
	
	Heapify the tree.
		
			Heapify after insertion Insert the new element at the end of the tree.
		
			Insert an element at the end of the queue Heapify the tree.
		
			Heapify after insertion Algorithm for insertion of an element into priority queue (max-heap) For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Deleting an element from a priority queue (max-heap) is done as follows: Select the element to be deleted.
		
			Select the element to be deleted
		
	
	Swap it with the last element.
		
			Swap with the last leaf node element
		
	
	Remove the last element.
		
			Remove the last element leaf
		
	
	Heapify the tree.
		
			Heapify the priority queue Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last leaf node element Remove the last element.
		
			Remove the last element leaf Heapify the tree.
		
			Heapify the priority queue Algorithm for deletion of an element in the priority queue (max-heap) For Min Heap, the above algorithm is modified so that the both childNodes are smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum value after removing it from Min Heap. Some of the applications of a priority queue are: Dijkstra's algorithm
	for implementing stack
	for load balancing and interrupt handling in an operating system
	for data compression in Huffman code Dijkstra's algorithm for implementing stack for load balancing and interrupt handling in an operating system for data compression in Huffman code","// Priority Queue implementation in C++

#include <iostream>
#include <vector>
using namespace std;

// Function to swap position of two elements
void swap(int *a, int *b) {
  int temp = *b;
  *b = *a;
  *a = temp;
}

// Function to heapify the tree
void heapify(vector<int> &hT, int i) {
  int size = hT.size();
  
  // Find the largest among root, left child and right child
  int largest = i;
  int l = 2 * i + 1;
  int r = 2 * i + 2;
  if (l < size && hT[l] > hT[largest])
    largest = l;
  if (r < size && hT[r] > hT[largest])
    largest = r;

  // Swap and continue heapifying if root is not largest
  if (largest != i) {
    swap(&hT[i], &hT[largest]);
    heapify(hT, largest);
  }
}

// Function to insert an element into the tree
void insert(vector<int> &hT, int newNum) {
  int size = hT.size();
  if (size == 0) {
    hT.push_back(newNum);
  } else {
    hT.push_back(newNum);
    for (int i = size / 2 - 1; i >= 0; i--) {
      heapify(hT, i);
    }
  }
}

// Function to delete an element from the tree
void deleteNode(vector<int> &hT, int num) {
  int size = hT.size();
  int i;
  for (i = 0; i < size; i++) {
    if (num == hT[i])
      break;
  }
  swap(&hT[i], &hT[size - 1]);

  hT.pop_back();
  for (int i = size / 2 - 1; i >= 0; i--) {
    heapify(hT, i);
  }
}

// Print the tree
void printArray(vector<int> &hT) {
  for (int i = 0; i < hT.size(); ++i)
    cout << hT[i] << "" "";
  cout << ""\n"";
}

// Driver code
int main() {
  vector<int> heapTree;

  insert(heapTree, 3);
  insert(heapTree, 4);
  insert(heapTree, 9);
  insert(heapTree, 5);
  insert(heapTree, 2);

  cout << ""Max-Heap array: "";
  printArray(heapTree);

  deleteNode(heapTree, 4);

  cout << ""After deleting an element: "";

  printArray(heapTree);
}"
Deque Data Structure,"Deque or Double Ended Queue is a type of queue in which insertion and removal of elements can either be performed from the front or the rear. Thus, it does not follow FIFO rule (First In First Out). Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends.
	Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends. Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Below is the circular array implementation of deque. In a circular array, if the array is full, we start from the beginning. But in a linear array implementation, if the array is full, no more elements can be inserted. In each of the operations below, if the array is full, ""overflow message"" is thrown. Before performing the following operations, these steps are followed. Take an array (deque) of size n. Set two pointers at the first position and set front = -1 and rear = 0. This operation adds an element at the front. Check the position of front.
		
			Check the position of front If front < 1, reinitialize front = n-1 (last index).
		
			Shift front to the end Else, decrease front by 1. Add the new key 5 into array[front].
		
			Insert the element at Front This operation adds an element to the rear. Check if the array is full.
		
			Check if deque is full If the deque is full, reinitialize rear = 0. Else, increase rear by 1.
		
			Increase the rear Add the new key 5 into array[rear].
		
			Insert the element at rear  The operation deletes an element from the front. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1. Else if front is at the end (i.e. front = n - 1), set go to the front front = 0. Else, front = front + 1.
		
			Increase the front This operation deletes an element from the rear. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1, else follow the steps below. If rear is at the front (i.e. rear = 0), set go to the front rear = n - 1. Else, rear = rear - 1.
		
			Decrease the rear This operation checks if the deque is empty. If front = -1, the deque is empty. This operation checks if the deque is full. If front = 0 and rear = n - 1 OR front = rear + 1, the deque is full. The time complexity of all the above operations is constant i.e. O(1). In undo operations on software. To store history in browsers. For implementing both stacks and queues.","# Deque implementaion in python

class Deque:
    def __init__(self):
        self.items = []

    def isEmpty(self):
        return self.items == []

    def addRear(self, item):
        self.items.append(item)

    def addFront(self, item):
        self.items.insert(0, item)

    def removeFront(self):
        return self.items.pop(0)

    def removeRear(self):
        return self.items.pop()

    def size(self):
        return len(self.items)


d = Deque()
print(d.isEmpty())
d.addRear(8)
d.addRear(5)
d.addFront(7)
d.addFront(10)
print(d.size())
print(d.isEmpty())
d.addRear(11)
print(d.removeRear())
print(d.removeFront())
d.addFront(55)
d.addRear(45)
print(d.items)"
Deque Data Structure,"Deque or Double Ended Queue is a type of queue in which insertion and removal of elements can either be performed from the front or the rear. Thus, it does not follow FIFO rule (First In First Out). Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends.
	Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends. Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Below is the circular array implementation of deque. In a circular array, if the array is full, we start from the beginning. But in a linear array implementation, if the array is full, no more elements can be inserted. In each of the operations below, if the array is full, ""overflow message"" is thrown. Before performing the following operations, these steps are followed. Take an array (deque) of size n. Set two pointers at the first position and set front = -1 and rear = 0. This operation adds an element at the front. Check the position of front.
		
			Check the position of front If front < 1, reinitialize front = n-1 (last index).
		
			Shift front to the end Else, decrease front by 1. Add the new key 5 into array[front].
		
			Insert the element at Front This operation adds an element to the rear. Check if the array is full.
		
			Check if deque is full If the deque is full, reinitialize rear = 0. Else, increase rear by 1.
		
			Increase the rear Add the new key 5 into array[rear].
		
			Insert the element at rear  The operation deletes an element from the front. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1. Else if front is at the end (i.e. front = n - 1), set go to the front front = 0. Else, front = front + 1.
		
			Increase the front This operation deletes an element from the rear. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1, else follow the steps below. If rear is at the front (i.e. rear = 0), set go to the front rear = n - 1. Else, rear = rear - 1.
		
			Decrease the rear This operation checks if the deque is empty. If front = -1, the deque is empty. This operation checks if the deque is full. If front = 0 and rear = n - 1 OR front = rear + 1, the deque is full. The time complexity of all the above operations is constant i.e. O(1). In undo operations on software. To store history in browsers. For implementing both stacks and queues.","// Deque implementation in Java

class Deque {
  static final int MAX = 100;
  int arr[];
  int front;
  int rear;
  int size;

  public Deque(int size) {
    arr = new int[MAX];
    front = -1;
    rear = 0;
    this.size = size;
  }

  boolean isFull() {
    return ((front == 0 && rear == size - 1) || front == rear + 1);
  }

  boolean isEmpty() {
    return (front == -1);
  }

  void insertfront(int key) {
    if (isFull()) {
      System.out.println(""Overflow"");
      return;
    }

    if (front == -1) {
      front = 0;
      rear = 0;
    }

    else if (front == 0)
      front = size - 1;

    else
      front = front - 1;

    arr[front] = key;
  }

  void insertrear(int key) {
    if (isFull()) {
      System.out.println("" Overflow "");
      return;
    }

    if (front == -1) {
      front = 0;
      rear = 0;
    }

    else if (rear == size - 1)
      rear = 0;

    else
      rear = rear + 1;

    arr[rear] = key;
  }

  void deletefront() {
    if (isEmpty()) {
      System.out.println(""Queue Underflow\n"");
      return;
    }

    // Deque has only one element
    if (front == rear) {
      front = -1;
      rear = -1;
    } else if (front == size - 1)
      front = 0;

    else
      front = front + 1;
  }

  void deleterear() {
    if (isEmpty()) {
      System.out.println("" Underflow"");
      return;
    }

    if (front == rear) {
      front = -1;
      rear = -1;
    } else if (rear == 0)
      rear = size - 1;
    else
      rear = rear - 1;
  }

  int getFront() {
    if (isEmpty()) {
      System.out.println("" Underflow"");
      return -1;
    }
    return arr[front];
  }

  int getRear() {
    if (isEmpty() || rear < 0) {
      System.out.println("" Underflow\n"");
      return -1;
    }
    return arr[rear];
  }

  public static void main(String[] args) {

    Deque dq = new Deque(4);

    System.out.println(""Insert element at rear end : 12 "");
    dq.insertrear(12);

    System.out.println(""insert element at rear end : 14 "");
    dq.insertrear(14);

    System.out.println(""get rear element : "" + dq.getRear());

    dq.deleterear();
    System.out.println(""After delete rear element new rear become : "" + dq.getRear());

    System.out.println(""inserting element at front end"");
    dq.insertfront(13);

    System.out.println(""get front element: "" + dq.getFront());

    dq.deletefront();

    System.out.println(""After delete front element new front become : "" + +dq.getFront());

  }
}"
Deque Data Structure,"Deque or Double Ended Queue is a type of queue in which insertion and removal of elements can either be performed from the front or the rear. Thus, it does not follow FIFO rule (First In First Out). Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends.
	Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends. Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Below is the circular array implementation of deque. In a circular array, if the array is full, we start from the beginning. But in a linear array implementation, if the array is full, no more elements can be inserted. In each of the operations below, if the array is full, ""overflow message"" is thrown. Before performing the following operations, these steps are followed. Take an array (deque) of size n. Set two pointers at the first position and set front = -1 and rear = 0. This operation adds an element at the front. Check the position of front.
		
			Check the position of front If front < 1, reinitialize front = n-1 (last index).
		
			Shift front to the end Else, decrease front by 1. Add the new key 5 into array[front].
		
			Insert the element at Front This operation adds an element to the rear. Check if the array is full.
		
			Check if deque is full If the deque is full, reinitialize rear = 0. Else, increase rear by 1.
		
			Increase the rear Add the new key 5 into array[rear].
		
			Insert the element at rear  The operation deletes an element from the front. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1. Else if front is at the end (i.e. front = n - 1), set go to the front front = 0. Else, front = front + 1.
		
			Increase the front This operation deletes an element from the rear. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1, else follow the steps below. If rear is at the front (i.e. rear = 0), set go to the front rear = n - 1. Else, rear = rear - 1.
		
			Decrease the rear This operation checks if the deque is empty. If front = -1, the deque is empty. This operation checks if the deque is full. If front = 0 and rear = n - 1 OR front = rear + 1, the deque is full. The time complexity of all the above operations is constant i.e. O(1). In undo operations on software. To store history in browsers. For implementing both stacks and queues.","// Deque implementation in C

#include <stdio.h>

#define MAX 10

void addFront(int *, int, int *, int *);
void addRear(int *, int, int *, int *);
int delFront(int *, int *, int *);
int delRear(int *, int *, int *);
void display(int *);
int count(int *);

int main() {
  int arr[MAX];
  int front, rear, i, n;

  front = rear = -1;
  for (i = 0; i < MAX; i++)
    arr[i] = 0;

  addRear(arr, 5, &front, &rear);
  addFront(arr, 12, &front, &rear);
  addRear(arr, 11, &front, &rear);
  addFront(arr, 5, &front, &rear);
  addRear(arr, 6, &front, &rear);
  addFront(arr, 8, &front, &rear);

  printf(""\nElements in a deque: "");
  display(arr);

  i = delFront(arr, &front, &rear);
  printf(""\nremoved item: %d"", i);

  printf(""\nElements in a deque after deletion: "");
  display(arr);

  addRear(arr, 16, &front, &rear);
  addRear(arr, 7, &front, &rear);

  printf(""\nElements in a deque after addition: "");
  display(arr);

  i = delRear(arr, &front, &rear);
  printf(""\nremoved item: %d"", i);

  printf(""\nElements in a deque after deletion: "");
  display(arr);

  n = count(arr);
  printf(""\nTotal number of elements in deque: %d"", n);
}

void addFront(int *arr, int item, int *pfront, int *prear) {
  int i, k, c;

  if (*pfront == 0 && *prear == MAX - 1) {
    printf(""\nDeque is full.\n"");
    return;
  }

  if (*pfront == -1) {
    *pfront = *prear = 0;
    arr[*pfront] = item;
    return;
  }

  if (*prear != MAX - 1) {
    c = count(arr);
    k = *prear + 1;
    for (i = 1; i <= c; i++) {
      arr[k] = arr[k - 1];
      k--;
    }
    arr[k] = item;
    *pfront = k;
    (*prear)++;
  } else {
    (*pfront)--;
    arr[*pfront] = item;
  }
}

void addRear(int *arr, int item, int *pfront, int *prear) {
  int i, k;

  if (*pfront == 0 && *prear == MAX - 1) {
    printf(""\nDeque is full.\n"");
    return;
  }

  if (*pfront == -1) {
    *prear = *pfront = 0;
    arr[*prear] = item;
    return;
  }

  if (*prear == MAX - 1) {
    k = *pfront - 1;
    for (i = *pfront - 1; i < *prear; i++) {
      k = i;
      if (k == MAX - 1)
        arr[k] = 0;
      else
        arr[k] = arr[i + 1];
    }
    (*prear)--;
    (*pfront)--;
  }
  (*prear)++;
  arr[*prear] = item;
}

int delFront(int *arr, int *pfront, int *prear) {
  int item;

  if (*pfront == -1) {
    printf(""\nDeque is empty.\n"");
    return 0;
  }

  item = arr[*pfront];
  arr[*pfront] = 0;

  if (*pfront == *prear)
    *pfront = *prear = -1;
  else
    (*pfront)++;

  return item;
}

int delRear(int *arr, int *pfront, int *prear) {
  int item;

  if (*pfront == -1) {
    printf(""\nDeque is empty.\n"");
    return 0;
  }

  item = arr[*prear];
  arr[*prear] = 0;
  (*prear)--;
  if (*prear == -1)
    *pfront = -1;
  return item;
}

void display(int *arr) {
  int i;

  printf(""\n front:  "");
  for (i = 0; i < MAX; i++)
    printf(""  %d"", arr[i]);
  printf(""  :rear"");
}

int count(int *arr) {
  int c = 0, i;

  for (i = 0; i < MAX; i++) {
    if (arr[i] != 0)
      c++;
  }
  return c;
}"
Deque Data Structure,"Deque or Double Ended Queue is a type of queue in which insertion and removal of elements can either be performed from the front or the rear. Thus, it does not follow FIFO rule (First In First Out). Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends.
	Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Input Restricted Deque
		In this deque, input is restricted at a single end but allows deletion at both the ends. Output Restricted Deque
		In this deque, output is restricted at a single end but allows insertion at both the ends. Below is the circular array implementation of deque. In a circular array, if the array is full, we start from the beginning. But in a linear array implementation, if the array is full, no more elements can be inserted. In each of the operations below, if the array is full, ""overflow message"" is thrown. Before performing the following operations, these steps are followed. Take an array (deque) of size n. Set two pointers at the first position and set front = -1 and rear = 0. This operation adds an element at the front. Check the position of front.
		
			Check the position of front If front < 1, reinitialize front = n-1 (last index).
		
			Shift front to the end Else, decrease front by 1. Add the new key 5 into array[front].
		
			Insert the element at Front This operation adds an element to the rear. Check if the array is full.
		
			Check if deque is full If the deque is full, reinitialize rear = 0. Else, increase rear by 1.
		
			Increase the rear Add the new key 5 into array[rear].
		
			Insert the element at rear  The operation deletes an element from the front. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1. Else if front is at the end (i.e. front = n - 1), set go to the front front = 0. Else, front = front + 1.
		
			Increase the front This operation deletes an element from the rear. Check if the deque is empty.
		
			Check if deque is empty If the deque is empty (i.e. front = -1), deletion cannot be performed (underflow condition). If the deque has only one element (i.e. front = rear), set front = -1 and rear = -1, else follow the steps below. If rear is at the front (i.e. rear = 0), set go to the front rear = n - 1. Else, rear = rear - 1.
		
			Decrease the rear This operation checks if the deque is empty. If front = -1, the deque is empty. This operation checks if the deque is full. If front = 0 and rear = n - 1 OR front = rear + 1, the deque is full. The time complexity of all the above operations is constant i.e. O(1). In undo operations on software. To store history in browsers. For implementing both stacks and queues.","// Deque implementation in C++

#include <iostream>
using namespace std;

#define MAX 10

class Deque {
  int arr[MAX];
  int front;
  int rear;
  int size;

   public:
  Deque(int size) {
    front = -1;
    rear = 0;
    this->size = size;
  }

  void insertfront(int key);
  void insertrear(int key);
  void deletefront();
  void deleterear();
  bool isFull();
  bool isEmpty();
  int getFront();
  int getRear();
};

bool Deque::isFull() {
  return ((front == 0 && rear == size - 1) ||
      front == rear + 1);
}

bool Deque::isEmpty() {
  return (front == -1);
}

void Deque::insertfront(int key) {
  if (isFull()) {
    cout << ""Overflow\n""
       << endl;
    return;
  }

  if (front == -1) {
    front = 0;
    rear = 0;
  }

  else if (front == 0)
    front = size - 1;

  else
    front = front - 1;

  arr[front] = key;
}

void Deque ::insertrear(int key) {
  if (isFull()) {
    cout << "" Overflow\n "" << endl;
    return;
  }

  if (front == -1) {
    front = 0;
    rear = 0;
  }

  else if (rear == size - 1)
    rear = 0;

  else
    rear = rear + 1;

  arr[rear] = key;
}

void Deque ::deletefront() {
  if (isEmpty()) {
    cout << ""Queue Underflow\n""
       << endl;
    return;
  }

  if (front == rear) {
    front = -1;
    rear = -1;
  } else if (front == size - 1)
    front = 0;

  else
    front = front + 1;
}

void Deque::deleterear() {
  if (isEmpty()) {
    cout << "" Underflow\n""
       << endl;
    return;
  }

  if (front == rear) {
    front = -1;
    rear = -1;
  } else if (rear == 0)
    rear = size - 1;
  else
    rear = rear - 1;
}

int Deque::getFront() {
  if (isEmpty()) {
    cout << "" Underflow\n""
       << endl;
    return -1;
  }
  return arr[front];
}

int Deque::getRear() {
  if (isEmpty() || rear < 0) {
    cout << "" Underflow\n""
       << endl;
    return -1;
  }
  return arr[rear];
}

int main() {
  Deque dq(4);

  cout << ""insert element at rear end \n"";
  dq.insertrear(5);
  dq.insertrear(11);

  cout << ""rear element: ""
     << dq.getRear() << endl;

  dq.deleterear();
  cout << ""after deletion of the rear element, the new rear element: "" << dq.getRear() << endl;

  cout << ""insert element at front end \n"";

  dq.insertfront(8);

  cout << ""front element: "" << dq.getFront() << endl;

  dq.deletefront();

  cout << ""after deletion of front element new front element: "" << dq.getFront() << endl;
}"
Linked List Data Structure,"A linked list is a linear data structure that includes a series of connected nodes. Here, each node stores the data and the address of the next node. For example, You have to start somewhere, so we give the address of the first node a special name called HEAD. Also, the last node in the linked list can be identified because its next portion points to NULL. Linked lists can be of multiple types: singly, doubly, and circular linked list. In this article, we will focus on the singly linked list. To learn about other types, visit Types of Linked List. Note: You might have played the game Treasure Hunt, where each clue includes the information about the next clue. That is how the linked list operates. Let's see how each node of the linked list is represented. Each node consists: A data item
	An address of another node A data item An address of another node We wrap both the data item and the next node reference in a struct as: Understanding the structure of a linked list node is the key to having a grasp on it.  Each struct node has a data item and a pointer to another struct node. Let us create a simple Linked List with three items to understand how this works. If you didn't understand any of the lines above, all you need is a refresher on pointers and structs. In just a few steps, we have created a simple linked list with three nodes. The power of a linked list comes from the ability to break the chain and rejoin it. E.g. if you wanted to put an element 4 between 1 and 2, the steps would be: Create a new struct node and allocate memory to it.
	Add its data value as 4
	Point its next pointer to the struct node containing 2 as the data value
	Change the next pointer of ""1"" to the node we just created. Create a new struct node and allocate memory to it. Add its data value as 4 Point its next pointer to the struct node containing 2 as the data value Change the next pointer of ""1"" to the node we just created. Doing something similar in an array would have required shifting the positions of all the subsequent elements. In python and Java, the linked list can be implemented using classes as shown in the codes below. Lists are one of the most popular and efficient data structures, with implementation in every programming language like C, C++, Python, Java, and C#. Apart from that, linked lists are a great way to learn how pointers work. By practicing how to manipulate linked lists, you can prepare yourself to learn more advanced data structures like graphs and trees. Time Complexity Space Complexity: O(n) Dynamic memory allocation
	Implemented in stack and queue
	In undo functionality of softwares
	Hash tables, Graphs Dynamic memory allocation Implemented in stack and queue In undo functionality of softwares Hash tables, Graphs Linked List Operations (Traverse, Insert, Delete)
	Types of Linked List
	Java LinkedList Linked List Operations (Traverse, Insert, Delete) Types of Linked List Java LinkedList Get the middle element of Linked List in a single iteration
	Convert the Linked List into an Array and vice versa
	Detect loop in a Linked List Get the middle element of Linked List in a single iteration Convert the Linked List into an Array and vice versa Detect loop in a Linked List","# Linked list implementation in Python


class Node:
    # Creating a node
    def __init__(self, item):
        self.item = item
        self.next = None


class LinkedList:

    def __init__(self):
        self.head = None


if __name__ == '__main__':

    linked_list = LinkedList()

    # Assign item values
    linked_list.head = Node(1)
    second = Node(2)
    third = Node(3)

    # Connect nodes
    linked_list.head.next = second
    second.next = third

    # Print the linked list item
    while linked_list.head != None:
        print(linked_list.head.item, end="" "")
        linked_list.head = linked_list.head.next
"
Linked List Data Structure,"A linked list is a linear data structure that includes a series of connected nodes. Here, each node stores the data and the address of the next node. For example, You have to start somewhere, so we give the address of the first node a special name called HEAD. Also, the last node in the linked list can be identified because its next portion points to NULL. Linked lists can be of multiple types: singly, doubly, and circular linked list. In this article, we will focus on the singly linked list. To learn about other types, visit Types of Linked List. Note: You might have played the game Treasure Hunt, where each clue includes the information about the next clue. That is how the linked list operates. Let's see how each node of the linked list is represented. Each node consists: A data item
	An address of another node A data item An address of another node We wrap both the data item and the next node reference in a struct as: Understanding the structure of a linked list node is the key to having a grasp on it.  Each struct node has a data item and a pointer to another struct node. Let us create a simple Linked List with three items to understand how this works. If you didn't understand any of the lines above, all you need is a refresher on pointers and structs. In just a few steps, we have created a simple linked list with three nodes. The power of a linked list comes from the ability to break the chain and rejoin it. E.g. if you wanted to put an element 4 between 1 and 2, the steps would be: Create a new struct node and allocate memory to it.
	Add its data value as 4
	Point its next pointer to the struct node containing 2 as the data value
	Change the next pointer of ""1"" to the node we just created. Create a new struct node and allocate memory to it. Add its data value as 4 Point its next pointer to the struct node containing 2 as the data value Change the next pointer of ""1"" to the node we just created. Doing something similar in an array would have required shifting the positions of all the subsequent elements. In python and Java, the linked list can be implemented using classes as shown in the codes below. Lists are one of the most popular and efficient data structures, with implementation in every programming language like C, C++, Python, Java, and C#. Apart from that, linked lists are a great way to learn how pointers work. By practicing how to manipulate linked lists, you can prepare yourself to learn more advanced data structures like graphs and trees. Time Complexity Space Complexity: O(n) Dynamic memory allocation
	Implemented in stack and queue
	In undo functionality of softwares
	Hash tables, Graphs Dynamic memory allocation Implemented in stack and queue In undo functionality of softwares Hash tables, Graphs Linked List Operations (Traverse, Insert, Delete)
	Types of Linked List
	Java LinkedList Linked List Operations (Traverse, Insert, Delete) Types of Linked List Java LinkedList Get the middle element of Linked List in a single iteration
	Convert the Linked List into an Array and vice versa
	Detect loop in a Linked List Get the middle element of Linked List in a single iteration Convert the Linked List into an Array and vice versa Detect loop in a Linked List","// Linked list implementation in Java

class LinkedList {
  // Creating a node
  Node head;

  static class Node {
    int value;
    Node next;

    Node(int d) {
      value = d;
      next = null;
    }
  }

  public static void main(String[] args) {
    LinkedList linkedList = new LinkedList();

    // Assign value values
    linkedList.head = new Node(1);
    Node second = new Node(2);
    Node third = new Node(3);

    // Connect nodess
    linkedList.head.next = second;
    second.next = third;

    // printing node-value
    while (linkedList.head != null) {
      System.out.print(linkedList.head.value + "" "");
      linkedList.head = linkedList.head.next;
    }
  }
}"
Linked List Data Structure,"A linked list is a linear data structure that includes a series of connected nodes. Here, each node stores the data and the address of the next node. For example, You have to start somewhere, so we give the address of the first node a special name called HEAD. Also, the last node in the linked list can be identified because its next portion points to NULL. Linked lists can be of multiple types: singly, doubly, and circular linked list. In this article, we will focus on the singly linked list. To learn about other types, visit Types of Linked List. Note: You might have played the game Treasure Hunt, where each clue includes the information about the next clue. That is how the linked list operates. Let's see how each node of the linked list is represented. Each node consists: A data item
	An address of another node A data item An address of another node We wrap both the data item and the next node reference in a struct as: Understanding the structure of a linked list node is the key to having a grasp on it.  Each struct node has a data item and a pointer to another struct node. Let us create a simple Linked List with three items to understand how this works. If you didn't understand any of the lines above, all you need is a refresher on pointers and structs. In just a few steps, we have created a simple linked list with three nodes. The power of a linked list comes from the ability to break the chain and rejoin it. E.g. if you wanted to put an element 4 between 1 and 2, the steps would be: Create a new struct node and allocate memory to it.
	Add its data value as 4
	Point its next pointer to the struct node containing 2 as the data value
	Change the next pointer of ""1"" to the node we just created. Create a new struct node and allocate memory to it. Add its data value as 4 Point its next pointer to the struct node containing 2 as the data value Change the next pointer of ""1"" to the node we just created. Doing something similar in an array would have required shifting the positions of all the subsequent elements. In python and Java, the linked list can be implemented using classes as shown in the codes below. Lists are one of the most popular and efficient data structures, with implementation in every programming language like C, C++, Python, Java, and C#. Apart from that, linked lists are a great way to learn how pointers work. By practicing how to manipulate linked lists, you can prepare yourself to learn more advanced data structures like graphs and trees. Time Complexity Space Complexity: O(n) Dynamic memory allocation
	Implemented in stack and queue
	In undo functionality of softwares
	Hash tables, Graphs Dynamic memory allocation Implemented in stack and queue In undo functionality of softwares Hash tables, Graphs Linked List Operations (Traverse, Insert, Delete)
	Types of Linked List
	Java LinkedList Linked List Operations (Traverse, Insert, Delete) Types of Linked List Java LinkedList Get the middle element of Linked List in a single iteration
	Convert the Linked List into an Array and vice versa
	Detect loop in a Linked List Get the middle element of Linked List in a single iteration Convert the Linked List into an Array and vice versa Detect loop in a Linked List","// Linked list implementation in C

#include <stdio.h>
#include <stdlib.h>

// Creating a node
struct node {
  int value;
  struct node *next;
};

// print the linked list value
void printLinkedlist(struct node *p) {
  while (p != NULL) {
    printf(""%d "", p->value);
    p = p->next;
  }
}

int main() {
  // Initialize nodes
  struct node *head;
  struct node *one = NULL;
  struct node *two = NULL;
  struct node *three = NULL;

  // Allocate memory
  one = malloc(sizeof(struct node));
  two = malloc(sizeof(struct node));
  three = malloc(sizeof(struct node));

  // Assign value values
  one->value = 1;
  two->value = 2;
  three->value = 3;

  // Connect nodes
  one->next = two;
  two->next = three;
  three->next = NULL;

  // printing node-value
  head = one;
  printLinkedlist(head);
}"
Linked List Data Structure,"A linked list is a linear data structure that includes a series of connected nodes. Here, each node stores the data and the address of the next node. For example, You have to start somewhere, so we give the address of the first node a special name called HEAD. Also, the last node in the linked list can be identified because its next portion points to NULL. Linked lists can be of multiple types: singly, doubly, and circular linked list. In this article, we will focus on the singly linked list. To learn about other types, visit Types of Linked List. Note: You might have played the game Treasure Hunt, where each clue includes the information about the next clue. That is how the linked list operates. Let's see how each node of the linked list is represented. Each node consists: A data item
	An address of another node A data item An address of another node We wrap both the data item and the next node reference in a struct as: Understanding the structure of a linked list node is the key to having a grasp on it.  Each struct node has a data item and a pointer to another struct node. Let us create a simple Linked List with three items to understand how this works. If you didn't understand any of the lines above, all you need is a refresher on pointers and structs. In just a few steps, we have created a simple linked list with three nodes. The power of a linked list comes from the ability to break the chain and rejoin it. E.g. if you wanted to put an element 4 between 1 and 2, the steps would be: Create a new struct node and allocate memory to it.
	Add its data value as 4
	Point its next pointer to the struct node containing 2 as the data value
	Change the next pointer of ""1"" to the node we just created. Create a new struct node and allocate memory to it. Add its data value as 4 Point its next pointer to the struct node containing 2 as the data value Change the next pointer of ""1"" to the node we just created. Doing something similar in an array would have required shifting the positions of all the subsequent elements. In python and Java, the linked list can be implemented using classes as shown in the codes below. Lists are one of the most popular and efficient data structures, with implementation in every programming language like C, C++, Python, Java, and C#. Apart from that, linked lists are a great way to learn how pointers work. By practicing how to manipulate linked lists, you can prepare yourself to learn more advanced data structures like graphs and trees. Time Complexity Space Complexity: O(n) Dynamic memory allocation
	Implemented in stack and queue
	In undo functionality of softwares
	Hash tables, Graphs Dynamic memory allocation Implemented in stack and queue In undo functionality of softwares Hash tables, Graphs Linked List Operations (Traverse, Insert, Delete)
	Types of Linked List
	Java LinkedList Linked List Operations (Traverse, Insert, Delete) Types of Linked List Java LinkedList Get the middle element of Linked List in a single iteration
	Convert the Linked List into an Array and vice versa
	Detect loop in a Linked List Get the middle element of Linked List in a single iteration Convert the Linked List into an Array and vice versa Detect loop in a Linked List","// Linked list implementation in C++

#include <bits/stdc++.h>
#include <iostream>
using namespace std;

// Creating a node
class Node {
   public:
  int value;
  Node* next;
};

int main() {
  Node* head;
  Node* one = NULL;
  Node* two = NULL;
  Node* three = NULL;

  // allocate 3 nodes in the heap
  one = new Node();
  two = new Node();
  three = new Node();

  // Assign value values
  one->value = 1;
  two->value = 2;
  three->value = 3;

  // Connect nodes
  one->next = two;
  two->next = three;
  three->next = NULL;

  // print the linked list value
  head = one;
  while (head != NULL) {
    cout << head->value;
    head = head->next;
  }
}"
"Linked List Operations: Traverse, Insert and Delete","There are various linked list operations that allow us to perform different actions on linked lists. For example, the insertion operation adds a new element to the linked list. Here's a list of basic linked list operations that we will cover in this article. Traversal - access each element of the linked list
	Insertion - adds a new element to the linked list
	Deletion - removes the existing elements
	Search - find a node in the linked list
	Sort - sort the nodes of the linked list Traversal - access each element of the linked list Insertion - adds a new element to the linked list Deletion - removes the existing elements Search - find a node in the linked list Sort - sort the nodes of the linked list Before you learn about linked list operations in detail, make sure to know about Linked List first. head points to the first node of the linked list
	next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. head points to the first node of the linked list next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. In all of the examples, we will assume that the linked list has three nodes 1 --->2 --->3 with node structure as below: Displaying the contents of a linked list is very simple. We keep moving the temp node to the next one and display its contents. When temp is NULL, we know that we have reached the end of the linked list so we get out of the while loop. The output of this program will be:  You can add elements to either the beginning, middle or end of the linked list. Allocate memory for new node
	Store data
	Change next of new node to point to head
	Change head to point to recently created node Allocate memory for new node Store data Change next of new node to point to head Change head to point to recently created node Allocate memory for new node
	Store data
	Traverse to last node
	Change next of last node to recently created node Allocate memory for new node Store data Traverse to last node Change next of last node to recently created node Allocate memory and store data for new node
	Traverse to node just before the required position of new node
	Change next pointers to include new node in between Allocate memory and store data for new node Traverse to node just before the required position of new node Change next pointers to include new node in between You can delete either from the beginning, end or from a particular position. Point head to the second node Point head to the second node Traverse to second last element
	Change its next pointer to null Traverse to second last element Change its next pointer to null Traverse to element before the element to be deleted
	Change next pointers to exclude the node from the chain Traverse to element before the element to be deleted Change next pointers to exclude the node from the chain You can search an element on a linked list using a loop using the following steps. We are finding item on a linked list. Make head as the current node.
	Run a loop until the current node is NULL because the last element points to NULL.
	In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. Make head as the current node. Run a loop until the current node is NULL because the last element points to NULL. In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. We will use a simple sorting algorithm, Bubble Sort, to sort the elements of a linked list in ascending order below. Make the head as the current node and create another node index for later use. If head is null, return. Else, run a loop till the last node (i.e. NULL). In each iteration, follow the following step 5-6. Store the next node of current in index. Check if the data of the current node is greater than the next node. If it is greater, swap current and index. Check the article on bubble sort for better understanding of its working.","# Linked list operations in Python


# Create a node
class Node:
    def __init__(self, data):
        self.data = data
        self.next = None


class LinkedList:

    def __init__(self):
        self.head = None

    # Insert at the beginning
    def insertAtBeginning(self, new_data):
        new_node = Node(new_data)

        new_node.next = self.head
        self.head = new_node

    # Insert after a node
    def insertAfter(self, prev_node, new_data):

        if prev_node is None:
            print(""The given previous node must inLinkedList."")
            return

        new_node = Node(new_data)
        new_node.next = prev_node.next
        prev_node.next = new_node

    # Insert at the end
    def insertAtEnd(self, new_data):
        new_node = Node(new_data)

        if self.head is None:
            self.head = new_node
            return

        last = self.head
        while (last.next):
            last = last.next

        last.next = new_node

    # Deleting a node
    def deleteNode(self, position):

        if self.head is None:
            return

        temp = self.head

        if position == 0:
            self.head = temp.next
            temp = None
            return

        # Find the key to be deleted
        for i in range(position - 1):
            temp = temp.next
            if temp is None:
                break

        # If the key is not present
        if temp is None:
            return

        if temp.next is None:
            return

        next = temp.next.next

        temp.next = None

        temp.next = next

    # Search an element
    def search(self, key):

        current = self.head

        while current is not None:
            if current.data == key:
                return True

            current = current.next

        return False

    # Sort the linked list
    def sortLinkedList(self, head):
        current = head
        index = Node(None)

        if head is None:
            return
        else:
            while current is not None:
                # index points to the node next to current
                index = current.next

                while index is not None:
                    if current.data > index.data:
                        current.data, index.data = index.data, current.data

                    index = index.next
                current = current.next

    # Print the linked list
    def printList(self):
        temp = self.head
        while (temp):
            print(str(temp.data) + "" "", end="""")
            temp = temp.next


if __name__ == '__main__':

    llist = LinkedList()
    llist.insertAtEnd(1)
    llist.insertAtBeginning(2)
    llist.insertAtBeginning(3)
    llist.insertAtEnd(4)
    llist.insertAfter(llist.head.next, 5)

    print('linked list:')
    llist.printList()

    print(""\nAfter deleting an element:"")
    llist.deleteNode(3)
    llist.printList()

    print()
    item_to_find = 3
    if llist.search(item_to_find):
        print(str(item_to_find) + "" is found"")
    else:
        print(str(item_to_find) + "" is not found"")

    llist.sortLinkedList(llist.head)
    print(""Sorted List: "")
    llist.printList()"
"Linked List Operations: Traverse, Insert and Delete","There are various linked list operations that allow us to perform different actions on linked lists. For example, the insertion operation adds a new element to the linked list. Here's a list of basic linked list operations that we will cover in this article. Traversal - access each element of the linked list
	Insertion - adds a new element to the linked list
	Deletion - removes the existing elements
	Search - find a node in the linked list
	Sort - sort the nodes of the linked list Traversal - access each element of the linked list Insertion - adds a new element to the linked list Deletion - removes the existing elements Search - find a node in the linked list Sort - sort the nodes of the linked list Before you learn about linked list operations in detail, make sure to know about Linked List first. head points to the first node of the linked list
	next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. head points to the first node of the linked list next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. In all of the examples, we will assume that the linked list has three nodes 1 --->2 --->3 with node structure as below: Displaying the contents of a linked list is very simple. We keep moving the temp node to the next one and display its contents. When temp is NULL, we know that we have reached the end of the linked list so we get out of the while loop. The output of this program will be:  You can add elements to either the beginning, middle or end of the linked list. Allocate memory for new node
	Store data
	Change next of new node to point to head
	Change head to point to recently created node Allocate memory for new node Store data Change next of new node to point to head Change head to point to recently created node Allocate memory for new node
	Store data
	Traverse to last node
	Change next of last node to recently created node Allocate memory for new node Store data Traverse to last node Change next of last node to recently created node Allocate memory and store data for new node
	Traverse to node just before the required position of new node
	Change next pointers to include new node in between Allocate memory and store data for new node Traverse to node just before the required position of new node Change next pointers to include new node in between You can delete either from the beginning, end or from a particular position. Point head to the second node Point head to the second node Traverse to second last element
	Change its next pointer to null Traverse to second last element Change its next pointer to null Traverse to element before the element to be deleted
	Change next pointers to exclude the node from the chain Traverse to element before the element to be deleted Change next pointers to exclude the node from the chain You can search an element on a linked list using a loop using the following steps. We are finding item on a linked list. Make head as the current node.
	Run a loop until the current node is NULL because the last element points to NULL.
	In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. Make head as the current node. Run a loop until the current node is NULL because the last element points to NULL. In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. We will use a simple sorting algorithm, Bubble Sort, to sort the elements of a linked list in ascending order below. Make the head as the current node and create another node index for later use. If head is null, return. Else, run a loop till the last node (i.e. NULL). In each iteration, follow the following step 5-6. Store the next node of current in index. Check if the data of the current node is greater than the next node. If it is greater, swap current and index. Check the article on bubble sort for better understanding of its working.","// Linked list operations in Java

class LinkedList {
  Node head;

  // Create a node
  class Node {
    int data;
    Node next;

    Node(int d) {
      data = d;
      next = null;
    }
  }

  // Insert at the beginning
  public void insertAtBeginning(int new_data) {
    // insert the data
    Node new_node = new Node(new_data);
    new_node.next = head;
    head = new_node;
  }

  // Insert after a node
  public void insertAfter(Node prev_node, int new_data) {
    if (prev_node == null) {
      System.out.println(""The given previous node cannot be null"");
      return;
    }
    Node new_node = new Node(new_data);
    new_node.next = prev_node.next;
    prev_node.next = new_node;
  }

  // Insert at the end
  public void insertAtEnd(int new_data) {
    Node new_node = new Node(new_data);

    if (head == null) {
      head = new Node(new_data);
      return;
    }

    new_node.next = null;

    Node last = head;
    while (last.next != null)
      last = last.next;

    last.next = new_node;
    return;
  }

  // Delete a node
  void deleteNode(int position) {
    if (head == null)
      return;

    Node temp = head;

    if (position == 0) {
      head = temp.next;
      return;
    }
    // Find the key to be deleted
    for (int i = 0; temp != null && i < position - 1; i++)
      temp = temp.next;

    // If the key is not present
    if (temp == null || temp.next == null)
      return;

    // Remove the node
    Node next = temp.next.next;

    temp.next = next;
  }

  // Search a node
  boolean search(Node head, int key) {
    Node current = head;
    while (current != null) {
      if (current.data == key)
        return true;
      current = current.next;
    }
    return false;
  }

  // Sort the linked list
  void sortLinkedList(Node head) {
    Node current = head;
    Node index = null;
    int temp;

    if (head == null) {
      return;
    } else {
      while (current != null) {
        // index points to the node next to current
        index = current.next;

        while (index != null) {
          if (current.data > index.data) {
            temp = current.data;
            current.data = index.data;
            index.data = temp;
          }
          index = index.next;
        }
        current = current.next;
      }
    }
  }

  // Print the linked list
  public void printList() {
    Node tnode = head;
    while (tnode != null) {
      System.out.print(tnode.data + "" "");
      tnode = tnode.next;
    }

  }

  public static void main(String[] args) {
    LinkedList llist = new LinkedList();

    llist.insertAtEnd(1);
    llist.insertAtBeginning(2);
    llist.insertAtBeginning(3);
    llist.insertAtEnd(4);
    llist.insertAfter(llist.head.next, 5);

    System.out.println(""Linked list: "");
    llist.printList();

    System.out.println(""\nAfter deleting an element: "");
    llist.deleteNode(3);
    llist.printList();

    System.out.println();
    int item_to_find = 3;
    if (llist.search(llist.head, item_to_find))
      System.out.println(item_to_find + "" is found"");
    else
      System.out.println(item_to_find + "" is not found"");

    llist.sortLinkedList(llist.head);
    System.out.println(""\nSorted List: "");
    llist.printList();
  }
}"
"Linked List Operations: Traverse, Insert and Delete","There are various linked list operations that allow us to perform different actions on linked lists. For example, the insertion operation adds a new element to the linked list. Here's a list of basic linked list operations that we will cover in this article. Traversal - access each element of the linked list
	Insertion - adds a new element to the linked list
	Deletion - removes the existing elements
	Search - find a node in the linked list
	Sort - sort the nodes of the linked list Traversal - access each element of the linked list Insertion - adds a new element to the linked list Deletion - removes the existing elements Search - find a node in the linked list Sort - sort the nodes of the linked list Before you learn about linked list operations in detail, make sure to know about Linked List first. head points to the first node of the linked list
	next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. head points to the first node of the linked list next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. In all of the examples, we will assume that the linked list has three nodes 1 --->2 --->3 with node structure as below: Displaying the contents of a linked list is very simple. We keep moving the temp node to the next one and display its contents. When temp is NULL, we know that we have reached the end of the linked list so we get out of the while loop. The output of this program will be:  You can add elements to either the beginning, middle or end of the linked list. Allocate memory for new node
	Store data
	Change next of new node to point to head
	Change head to point to recently created node Allocate memory for new node Store data Change next of new node to point to head Change head to point to recently created node Allocate memory for new node
	Store data
	Traverse to last node
	Change next of last node to recently created node Allocate memory for new node Store data Traverse to last node Change next of last node to recently created node Allocate memory and store data for new node
	Traverse to node just before the required position of new node
	Change next pointers to include new node in between Allocate memory and store data for new node Traverse to node just before the required position of new node Change next pointers to include new node in between You can delete either from the beginning, end or from a particular position. Point head to the second node Point head to the second node Traverse to second last element
	Change its next pointer to null Traverse to second last element Change its next pointer to null Traverse to element before the element to be deleted
	Change next pointers to exclude the node from the chain Traverse to element before the element to be deleted Change next pointers to exclude the node from the chain You can search an element on a linked list using a loop using the following steps. We are finding item on a linked list. Make head as the current node.
	Run a loop until the current node is NULL because the last element points to NULL.
	In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. Make head as the current node. Run a loop until the current node is NULL because the last element points to NULL. In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. We will use a simple sorting algorithm, Bubble Sort, to sort the elements of a linked list in ascending order below. Make the head as the current node and create another node index for later use. If head is null, return. Else, run a loop till the last node (i.e. NULL). In each iteration, follow the following step 5-6. Store the next node of current in index. Check if the data of the current node is greater than the next node. If it is greater, swap current and index. Check the article on bubble sort for better understanding of its working.","// Linked list operations in C

#include <stdio.h>
#include <stdlib.h>

// Create a node
struct Node {
  int data;
  struct Node* next;
};

// Insert at the beginning
void insertAtBeginning(struct Node** head_ref, int new_data) {
  // Allocate memory to a node
  struct Node* new_node = (struct Node*)malloc(sizeof(struct Node));

  // insert the data
  new_node->data = new_data;

  new_node->next = (*head_ref);

  // Move head to new node
  (*head_ref) = new_node;
}

// Insert a node after a node
void insertAfter(struct Node* prev_node, int new_data) {
  if (prev_node == NULL) {
  printf(""the given previous node cannot be NULL"");
  return;
  }

  struct Node* new_node = (struct Node*)malloc(sizeof(struct Node));
  new_node->data = new_data;
  new_node->next = prev_node->next;
  prev_node->next = new_node;
}

// Insert the the end
void insertAtEnd(struct Node** head_ref, int new_data) {
  struct Node* new_node = (struct Node*)malloc(sizeof(struct Node));
  struct Node* last = *head_ref; /* used in step 5*/

  new_node->data = new_data;
  new_node->next = NULL;

  if (*head_ref == NULL) {
  *head_ref = new_node;
  return;
  }

  while (last->next != NULL) last = last->next;

  last->next = new_node;
  return;
}

// Delete a node
void deleteNode(struct Node** head_ref, int key) {
  struct Node *temp = *head_ref, *prev;

  if (temp != NULL && temp->data == key) {
  *head_ref = temp->next;
  free(temp);
  return;
  }
  // Find the key to be deleted
  while (temp != NULL && temp->data != key) {
  prev = temp;
  temp = temp->next;
  }

  // If the key is not present
  if (temp == NULL) return;

  // Remove the node
  prev->next = temp->next;

  free(temp);
}

// Search a node
int searchNode(struct Node** head_ref, int key) {
  struct Node* current = *head_ref;

  while (current != NULL) {
  if (current->data == key) return 1;
  current = current->next;
  }
  return 0;
}

// Sort the linked list
void sortLinkedList(struct Node** head_ref) {
  struct Node *current = *head_ref, *index = NULL;
  int temp;

  if (head_ref == NULL) {
  return;
  } else {
  while (current != NULL) {
    // index points to the node next to current
    index = current->next;

    while (index != NULL) {
    if (current->data > index->data) {
      temp = current->data;
      current->data = index->data;
      index->data = temp;
    }
    index = index->next;
    }
    current = current->next;
  }
  }
}

// Print the linked list
void printList(struct Node* node) {
  while (node != NULL) {
  printf("" %d "", node->data);
  node = node->next;
  }
}

// Driver program
int main() {
  struct Node* head = NULL;

  insertAtEnd(&head, 1);
  insertAtBeginning(&head, 2);
  insertAtBeginning(&head, 3);
  insertAtEnd(&head, 4);
  insertAfter(head->next, 5);

  printf(""Linked list: "");
  printList(head);

  printf(""\nAfter deleting an element: "");
  deleteNode(&head, 3);
  printList(head);

  int item_to_find = 3;
  if (searchNode(&head, item_to_find)) {
  printf(""\n%d is found"", item_to_find);
  } else {
  printf(""\n%d is not found"", item_to_find);
  }

  sortLinkedList(&head);
  printf(""\nSorted List: "");
  printList(head);
}"
"Linked List Operations: Traverse, Insert and Delete","There are various linked list operations that allow us to perform different actions on linked lists. For example, the insertion operation adds a new element to the linked list. Here's a list of basic linked list operations that we will cover in this article. Traversal - access each element of the linked list
	Insertion - adds a new element to the linked list
	Deletion - removes the existing elements
	Search - find a node in the linked list
	Sort - sort the nodes of the linked list Traversal - access each element of the linked list Insertion - adds a new element to the linked list Deletion - removes the existing elements Search - find a node in the linked list Sort - sort the nodes of the linked list Before you learn about linked list operations in detail, make sure to know about Linked List first. head points to the first node of the linked list
	next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. head points to the first node of the linked list next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list. In all of the examples, we will assume that the linked list has three nodes 1 --->2 --->3 with node structure as below: Displaying the contents of a linked list is very simple. We keep moving the temp node to the next one and display its contents. When temp is NULL, we know that we have reached the end of the linked list so we get out of the while loop. The output of this program will be:  You can add elements to either the beginning, middle or end of the linked list. Allocate memory for new node
	Store data
	Change next of new node to point to head
	Change head to point to recently created node Allocate memory for new node Store data Change next of new node to point to head Change head to point to recently created node Allocate memory for new node
	Store data
	Traverse to last node
	Change next of last node to recently created node Allocate memory for new node Store data Traverse to last node Change next of last node to recently created node Allocate memory and store data for new node
	Traverse to node just before the required position of new node
	Change next pointers to include new node in between Allocate memory and store data for new node Traverse to node just before the required position of new node Change next pointers to include new node in between You can delete either from the beginning, end or from a particular position. Point head to the second node Point head to the second node Traverse to second last element
	Change its next pointer to null Traverse to second last element Change its next pointer to null Traverse to element before the element to be deleted
	Change next pointers to exclude the node from the chain Traverse to element before the element to be deleted Change next pointers to exclude the node from the chain You can search an element on a linked list using a loop using the following steps. We are finding item on a linked list. Make head as the current node.
	Run a loop until the current node is NULL because the last element points to NULL.
	In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. Make head as the current node. Run a loop until the current node is NULL because the last element points to NULL. In each iteration, check if the key of the node is equal to item. If it the key matches the item, return true otherwise return false. We will use a simple sorting algorithm, Bubble Sort, to sort the elements of a linked list in ascending order below. Make the head as the current node and create another node index for later use. If head is null, return. Else, run a loop till the last node (i.e. NULL). In each iteration, follow the following step 5-6. Store the next node of current in index. Check if the data of the current node is greater than the next node. If it is greater, swap current and index. Check the article on bubble sort for better understanding of its working.","// Linked list operations in C++

#include <stdlib.h>

#include <iostream>
using namespace std;

// Create a node
struct Node {
  int data;
  struct Node* next;
};

void insertAtBeginning(struct Node** head_ref, int new_data) {
  // Allocate memory to a node
  struct Node* new_node = (struct Node*)malloc(sizeof(struct Node));

  // insert the data
  new_node->data = new_data;
  new_node->next = (*head_ref);

  // Move head to new node
  (*head_ref) = new_node;
}

// Insert a node after a node
void insertAfter(struct Node* prev_node, int new_data) {
  if (prev_node == NULL) {
  cout << ""the given previous node cannot be NULL"";
  return;
  }

  struct Node* new_node = (struct Node*)malloc(sizeof(struct Node));
  new_node->data = new_data;
  new_node->next = prev_node->next;
  prev_node->next = new_node;
}

// Insert at the end
void insertAtEnd(struct Node** head_ref, int new_data) {
  struct Node* new_node = (struct Node*)malloc(sizeof(struct Node));
  struct Node* last = *head_ref; /* used in step 5*/

  new_node->data = new_data;
  new_node->next = NULL;

  if (*head_ref == NULL) {
  *head_ref = new_node;
  return;
  }

  while (last->next != NULL) last = last->next;

  last->next = new_node;
  return;
}

// Delete a node
void deleteNode(struct Node** head_ref, int key) {
  struct Node *temp = *head_ref, *prev;

  if (temp != NULL && temp->data == key) {
  *head_ref = temp->next;
  free(temp);
  return;
  }
  // Find the key to be deleted
  while (temp != NULL && temp->data != key) {
  prev = temp;
  temp = temp->next;
  }

  // If the key is not present
  if (temp == NULL) return;

  // Remove the node
  prev->next = temp->next;

  free(temp);
}

// Search a node
bool searchNode(struct Node** head_ref, int key) {
  struct Node* current = *head_ref;

  while (current != NULL) {
  if (current->data == key) return true;
  current = current->next;
  }
  return false;
}

// Sort the linked list
void sortLinkedList(struct Node** head_ref) {
  struct Node *current = *head_ref, *index = NULL;
  int temp;

  if (head_ref == NULL) {
  return;
  } else {
  while (current != NULL) {
    // index points to the node next to current
    index = current->next;

    while (index != NULL) {
    if (current->data > index->data) {
      temp = current->data;
      current->data = index->data;
      index->data = temp;
    }
    index = index->next;
    }
    current = current->next;
  }
  }
}

// Print the linked list
void printList(struct Node* node) {
  while (node != NULL) {
  cout << node->data << "" "";
  node = node->next;
  }
}

// Driver program
int main() {
  struct Node* head = NULL;

  insertAtEnd(&head, 1);
  insertAtBeginning(&head, 2);
  insertAtBeginning(&head, 3);
  insertAtEnd(&head, 4);
  insertAfter(head->next, 5);

  cout << ""Linked list: "";
  printList(head);

  cout << ""\nAfter deleting an element: "";
  deleteNode(&head, 3);
  printList(head);

  int item_to_find = 3;
  if (searchNode(&head, item_to_find)) {
  cout << endl << item_to_find << "" is found"";
  } else {
  cout << endl << item_to_find << "" is not found"";
  }

  sortLinkedList(&head);
  cout << ""\nSorted List: "";
  printList(head);
}"
Hash Table Data Structure,"The Hash table data structure stores elements in key-value pairs where Key- unique integer that is used for indexing the values
	Value - data that are associated with keys. Key- unique integer that is used for indexing the values Value - data that are associated with keys. In a hash table, a new index is processed using the keys. And, the element corresponding to that key is stored in the index. This process is called hashing. Let k be a key and h(x) be a hash function. Here, h(k) will give us a new index to store the element linked with k. To learn more, visit Hashing. When the hash function generates the same index for multiple keys, there will be a conflict (what value to be stored in that index). This is called a hash collision. We can resolve the hash collision using one of the following techniques. Collision resolution by chaining
	Open Addressing: Linear/Quadratic Probing and Double Hashing Collision resolution by chaining Open Addressing: Linear/Quadratic Probing and Double Hashing In chaining, if a hash function produces the same index for multiple elements, these elements are stored in the same index by using a doubly-linked list. If j is the slot for multiple elements, it contains a pointer to the head of the list of elements. If no element is present, j contains NIL. Pseudocode for operations Unlike chaining, open addressing doesn't store multiple elements into the same slot. Here, each slot is either filled with a single key or left NIL. Different techniques used in open addressing are: In linear probing, collision is resolved by checking the next slot.  h(k, i) = (h′(k) + i) mod m where i = {0, 1, ….}
	h'(k) is a new hash function i = {0, 1, ….} h'(k) is a new hash function If a collision occurs at h(k, 0), then h(k, 1) is checked. In this way, the value of i is incremented linearly. The problem with linear probing is that a cluster of adjacent slots is filled. When inserting a new element, the entire cluster must be traversed. This adds to the time required to perform operations on the hash table. It works similar to linear probing but the spacing between the slots is increased (greater than one) by using the following relation. h(k, i) = (h′(k) + c1i + c2i2) mod m where, c1 and c2 are positive auxiliary constants,
	i = {0, 1, ….} c1 and c2 are positive auxiliary constants, i = {0, 1, ….} If a collision occurs after applying a hash function h(k), then another hash function is calculated for finding the next slot. h(k, i) = (h1(k) + ih2(k)) mod m A good hash function may not prevent the collisions completely however it can reduce the number of collisions. Here, we will look into different methods to find a good hash function If k is a key and m is the size of the hash table, the hash function h() is calculated as: h(k) = k mod m For example, If the size of a hash table is 10 and k = 112 then h(k) = 112 mod 10 = 2. The value of m must not be the powers of 2. This is because the powers of 2 in binary format are 10, 100, 1000, …. When we find k mod m, we will always get the lower order p-bits. h(k) = ⌊m(kA mod 1)⌋ where, kA mod 1 gives the fractional part kA,
	⌊ ⌋ gives the floor value
	A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. kA mod 1 gives the fractional part kA, ⌊ ⌋ gives the floor value A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. In Universal hashing, the hash function is chosen at random independent of keys. Hash tables are implemented where constant time lookup and insertion is required
	cryptographic applications
	indexing data is required constant time lookup and insertion is required cryptographic applications indexing data is required","# Python program to demonstrate working of HashTable 

hashTable = [[],] * 10

def checkPrime(n):
    if n == 1 or n == 0:
        return 0

    for i in range(2, n//2):
        if n % i == 0:
            return 0

    return 1


def getPrime(n):
    if n % 2 == 0:
        n = n + 1

    while not checkPrime(n):
        n += 2

    return n


def hashFunction(key):
    capacity = getPrime(10)
    return key % capacity


def insertData(key, data):
    index = hashFunction(key)
    hashTable[index] = [key, data]

def removeData(key):
    index = hashFunction(key)
    hashTable[index] = 0

insertData(123, ""apple"")
insertData(432, ""mango"")
insertData(213, ""banana"")
insertData(654, ""guava"")

print(hashTable)

removeData(123)

print(hashTable)"
Hash Table Data Structure,"The Hash table data structure stores elements in key-value pairs where Key- unique integer that is used for indexing the values
	Value - data that are associated with keys. Key- unique integer that is used for indexing the values Value - data that are associated with keys. In a hash table, a new index is processed using the keys. And, the element corresponding to that key is stored in the index. This process is called hashing. Let k be a key and h(x) be a hash function. Here, h(k) will give us a new index to store the element linked with k. To learn more, visit Hashing. When the hash function generates the same index for multiple keys, there will be a conflict (what value to be stored in that index). This is called a hash collision. We can resolve the hash collision using one of the following techniques. Collision resolution by chaining
	Open Addressing: Linear/Quadratic Probing and Double Hashing Collision resolution by chaining Open Addressing: Linear/Quadratic Probing and Double Hashing In chaining, if a hash function produces the same index for multiple elements, these elements are stored in the same index by using a doubly-linked list. If j is the slot for multiple elements, it contains a pointer to the head of the list of elements. If no element is present, j contains NIL. Pseudocode for operations Unlike chaining, open addressing doesn't store multiple elements into the same slot. Here, each slot is either filled with a single key or left NIL. Different techniques used in open addressing are: In linear probing, collision is resolved by checking the next slot.  h(k, i) = (h′(k) + i) mod m where i = {0, 1, ….}
	h'(k) is a new hash function i = {0, 1, ….} h'(k) is a new hash function If a collision occurs at h(k, 0), then h(k, 1) is checked. In this way, the value of i is incremented linearly. The problem with linear probing is that a cluster of adjacent slots is filled. When inserting a new element, the entire cluster must be traversed. This adds to the time required to perform operations on the hash table. It works similar to linear probing but the spacing between the slots is increased (greater than one) by using the following relation. h(k, i) = (h′(k) + c1i + c2i2) mod m where, c1 and c2 are positive auxiliary constants,
	i = {0, 1, ….} c1 and c2 are positive auxiliary constants, i = {0, 1, ….} If a collision occurs after applying a hash function h(k), then another hash function is calculated for finding the next slot. h(k, i) = (h1(k) + ih2(k)) mod m A good hash function may not prevent the collisions completely however it can reduce the number of collisions. Here, we will look into different methods to find a good hash function If k is a key and m is the size of the hash table, the hash function h() is calculated as: h(k) = k mod m For example, If the size of a hash table is 10 and k = 112 then h(k) = 112 mod 10 = 2. The value of m must not be the powers of 2. This is because the powers of 2 in binary format are 10, 100, 1000, …. When we find k mod m, we will always get the lower order p-bits. h(k) = ⌊m(kA mod 1)⌋ where, kA mod 1 gives the fractional part kA,
	⌊ ⌋ gives the floor value
	A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. kA mod 1 gives the fractional part kA, ⌊ ⌋ gives the floor value A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. In Universal hashing, the hash function is chosen at random independent of keys. Hash tables are implemented where constant time lookup and insertion is required
	cryptographic applications
	indexing data is required constant time lookup and insertion is required cryptographic applications indexing data is required","// Java program to demonstrate working of HashTable 

import java.util.*; 

class HashTable { 
  public static void main(String args[]) 
  {
  Hashtable<Integer, Integer> 
    ht = new Hashtable<Integer, Integer>(); 
  
  ht.put(123, 432); 
  ht.put(12, 2345);
  ht.put(15, 5643); 
  ht.put(3, 321);

  ht.remove(12);

  System.out.println(ht); 
  } 
} "
Hash Table Data Structure,"The Hash table data structure stores elements in key-value pairs where Key- unique integer that is used for indexing the values
	Value - data that are associated with keys. Key- unique integer that is used for indexing the values Value - data that are associated with keys. In a hash table, a new index is processed using the keys. And, the element corresponding to that key is stored in the index. This process is called hashing. Let k be a key and h(x) be a hash function. Here, h(k) will give us a new index to store the element linked with k. To learn more, visit Hashing. When the hash function generates the same index for multiple keys, there will be a conflict (what value to be stored in that index). This is called a hash collision. We can resolve the hash collision using one of the following techniques. Collision resolution by chaining
	Open Addressing: Linear/Quadratic Probing and Double Hashing Collision resolution by chaining Open Addressing: Linear/Quadratic Probing and Double Hashing In chaining, if a hash function produces the same index for multiple elements, these elements are stored in the same index by using a doubly-linked list. If j is the slot for multiple elements, it contains a pointer to the head of the list of elements. If no element is present, j contains NIL. Pseudocode for operations Unlike chaining, open addressing doesn't store multiple elements into the same slot. Here, each slot is either filled with a single key or left NIL. Different techniques used in open addressing are: In linear probing, collision is resolved by checking the next slot.  h(k, i) = (h′(k) + i) mod m where i = {0, 1, ….}
	h'(k) is a new hash function i = {0, 1, ….} h'(k) is a new hash function If a collision occurs at h(k, 0), then h(k, 1) is checked. In this way, the value of i is incremented linearly. The problem with linear probing is that a cluster of adjacent slots is filled. When inserting a new element, the entire cluster must be traversed. This adds to the time required to perform operations on the hash table. It works similar to linear probing but the spacing between the slots is increased (greater than one) by using the following relation. h(k, i) = (h′(k) + c1i + c2i2) mod m where, c1 and c2 are positive auxiliary constants,
	i = {0, 1, ….} c1 and c2 are positive auxiliary constants, i = {0, 1, ….} If a collision occurs after applying a hash function h(k), then another hash function is calculated for finding the next slot. h(k, i) = (h1(k) + ih2(k)) mod m A good hash function may not prevent the collisions completely however it can reduce the number of collisions. Here, we will look into different methods to find a good hash function If k is a key and m is the size of the hash table, the hash function h() is calculated as: h(k) = k mod m For example, If the size of a hash table is 10 and k = 112 then h(k) = 112 mod 10 = 2. The value of m must not be the powers of 2. This is because the powers of 2 in binary format are 10, 100, 1000, …. When we find k mod m, we will always get the lower order p-bits. h(k) = ⌊m(kA mod 1)⌋ where, kA mod 1 gives the fractional part kA,
	⌊ ⌋ gives the floor value
	A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. kA mod 1 gives the fractional part kA, ⌊ ⌋ gives the floor value A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. In Universal hashing, the hash function is chosen at random independent of keys. Hash tables are implemented where constant time lookup and insertion is required
	cryptographic applications
	indexing data is required constant time lookup and insertion is required cryptographic applications indexing data is required","// Implementing hash table in C

#include <stdio.h>
#include <stdlib.h>

struct set
{
  int key;
  int data;
};
struct set *array;
int capacity = 10;
int size = 0;

int hashFunction(int key)
{
  return (key % capacity);
}
int checkPrime(int n)
{
  int i;
  if (n == 1 || n == 0)
  {
  return 0;
  }
  for (i = 2; i < n / 2; i++)
  {
  if (n % i == 0)
  {
    return 0;
  }
  }
  return 1;
}
int getPrime(int n)
{
  if (n % 2 == 0)
  {
  n++;
  }
  while (!checkPrime(n))
  {
  n += 2;
  }
  return n;
}
void init_array()
{
  capacity = getPrime(capacity);
  array = (struct set *)malloc(capacity * sizeof(struct set));
  for (int i = 0; i < capacity; i++)
  {
  array[i].key = 0;
  array[i].data = 0;
  }
}

void insert(int key, int data)
{
  int index = hashFunction(key);
  if (array[index].data == 0)
  {
  array[index].key = key;
  array[index].data = data;
  size++;
  printf(""\n Key (%d) has been inserted \n"", key);
  }
  else if (array[index].key == key)
  {
  array[index].data = data;
  }
  else
  {
  printf(""\n Collision occured  \n"");
  }
}

void remove_element(int key)
{
  int index = hashFunction(key);
  if (array[index].data == 0)
  {
  printf(""\n This key does not exist \n"");
  }
  else
  {
  array[index].key = 0;
  array[index].data = 0;
  size--;
  printf(""\n Key (%d) has been removed \n"", key);
  }
}
void display()
{
  int i;
  for (i = 0; i < capacity; i++)
  {
  if (array[i].data == 0)
  {
    printf(""\n array[%d]: / "", i);
  }
  else
  {
    printf(""\n key: %d array[%d]: %d \t"", array[i].key, i, array[i].data);
  }
  }
}

int size_of_hashtable()
{
  return size;
}

int main()
{
  int choice, key, data, n;
  int c = 0;
  init_array();

  do
  {
  printf(""1.Insert item in the Hash Table""
     ""\n2.Remove item from the Hash Table""
     ""\n3.Check the size of Hash Table""
     ""\n4.Display a Hash Table""
     ""\n\n Please enter your choice: "");

  scanf(""%d"", &choice);
  switch (choice)
  {
  case 1:

    printf(""Enter key -:\t"");
    scanf(""%d"", &key);
    printf(""Enter data -:\t"");
    scanf(""%d"", &data);
    insert(key, data);

    break;

  case 2:

    printf(""Enter the key to delete-:"");
    scanf(""%d"", &key);
    remove_element(key);

    break;

  case 3:

    n = size_of_hashtable();
    printf(""Size of Hash Table is-:%d\n"", n);

    break;

  case 4:

    display();

    break;

  default:

    printf(""Invalid Input\n"");
  }

  printf(""\nDo you want to continue (press 1 for yes): "");
  scanf(""%d"", &c);

  } while (c == 1);
}"
Hash Table Data Structure,"The Hash table data structure stores elements in key-value pairs where Key- unique integer that is used for indexing the values
	Value - data that are associated with keys. Key- unique integer that is used for indexing the values Value - data that are associated with keys. In a hash table, a new index is processed using the keys. And, the element corresponding to that key is stored in the index. This process is called hashing. Let k be a key and h(x) be a hash function. Here, h(k) will give us a new index to store the element linked with k. To learn more, visit Hashing. When the hash function generates the same index for multiple keys, there will be a conflict (what value to be stored in that index). This is called a hash collision. We can resolve the hash collision using one of the following techniques. Collision resolution by chaining
	Open Addressing: Linear/Quadratic Probing and Double Hashing Collision resolution by chaining Open Addressing: Linear/Quadratic Probing and Double Hashing In chaining, if a hash function produces the same index for multiple elements, these elements are stored in the same index by using a doubly-linked list. If j is the slot for multiple elements, it contains a pointer to the head of the list of elements. If no element is present, j contains NIL. Pseudocode for operations Unlike chaining, open addressing doesn't store multiple elements into the same slot. Here, each slot is either filled with a single key or left NIL. Different techniques used in open addressing are: In linear probing, collision is resolved by checking the next slot.  h(k, i) = (h′(k) + i) mod m where i = {0, 1, ….}
	h'(k) is a new hash function i = {0, 1, ….} h'(k) is a new hash function If a collision occurs at h(k, 0), then h(k, 1) is checked. In this way, the value of i is incremented linearly. The problem with linear probing is that a cluster of adjacent slots is filled. When inserting a new element, the entire cluster must be traversed. This adds to the time required to perform operations on the hash table. It works similar to linear probing but the spacing between the slots is increased (greater than one) by using the following relation. h(k, i) = (h′(k) + c1i + c2i2) mod m where, c1 and c2 are positive auxiliary constants,
	i = {0, 1, ….} c1 and c2 are positive auxiliary constants, i = {0, 1, ….} If a collision occurs after applying a hash function h(k), then another hash function is calculated for finding the next slot. h(k, i) = (h1(k) + ih2(k)) mod m A good hash function may not prevent the collisions completely however it can reduce the number of collisions. Here, we will look into different methods to find a good hash function If k is a key and m is the size of the hash table, the hash function h() is calculated as: h(k) = k mod m For example, If the size of a hash table is 10 and k = 112 then h(k) = 112 mod 10 = 2. The value of m must not be the powers of 2. This is because the powers of 2 in binary format are 10, 100, 1000, …. When we find k mod m, we will always get the lower order p-bits. h(k) = ⌊m(kA mod 1)⌋ where, kA mod 1 gives the fractional part kA,
	⌊ ⌋ gives the floor value
	A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. kA mod 1 gives the fractional part kA, ⌊ ⌋ gives the floor value A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth. In Universal hashing, the hash function is chosen at random independent of keys. Hash tables are implemented where constant time lookup and insertion is required
	cryptographic applications
	indexing data is required constant time lookup and insertion is required cryptographic applications indexing data is required","// Implementing hash table in C++

#include <iostream>
#include <list>
using namespace std;

class HashTable
{
  int capacity;
  list<int> *table;

public:
  HashTable(int V);
  void insertItem(int key, int data);
  void deleteItem(int key);

  int checkPrime(int n)
  {
  int i;
  if (n == 1 || n == 0)
  {
    return 0;
  }
  for (i = 2; i < n / 2; i++)
  {
    if (n % i == 0)
    {
    return 0;
    }
  }
  return 1;
  }
  int getPrime(int n)
  {
  if (n % 2 == 0)
  {
    n++;
  }
  while (!checkPrime(n))
  {
    n += 2;
  }
  return n;
  }

  int hashFunction(int key)
  {
  return (key % capacity);
  }
  void displayHash();
};
HashTable::HashTable(int c)
{
  int size = getPrime(c);
  this->capacity = size;
  table = new list<int>[capacity];
}
void HashTable::insertItem(int key, int data)
{
  int index = hashFunction(key);
  table[index].push_back(data);
}

void HashTable::deleteItem(int key)
{
  int index = hashFunction(key);

  list<int>::iterator i;
  for (i = table[index].begin();
   i != table[index].end(); i++)
  {
  if (*i == key)
    break;
  }

  if (i != table[index].end())
  table[index].erase(i);
}

void HashTable::displayHash()
{
  for (int i = 0; i < capacity; i++)
  {
  cout << ""table["" << i << ""]"";
  for (auto x : table[i])
    cout << "" --> "" << x;
  cout << endl;
  }
}

int main()
{
  int key[] = {231, 321, 212, 321, 433, 262};
  int data[] = {123, 432, 523, 43, 423, 111};
  int size = sizeof(key) / sizeof(key[0]);

  HashTable h(size);

  for (int i = 0; i < size; i++)
  h.insertItem(key[i], data[i]);

  h.deleteItem(12);
  h.displayHash();
}"
Heap Data Structure,"Heap data structure is a complete binary tree that satisfies the heap property, where any given node is always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property.
	always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property. always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. This type of data structure is also called a binary heap. Some of the important operations performed on a heap are described below along with their algorithms. Heapify is the process of creating a heap data structure from a binary tree. It is used to create a Min-Heap or a Max-Heap. Let the input array be
		
			Initial Array Create a complete binary tree from the array
		
			Complete binary tree Start from the first index of non-leaf node whose index is given by n/2 - 1.
		
			Start from the first on leaf node Set current element i as largest. The index of left child is given by 2i + 1 and the right child is given by 2i + 2.
		If leftChild is greater than currentElement (i.e. element at ith index), set leftChildIndex as largest.
		If rightChild is greater than element in largest, set rightChildIndex as largest. Swap largest with currentElement
		
			Swap if necessary Repeat steps 3-7 until the subtrees are also heapified. Algorithm To create a Max-Heap: For Min-Heap, both leftChild and rightChild must be larger than the parent for all nodes. Algorithm for insertion in Max Heap Insert the new element at the end of the tree.
		
			Insert at the end Heapify the tree.  For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Algorithm for deletion in Max Heap Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last element Remove the last element.
		
			Remove the last element Heapify the tree. For Min Heap, above algorithm is modified so that both childNodes are greater smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum after removing it from Min Heap. Heap is used while implementing a priority queue.
	Dijkstra's Algorithm
	Heap Sort Heap is used while implementing a priority queue. Dijkstra's Algorithm Heap Sort","# Max-Heap data structure in Python

def heapify(arr, n, i):
    largest = i
    l = 2 * i + 1
    r = 2 * i + 2 
    
    if l < n and arr[i] < arr[l]:
        largest = l
    
    if r < n and arr[largest] < arr[r]:
        largest = r
    
    if largest != i:
        arr[i],arr[largest] = arr[largest],arr[i]
        heapify(arr, n, largest)

def insert(array, newNum):
    size = len(array)
    if size == 0:
        array.append(newNum)
    else:
        array.append(newNum);
        for i in range((size//2)-1, -1, -1):
            heapify(array, size, i)

def deleteNode(array, num):
    size = len(array)
    i = 0
    for i in range(0, size):
        if num == array[i]:
            break
        
    array[i], array[size-1] = array[size-1], array[i]

    array.remove(num)
    
    for i in range((len(array)//2)-1, -1, -1):
        heapify(array, len(array), i)
    
arr = []

insert(arr, 3)
insert(arr, 4)
insert(arr, 9)
insert(arr, 5)
insert(arr, 2)

print (""Max-Heap array: "" + str(arr))

deleteNode(arr, 4)
print(""After deleting an element: "" + str(arr))"
Heap Data Structure,"Heap data structure is a complete binary tree that satisfies the heap property, where any given node is always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property.
	always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property. always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. This type of data structure is also called a binary heap. Some of the important operations performed on a heap are described below along with their algorithms. Heapify is the process of creating a heap data structure from a binary tree. It is used to create a Min-Heap or a Max-Heap. Let the input array be
		
			Initial Array Create a complete binary tree from the array
		
			Complete binary tree Start from the first index of non-leaf node whose index is given by n/2 - 1.
		
			Start from the first on leaf node Set current element i as largest. The index of left child is given by 2i + 1 and the right child is given by 2i + 2.
		If leftChild is greater than currentElement (i.e. element at ith index), set leftChildIndex as largest.
		If rightChild is greater than element in largest, set rightChildIndex as largest. Swap largest with currentElement
		
			Swap if necessary Repeat steps 3-7 until the subtrees are also heapified. Algorithm To create a Max-Heap: For Min-Heap, both leftChild and rightChild must be larger than the parent for all nodes. Algorithm for insertion in Max Heap Insert the new element at the end of the tree.
		
			Insert at the end Heapify the tree.  For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Algorithm for deletion in Max Heap Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last element Remove the last element.
		
			Remove the last element Heapify the tree. For Min Heap, above algorithm is modified so that both childNodes are greater smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum after removing it from Min Heap. Heap is used while implementing a priority queue.
	Dijkstra's Algorithm
	Heap Sort Heap is used while implementing a priority queue. Dijkstra's Algorithm Heap Sort","// Max-Heap data structure in Java

import java.util.ArrayList;

class Heap {
  void heapify(ArrayList<Integer> hT, int i) {
    int size = hT.size();
    int largest = i;
    int l = 2 * i + 1;
    int r = 2 * i + 2;
    if (l < size && hT.get(l) > hT.get(largest))
      largest = l;
    if (r < size && hT.get(r) > hT.get(largest))
      largest = r;

    if (largest != i) {
      int temp = hT.get(largest);
      hT.set(largest, hT.get(i));
      hT.set(i, temp);

      heapify(hT, largest);
    }
  }

  void insert(ArrayList<Integer> hT, int newNum) {
    int size = hT.size();
    if (size == 0) {
      hT.add(newNum);
    } else {
      hT.add(newNum);
      for (int i = size / 2 - 1; i >= 0; i--) {
        heapify(hT, i);
      }
    }
  }

  void deleteNode(ArrayList<Integer> hT, int num)
  {
    int size = hT.size();
    int i;
    for (i = 0; i < size; i++)
    {
      if (num == hT.get(i))
        break;
    }

    int temp = hT.get(i);
    hT.set(i, hT.get(size-1));
    hT.set(size-1, temp);

    hT.remove(size-1);
    for (int j = size / 2 - 1; j >= 0; j--)
    {
      heapify(hT, j);
    }
  }

  void printArray(ArrayList<Integer> array, int size) {
    for (Integer i : array) {
      System.out.print(i + "" "");
    }
    System.out.println();
  }

  public static void main(String args[]) {

    ArrayList<Integer> array = new ArrayList<Integer>();
    int size = array.size();

    Heap h = new Heap();
    h.insert(array, 3);
    h.insert(array, 4);
    h.insert(array, 9);
    h.insert(array, 5);
    h.insert(array, 2);

    System.out.println(""Max-Heap array: "");
    h.printArray(array, size);

    h.deleteNode(array, 4);
    System.out.println(""After deleting an element: "");
    h.printArray(array, size);
  }
}"
Heap Data Structure,"Heap data structure is a complete binary tree that satisfies the heap property, where any given node is always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property.
	always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property. always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. This type of data structure is also called a binary heap. Some of the important operations performed on a heap are described below along with their algorithms. Heapify is the process of creating a heap data structure from a binary tree. It is used to create a Min-Heap or a Max-Heap. Let the input array be
		
			Initial Array Create a complete binary tree from the array
		
			Complete binary tree Start from the first index of non-leaf node whose index is given by n/2 - 1.
		
			Start from the first on leaf node Set current element i as largest. The index of left child is given by 2i + 1 and the right child is given by 2i + 2.
		If leftChild is greater than currentElement (i.e. element at ith index), set leftChildIndex as largest.
		If rightChild is greater than element in largest, set rightChildIndex as largest. Swap largest with currentElement
		
			Swap if necessary Repeat steps 3-7 until the subtrees are also heapified. Algorithm To create a Max-Heap: For Min-Heap, both leftChild and rightChild must be larger than the parent for all nodes. Algorithm for insertion in Max Heap Insert the new element at the end of the tree.
		
			Insert at the end Heapify the tree.  For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Algorithm for deletion in Max Heap Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last element Remove the last element.
		
			Remove the last element Heapify the tree. For Min Heap, above algorithm is modified so that both childNodes are greater smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum after removing it from Min Heap. Heap is used while implementing a priority queue.
	Dijkstra's Algorithm
	Heap Sort Heap is used while implementing a priority queue. Dijkstra's Algorithm Heap Sort","// Max-Heap data structure in C

#include <stdio.h>
int size = 0;
void swap(int *a, int *b)
{
  int temp = *b;
  *b = *a;
  *a = temp;
}
void heapify(int array[], int size, int i)
{
  if (size == 1)
  {
    printf(""Single element in the heap"");
  }
  else
  {
    int largest = i;
    int l = 2 * i + 1;
    int r = 2 * i + 2;
    if (l < size && array[l] > array[largest])
      largest = l;
    if (r < size && array[r] > array[largest])
      largest = r;
    if (largest != i)
    {
      swap(&array[i], &array[largest]);
      heapify(array, size, largest);
    }
  }
}
void insert(int array[], int newNum)
{
  if (size == 0)
  {
    array[0] = newNum;
    size += 1;
  }
  else
  {
    array[size] = newNum;
    size += 1;
    for (int i = size / 2 - 1; i >= 0; i--)
    {
      heapify(array, size, i);
    }
  }
}
void deleteRoot(int array[], int num)
{
  int i;
  for (i = 0; i < size; i++)
  {
    if (num == array[i])
      break;
  }

  swap(&array[i], &array[size - 1]);
  size -= 1;
  for (int i = size / 2 - 1; i >= 0; i--)
  {
    heapify(array, size, i);
  }
}
void printArray(int array[], int size)
{
  for (int i = 0; i < size; ++i)
    printf(""%d "", array[i]);
  printf(""\n"");
}
int main()
{
  int array[10];

  insert(array, 3);
  insert(array, 4);
  insert(array, 9);
  insert(array, 5);
  insert(array, 2);

  printf(""Max-Heap array: "");
  printArray(array, size);

  deleteRoot(array, 4);

  printf(""After deleting an element: "");

  printArray(array, size);
}"
Heap Data Structure,"Heap data structure is a complete binary tree that satisfies the heap property, where any given node is always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property.
	always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. always greater than its child node/s and the key of the root node is the largest among all other nodes. This property is also called max heap property. always smaller than the child node/s and the key of the root node is the smallest among all other nodes. This property is also called min heap property. This type of data structure is also called a binary heap. Some of the important operations performed on a heap are described below along with their algorithms. Heapify is the process of creating a heap data structure from a binary tree. It is used to create a Min-Heap or a Max-Heap. Let the input array be
		
			Initial Array Create a complete binary tree from the array
		
			Complete binary tree Start from the first index of non-leaf node whose index is given by n/2 - 1.
		
			Start from the first on leaf node Set current element i as largest. The index of left child is given by 2i + 1 and the right child is given by 2i + 2.
		If leftChild is greater than currentElement (i.e. element at ith index), set leftChildIndex as largest.
		If rightChild is greater than element in largest, set rightChildIndex as largest. Swap largest with currentElement
		
			Swap if necessary Repeat steps 3-7 until the subtrees are also heapified. Algorithm To create a Max-Heap: For Min-Heap, both leftChild and rightChild must be larger than the parent for all nodes. Algorithm for insertion in Max Heap Insert the new element at the end of the tree.
		
			Insert at the end Heapify the tree.  For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode. Algorithm for deletion in Max Heap Select the element to be deleted.
		
			Select the element to be deleted Swap it with the last element.
		
			Swap with the last element Remove the last element.
		
			Remove the last element Heapify the tree. For Min Heap, above algorithm is modified so that both childNodes are greater smaller than currentNode. Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node. For both Max heap and Min Heap Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum after removing it from Min Heap. Heap is used while implementing a priority queue.
	Dijkstra's Algorithm
	Heap Sort Heap is used while implementing a priority queue. Dijkstra's Algorithm Heap Sort","// Max-Heap data structure in C++

#include <iostream>
#include <vector>
using namespace std;

void swap(int *a, int *b)
{
  int temp = *b;
  *b = *a;
  *a = temp;
}
void heapify(vector<int> &hT, int i)
{
  int size = hT.size();
  int largest = i;
  int l = 2 * i + 1;
  int r = 2 * i + 2;
  if (l < size && hT[l] > hT[largest])
    largest = l;
  if (r < size && hT[r] > hT[largest])
    largest = r;

  if (largest != i)
  {
    swap(&hT[i], &hT[largest]);
    heapify(hT, largest);
  }
}
void insert(vector<int> &hT, int newNum)
{
  int size = hT.size();
  if (size == 0)
  {
    hT.push_back(newNum);
  }
  else
  {
    hT.push_back(newNum);
    for (int i = size / 2 - 1; i >= 0; i--)
    {
      heapify(hT, i);
    }
  }
}
void deleteNode(vector<int> &hT, int num)
{
  int size = hT.size();
  int i;
  for (i = 0; i < size; i++)
  {
    if (num == hT[i])
      break;
  }
  swap(&hT[i], &hT[size - 1]);

  hT.pop_back();
  for (int i = size / 2 - 1; i >= 0; i--)
  {
    heapify(hT, i);
  }
}
void printArray(vector<int> &hT)
{
  for (int i = 0; i < hT.size(); ++i)
    cout << hT[i] << "" "";
  cout << ""\n"";
}

int main()
{
  vector<int> heapTree;

  insert(heapTree, 3);
  insert(heapTree, 4);
  insert(heapTree, 9);
  insert(heapTree, 5);
  insert(heapTree, 2);

  cout << ""Max-Heap array: "";
  printArray(heapTree);

  deleteNode(heapTree, 4);

  cout << ""After deleting an element: "";

  printArray(heapTree);
}"
Fibonacci Heap,"A fibonacci heap is a data structure that consists of a collection of trees which follow min heap or max heap property. We have already discussed min heap and max heap property in the Heap Data Structure article. These two properties are the characteristics of the trees present on a fibonacci heap. In a fibonacci heap, a node can have more than two children or no children at all. Also, it has more efficient heap operations than that supported by the binomial and binary heaps. The fibonacci heap is called a fibonacci heap because the trees are constructed in a way such that a tree of order n has at least Fn+2 nodes in it, where Fn+2 is the (n + 2)th Fibonacci number. Important properties of a Fibonacci heap are: It is a set of min heap-ordered trees. (i.e. The parent is always smaller than the children.) A pointer is maintained at the minimum element node. It consists of a set of marked nodes. (Decrease key operation) The trees within a Fibonacci heap are unordered but rooted. The roots of all the trees are linked together for faster access. The child nodes of a parent node are connected to each other through a circular doubly linked list as shown below. There are two main advantages of using a circular doubly linked list. Deleting a node from the tree takes O(1) time. The concatenation of two such lists takes O(1) time. Algorithm Inserting a node into an already existing heap follows the steps below. Create a new node for the element. Check if the heap is empty. If the heap is empty, set the new node as a root node and mark it min. Else, insert the node into the root list and update min.  The minimum element is always given by the min pointer. Union of two fibonacci heaps consists of following steps. Concatenate the roots of both the heaps. Update min by selecting a minimum key from the new root lists. It is the most important operation on a fibonacci heap. In this operation, the node with minimum value is removed from the heap and the tree is re-adjusted. The following steps are followed: Delete the min node. Set the min-pointer to the next root in the root list. Create an array of size equal to the maximum degree of the trees in the heap before deletion. Do the following (steps 5-7) until there are no multiple roots with the same degree. Map the degree of current root (min-pointer) to the degree in the array. Map the degree of next root to the degree in array. If there are more than two mappings for the same degree, then apply union operation to those roots such that the min-heap property is maintained (i.e. the minimum is at the root). An implementation of the above steps can be understood in the example below. We will perform an extract-min operation on the heap below.
		
			Fibonacci Heap Delete the min node, add all its child nodes to the root list and set the min-pointer to the next root in the root list.
		
			Delete the min node The maximum degree in the tree is 3. Create an array of size 4 and map degree of the next roots with the array.
		
			Create an array Here, 23 and 7 have the same degrees, so unite them.
		
			Unite those having the same degrees Again, 7 and 17 have the same degrees, so unite them as well.
		
			Unite those having the same degrees Again 7 and 24 have the same degree, so unite them.
		
			Unite those having the same degrees Map the next nodes.
		
			Map the remaining nodes Again, 52 and 21 have the same degree, so unite them
		
			Unite those having the same degrees Similarly, unite 21 and 18.
		
			Unite those having the same degrees Map the remaining root.
		
			Map the remaining nodes The final heap is.
		
			Final fibonacci heap These are the most important operations which are discussed in Decrease Key and Delete Node Operations. To improve the asymptotic running time of Dijkstra's algorithm.","# Fibonacci Heap in python

import math

# Creating fibonacci tree
class FibonacciTree:
    def __init__(self, value):
        self.value = value
        self.child = []
        self.order = 0

    # Adding tree at the end of the tree
    def add_at_end(self, t):
        self.child.append(t)
        self.order = self.order + 1


# Creating Fibonacci heap
class FibonacciHeap:
    def __init__(self):
        self.trees = []
        self.least = None
        self.count = 0

    # Insert a node
    def insert_node(self, value):
        new_tree = FibonacciTree(value)
        self.trees.append(new_tree)
        if (self.least is None or value < self.least.value):
            self.least = new_tree
        self.count = self.count + 1

    # Get minimum value
    def get_min(self):
        if self.least is None:
            return None
        return self.least.value

    # Extract the minimum value
    def extract_min(self):
        smallest = self.least
        if smallest is not None:
            for child in smallest.child:
                self.trees.append(child)
            self.trees.remove(smallest)
            if self.trees == []:
                self.least = None
            else:
                self.least = self.trees[0]
                self.consolidate()
            self.count = self.count - 1
            return smallest.value

    # Consolidate the tree
    def consolidate(self):
        aux = (floor_log(self.count) + 1) * [None]

        while self.trees != []:
            x = self.trees[0]
            order = x.order
            self.trees.remove(x)
            while aux[order] is not None:
                y = aux[order]
                if x.value > y.value:
                    x, y = y, x
                x.add_at_end(y)
                aux[order] = None
                order = order + 1
            aux[order] = x

        self.least = None
        for k in aux:
            if k is not None:
                self.trees.append(k)
                if (self.least is None
                        or k.value < self.least.value):
                    self.least = k


def floor_log(x):
    return math.frexp(x)[1] - 1


fibonacci_heap = FibonacciHeap()

fibonacci_heap.insert_node(7)
fibonacci_heap.insert_node(3)
fibonacci_heap.insert_node(17)
fibonacci_heap.insert_node(24)

print('the minimum value of the fibonacci heap: {}'.format(fibonacci_heap.get_min()))

print('the minimum value removed: {}'.format(fibonacci_heap.extract_min()))"
Fibonacci Heap,"A fibonacci heap is a data structure that consists of a collection of trees which follow min heap or max heap property. We have already discussed min heap and max heap property in the Heap Data Structure article. These two properties are the characteristics of the trees present on a fibonacci heap. In a fibonacci heap, a node can have more than two children or no children at all. Also, it has more efficient heap operations than that supported by the binomial and binary heaps. The fibonacci heap is called a fibonacci heap because the trees are constructed in a way such that a tree of order n has at least Fn+2 nodes in it, where Fn+2 is the (n + 2)th Fibonacci number. Important properties of a Fibonacci heap are: It is a set of min heap-ordered trees. (i.e. The parent is always smaller than the children.) A pointer is maintained at the minimum element node. It consists of a set of marked nodes. (Decrease key operation) The trees within a Fibonacci heap are unordered but rooted. The roots of all the trees are linked together for faster access. The child nodes of a parent node are connected to each other through a circular doubly linked list as shown below. There are two main advantages of using a circular doubly linked list. Deleting a node from the tree takes O(1) time. The concatenation of two such lists takes O(1) time. Algorithm Inserting a node into an already existing heap follows the steps below. Create a new node for the element. Check if the heap is empty. If the heap is empty, set the new node as a root node and mark it min. Else, insert the node into the root list and update min.  The minimum element is always given by the min pointer. Union of two fibonacci heaps consists of following steps. Concatenate the roots of both the heaps. Update min by selecting a minimum key from the new root lists. It is the most important operation on a fibonacci heap. In this operation, the node with minimum value is removed from the heap and the tree is re-adjusted. The following steps are followed: Delete the min node. Set the min-pointer to the next root in the root list. Create an array of size equal to the maximum degree of the trees in the heap before deletion. Do the following (steps 5-7) until there are no multiple roots with the same degree. Map the degree of current root (min-pointer) to the degree in the array. Map the degree of next root to the degree in array. If there are more than two mappings for the same degree, then apply union operation to those roots such that the min-heap property is maintained (i.e. the minimum is at the root). An implementation of the above steps can be understood in the example below. We will perform an extract-min operation on the heap below.
		
			Fibonacci Heap Delete the min node, add all its child nodes to the root list and set the min-pointer to the next root in the root list.
		
			Delete the min node The maximum degree in the tree is 3. Create an array of size 4 and map degree of the next roots with the array.
		
			Create an array Here, 23 and 7 have the same degrees, so unite them.
		
			Unite those having the same degrees Again, 7 and 17 have the same degrees, so unite them as well.
		
			Unite those having the same degrees Again 7 and 24 have the same degree, so unite them.
		
			Unite those having the same degrees Map the next nodes.
		
			Map the remaining nodes Again, 52 and 21 have the same degree, so unite them
		
			Unite those having the same degrees Similarly, unite 21 and 18.
		
			Unite those having the same degrees Map the remaining root.
		
			Map the remaining nodes The final heap is.
		
			Final fibonacci heap These are the most important operations which are discussed in Decrease Key and Delete Node Operations. To improve the asymptotic running time of Dijkstra's algorithm.","// Operations on Fibonacci Heap in Java

// Node creation
class node {
  node parent;
  node left;
  node right;
  node child;
  int degree;
  boolean mark;
  int key;

  public node() {
    this.degree = 0;
    this.mark = false;
    this.parent = null;
    this.left = this;
    this.right = this;
    this.child = null;
    this.key = Integer.MAX_VALUE;
  }

  node(int x) {
    this();
    this.key = x;
  }

  void set_parent(node x) {
    this.parent = x;
  }

  node get_parent() {
    return this.parent;
  }

  void set_left(node x) {
    this.left = x;
  }

  node get_left() {
    return this.left;
  }

  void set_right(node x) {
    this.right = x;
  }

  node get_right() {
    return this.right;
  }

  void set_child(node x) {
    this.child = x;
  }

  node get_child() {
    return this.child;
  }

  void set_degree(int x) {
    this.degree = x;
  }

  int get_degree() {
    return this.degree;
  }

  void set_mark(boolean m) {
    this.mark = m;
  }

  boolean get_mark() {
    return this.mark;
  }

  void set_key(int x) {
    this.key = x;
  }

  int get_key() {
    return this.key;
  }
}

public class fibHeap {
  node min;
  int n;
  boolean trace;
  node found;

  public boolean get_trace() {
    return trace;
  }

  public void set_trace(boolean t) {
    this.trace = t;
  }

  public static fibHeap create_heap() {
    return new fibHeap();
  }

  fibHeap() {
    min = null;
    n = 0;
    trace = false;
  }

  private void insert(node x) {
    if (min == null) {
      min = x;
      x.set_left(min);
      x.set_right(min);
    } else {
      x.set_right(min);
      x.set_left(min.get_left());
      min.get_left().set_right(x);
      min.set_left(x);
      if (x.get_key() < min.get_key())
        min = x;
    }
    n += 1;
  }

  public void insert(int key) {
    insert(new node(key));
  }

  public void display() {
    display(min);
    System.out.println();
  }

  private void display(node c) {
    System.out.print(""("");
    if (c == null) {
      System.out.print("")"");
      return;
    } else {
      node temp = c;
      do {
        System.out.print(temp.get_key());
        node k = temp.get_child();
        display(k);
        System.out.print(""->"");
        temp = temp.get_right();
      } while (temp != c);
      System.out.print("")"");
    }
  }

  public static void merge_heap(fibHeap H1, fibHeap H2, fibHeap H3) {
    H3.min = H1.min;

    if (H1.min != null && H2.min != null) {
      node t1 = H1.min.get_left();
      node t2 = H2.min.get_left();
      H1.min.set_left(t2);
      t1.set_right(H2.min);
      H2.min.set_left(t1);
      t2.set_right(H1.min);
    }
    if (H1.min == null || (H2.min != null && H2.min.get_key() < H1.min.get_key()))
      H3.min = H2.min;
    H3.n = H1.n + H2.n;
  }

  public int find_min() {
    return this.min.get_key();
  }

  private void display_node(node z) {
    System.out.println(""right: "" + ((z.get_right() == null) ? ""-1"" : z.get_right().get_key()));
    System.out.println(""left: "" + ((z.get_left() == null) ? ""-1"" : z.get_left().get_key()));
    System.out.println(""child: "" + ((z.get_child() == null) ? ""-1"" : z.get_child().get_key()));
    System.out.println(""degree "" + z.get_degree());
  }

  public int extract_min() {
    node z = this.min;
    if (z != null) {
      node c = z.get_child();
      node k = c, p;
      if (c != null) {
        do {
          p = c.get_right();
          insert(c);
          c.set_parent(null);
          c = p;
        } while (c != null && c != k);
      }
      z.get_left().set_right(z.get_right());
      z.get_right().set_left(z.get_left());
      z.set_child(null);
      if (z == z.get_right())
        this.min = null;
      else {
        this.min = z.get_right();
        this.consolidate();
      }
      this.n -= 1;
      return z.get_key();
    }
    return Integer.MAX_VALUE;
  }

  public void consolidate() {
    double phi = (1 + Math.sqrt(5)) / 2;
    int Dofn = (int) (Math.log(this.n) / Math.log(phi));
    node[] A = new node[Dofn + 1];
    for (int i = 0; i <= Dofn; ++i)
      A[i] = null;
    node w = min;
    if (w != null) {
      node check = min;
      do {
        node x = w;
        int d = x.get_degree();
        while (A[d] != null) {
          node y = A[d];
          if (x.get_key() > y.get_key()) {
            node temp = x;
            x = y;
            y = temp;
            w = x;
          }
          fib_heap_link(y, x);
          check = x;
          A[d] = null;
          d += 1;
        }
        A[d] = x;
        w = w.get_right();
      } while (w != null && w != check);
      this.min = null;
      for (int i = 0; i <= Dofn; ++i) {
        if (A[i] != null) {
          insert(A[i]);
        }
      }
    }
  }

  // Linking operation
  private void fib_heap_link(node y, node x) {
    y.get_left().set_right(y.get_right());
    y.get_right().set_left(y.get_left());

    node p = x.get_child();
    if (p == null) {
      y.set_right(y);
      y.set_left(y);
    } else {
      y.set_right(p);
      y.set_left(p.get_left());
      p.get_left().set_right(y);
      p.set_left(y);
    }
    y.set_parent(x);
    x.set_child(y);
    x.set_degree(x.get_degree() + 1);
    y.set_mark(false);
  }

  // Search operation
  private void find(int key, node c) {
    if (found != null || c == null)
      return;
    else {
      node temp = c;
      do {
        if (key == temp.get_key())
          found = temp;
        else {
          node k = temp.get_child();
          find(key, k);
          temp = temp.get_right();
        }
      } while (temp != c && found == null);
    }
  }

  public node find(int k) {
    found = null;
    find(k, this.min);
    return found;
  }

  public void decrease_key(int key, int nval) {
    node x = find(key);
    decrease_key(x, nval);
  }

  // Decrease key operation
  private void decrease_key(node x, int k) {
    if (k > x.get_key())
      return;
    x.set_key(k);
    node y = x.get_parent();
    if (y != null && x.get_key() < y.get_key()) {
      cut(x, y);
      cascading_cut(y);
    }
    if (x.get_key() < min.get_key())
      min = x;
  }

  // Cut operation
  private void cut(node x, node y) {
    x.get_right().set_left(x.get_left());
    x.get_left().set_right(x.get_right());

    y.set_degree(y.get_degree() - 1);

    x.set_right(null);
    x.set_left(null);
    insert(x);
    x.set_parent(null);
    x.set_mark(false);
  }

  private void cascading_cut(node y) {
    node z = y.get_parent();
    if (z != null) {
      if (y.get_mark() == false)
        y.set_mark(true);
      else {
        cut(y, z);
        cascading_cut(z);
      }
    }
  }

  // Delete operations
  public void delete(node x) {
    decrease_key(x, Integer.MIN_VALUE);
    int p = extract_min();
  }

  public static void main(String[] args) {
    fibHeap obj = create_heap();
    obj.insert(7);
    obj.insert(26);
    obj.insert(30);
    obj.insert(39);
    obj.insert(10);
    obj.display();

    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
  }
}"
Fibonacci Heap,"A fibonacci heap is a data structure that consists of a collection of trees which follow min heap or max heap property. We have already discussed min heap and max heap property in the Heap Data Structure article. These two properties are the characteristics of the trees present on a fibonacci heap. In a fibonacci heap, a node can have more than two children or no children at all. Also, it has more efficient heap operations than that supported by the binomial and binary heaps. The fibonacci heap is called a fibonacci heap because the trees are constructed in a way such that a tree of order n has at least Fn+2 nodes in it, where Fn+2 is the (n + 2)th Fibonacci number. Important properties of a Fibonacci heap are: It is a set of min heap-ordered trees. (i.e. The parent is always smaller than the children.) A pointer is maintained at the minimum element node. It consists of a set of marked nodes. (Decrease key operation) The trees within a Fibonacci heap are unordered but rooted. The roots of all the trees are linked together for faster access. The child nodes of a parent node are connected to each other through a circular doubly linked list as shown below. There are two main advantages of using a circular doubly linked list. Deleting a node from the tree takes O(1) time. The concatenation of two such lists takes O(1) time. Algorithm Inserting a node into an already existing heap follows the steps below. Create a new node for the element. Check if the heap is empty. If the heap is empty, set the new node as a root node and mark it min. Else, insert the node into the root list and update min.  The minimum element is always given by the min pointer. Union of two fibonacci heaps consists of following steps. Concatenate the roots of both the heaps. Update min by selecting a minimum key from the new root lists. It is the most important operation on a fibonacci heap. In this operation, the node with minimum value is removed from the heap and the tree is re-adjusted. The following steps are followed: Delete the min node. Set the min-pointer to the next root in the root list. Create an array of size equal to the maximum degree of the trees in the heap before deletion. Do the following (steps 5-7) until there are no multiple roots with the same degree. Map the degree of current root (min-pointer) to the degree in the array. Map the degree of next root to the degree in array. If there are more than two mappings for the same degree, then apply union operation to those roots such that the min-heap property is maintained (i.e. the minimum is at the root). An implementation of the above steps can be understood in the example below. We will perform an extract-min operation on the heap below.
		
			Fibonacci Heap Delete the min node, add all its child nodes to the root list and set the min-pointer to the next root in the root list.
		
			Delete the min node The maximum degree in the tree is 3. Create an array of size 4 and map degree of the next roots with the array.
		
			Create an array Here, 23 and 7 have the same degrees, so unite them.
		
			Unite those having the same degrees Again, 7 and 17 have the same degrees, so unite them as well.
		
			Unite those having the same degrees Again 7 and 24 have the same degree, so unite them.
		
			Unite those having the same degrees Map the next nodes.
		
			Map the remaining nodes Again, 52 and 21 have the same degree, so unite them
		
			Unite those having the same degrees Similarly, unite 21 and 18.
		
			Unite those having the same degrees Map the remaining root.
		
			Map the remaining nodes The final heap is.
		
			Final fibonacci heap These are the most important operations which are discussed in Decrease Key and Delete Node Operations. To improve the asymptotic running time of Dijkstra's algorithm.","// Operations on a Fibonacci heap in C

#include <math.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>

typedef struct _NODE {
  int key;
  int degree;
  struct _NODE *left_sibling;
  struct _NODE *right_sibling;
  struct _NODE *parent;
  struct _NODE *child;
  bool mark;
  bool visited;
} NODE;

typedef struct fibanocci_heap {
  int n;
  NODE *min;
  int phi;
  int degree;
} FIB_HEAP;

FIB_HEAP *make_fib_heap();
void insertion(FIB_HEAP *H, NODE *new, int val);
NODE *extract_min(FIB_HEAP *H);
void consolidate(FIB_HEAP *H);
void fib_heap_link(FIB_HEAP *H, NODE *y, NODE *x);
NODE *find_min_node(FIB_HEAP *H);
void decrease_key(FIB_HEAP *H, NODE *node, int key);
void cut(FIB_HEAP *H, NODE *node_to_be_decrease, NODE *parent_node);
void cascading_cut(FIB_HEAP *H, NODE *parent_node);
void Delete_Node(FIB_HEAP *H, int dec_key);

FIB_HEAP *make_fib_heap() {
  FIB_HEAP *H;
  H = (FIB_HEAP *)malloc(sizeof(FIB_HEAP));
  H->n = 0;
  H->min = NULL;
  H->phi = 0;
  H->degree = 0;
  return H;
}

// Printing the heap
void print_heap(NODE *n) {
  NODE *x;
  for (x = n;; x = x->right_sibling) {
    if (x->child == NULL) {
      printf(""node with no child (%d) \n"", x->key);
    } else {
      printf(""NODE(%d) with child (%d)\n"", x->key, x->child->key);
      print_heap(x->child);
    }
    if (x->right_sibling == n) {
      break;
    }
  }
}

// Inserting nodes
void insertion(FIB_HEAP *H, NODE *new, int val) {
  new = (NODE *)malloc(sizeof(NODE));
  new->key = val;
  new->degree = 0;
  new->mark = false;
  new->parent = NULL;
  new->child = NULL;
  new->visited = false;
  new->left_sibling = new;
  new->right_sibling = new;
  if (H->min == NULL) {
    H->min = new;
  } else {
    H->min->left_sibling->right_sibling = new;
    new->right_sibling = H->min;
    new->left_sibling = H->min->left_sibling;
    H->min->left_sibling = new;
    if (new->key < H->min->key) {
      H->min = new;
    }
  }
  (H->n)++;
}

// Find min node
NODE *find_min_node(FIB_HEAP *H) {
  if (H == NULL) {
    printf("" \n Fibonacci heap not yet created \n"");
    return NULL;
  } else
    return H->min;
}

// Union operation
FIB_HEAP *unionHeap(FIB_HEAP *H1, FIB_HEAP *H2) {
  FIB_HEAP *Hnew;
  Hnew = make_fib_heap();
  Hnew->min = H1->min;

  NODE *temp1, *temp2;
  temp1 = Hnew->min->right_sibling;
  temp2 = H2->min->left_sibling;

  Hnew->min->right_sibling->left_sibling = H2->min->left_sibling;
  Hnew->min->right_sibling = H2->min;
  H2->min->left_sibling = Hnew->min;
  temp2->right_sibling = temp1;

  if ((H1->min == NULL) || (H2->min != NULL && H2->min->key < H1->min->key))
    Hnew->min = H2->min;
  Hnew->n = H1->n + H2->n;
  return Hnew;
}

// Calculate the degree
int cal_degree(int n) {
  int count = 0;
  while (n > 0) {
    n = n / 2;
    count++;
  }
  return count;
}

// Consolidate function
void consolidate(FIB_HEAP *H) {
  int degree, i, d;
  degree = cal_degree(H->n);
  NODE *A[degree], *x, *y, *z;
  for (i = 0; i <= degree; i++) {
    A[i] = NULL;
  }
  x = H->min;
  do {
    d = x->degree;
    while (A[d] != NULL) {
      y = A[d];
      if (x->key > y->key) {
        NODE *exchange_help;
        exchange_help = x;
        x = y;
        y = exchange_help;
      }
      if (y == H->min)
        H->min = x;
      fib_heap_link(H, y, x);
      if (y->right_sibling == x)
        H->min = x;
      A[d] = NULL;
      d++;
    }
    A[d] = x;
    x = x->right_sibling;
  } while (x != H->min);

  H->min = NULL;
  for (i = 0; i < degree; i++) {
    if (A[i] != NULL) {
      A[i]->left_sibling = A[i];
      A[i]->right_sibling = A[i];
      if (H->min == NULL) {
        H->min = A[i];
      } else {
        H->min->left_sibling->right_sibling = A[i];
        A[i]->right_sibling = H->min;
        A[i]->left_sibling = H->min->left_sibling;
        H->min->left_sibling = A[i];
        if (A[i]->key < H->min->key) {
          H->min = A[i];
        }
      }
      if (H->min == NULL) {
        H->min = A[i];
      } else if (A[i]->key < H->min->key) {
        H->min = A[i];
      }
    }
  }
}

// Linking
void fib_heap_link(FIB_HEAP *H, NODE *y, NODE *x) {
  y->right_sibling->left_sibling = y->left_sibling;
  y->left_sibling->right_sibling = y->right_sibling;

  if (x->right_sibling == x)
    H->min = x;

  y->left_sibling = y;
  y->right_sibling = y;
  y->parent = x;

  if (x->child == NULL) {
    x->child = y;
  }
  y->right_sibling = x->child;
  y->left_sibling = x->child->left_sibling;
  x->child->left_sibling->right_sibling = y;
  x->child->left_sibling = y;
  if ((y->key) < (x->child->key))
    x->child = y;

  (x->degree)++;
}

// Extract min
NODE *extract_min(FIB_HEAP *H) {
  if (H->min == NULL)
    printf(""\n The heap is empty"");
  else {
    NODE *temp = H->min;
    NODE *pntr;
    pntr = temp;
    NODE *x = NULL;
    if (temp->child != NULL) {
      x = temp->child;
      do {
        pntr = x->right_sibling;
        (H->min->left_sibling)->right_sibling = x;
        x->right_sibling = H->min;
        x->left_sibling = H->min->left_sibling;
        H->min->left_sibling = x;
        if (x->key < H->min->key)
          H->min = x;
        x->parent = NULL;
        x = pntr;
      } while (pntr != temp->child);
    }

    (temp->left_sibling)->right_sibling = temp->right_sibling;
    (temp->right_sibling)->left_sibling = temp->left_sibling;
    H->min = temp->right_sibling;

    if (temp == temp->right_sibling && temp->child == NULL)
      H->min = NULL;
    else {
      H->min = temp->right_sibling;
      consolidate(H);
    }
    H->n = H->n - 1;
    return temp;
  }
  return H->min;
}

void cut(FIB_HEAP *H, NODE *node_to_be_decrease, NODE *parent_node) {
  NODE *temp_parent_check;

  if (node_to_be_decrease == node_to_be_decrease->right_sibling)
    parent_node->child = NULL;

  node_to_be_decrease->left_sibling->right_sibling = node_to_be_decrease->right_sibling;
  node_to_be_decrease->right_sibling->left_sibling = node_to_be_decrease->left_sibling;
  if (node_to_be_decrease == parent_node->child)
    parent_node->child = node_to_be_decrease->right_sibling;
  (parent_node->degree)--;

  node_to_be_decrease->left_sibling = node_to_be_decrease;
  node_to_be_decrease->right_sibling = node_to_be_decrease;
  H->min->left_sibling->right_sibling = node_to_be_decrease;
  node_to_be_decrease->right_sibling = H->min;
  node_to_be_decrease->left_sibling = H->min->left_sibling;
  H->min->left_sibling = node_to_be_decrease;

  node_to_be_decrease->parent = NULL;
  node_to_be_decrease->mark = false;
}

void cascading_cut(FIB_HEAP *H, NODE *parent_node) {
  NODE *aux;
  aux = parent_node->parent;
  if (aux != NULL) {
    if (parent_node->mark == false) {
      parent_node->mark = true;
    } else {
      cut(H, parent_node, aux);
      cascading_cut(H, aux);
    }
  }
}

void decrease_key(FIB_HEAP *H, NODE *node_to_be_decrease, int new_key) {
  NODE *parent_node;
  if (H == NULL) {
    printf(""\n FIbonacci heap not created "");
    return;
  }
  if (node_to_be_decrease == NULL) {
    printf(""Node is not in the heap"");
  }

  else {
    if (node_to_be_decrease->key < new_key) {
      printf(""\n Invalid new key for decrease key operation \n "");
    } else {
      node_to_be_decrease->key = new_key;
      parent_node = node_to_be_decrease->parent;
      if ((parent_node != NULL) && (node_to_be_decrease->key < parent_node->key)) {
        printf(""\n cut called"");
        cut(H, node_to_be_decrease, parent_node);
        printf(""\n cascading cut called"");
        cascading_cut(H, parent_node);
      }
      if (node_to_be_decrease->key < H->min->key) {
        H->min = node_to_be_decrease;
      }
    }
  }
}

void *find_node(FIB_HEAP *H, NODE *n, int key, int new_key) {
  NODE *find_use = n;
  NODE *f = NULL;
  find_use->visited = true;
  if (find_use->key == key) {
    find_use->visited = false;
    f = find_use;
    decrease_key(H, f, new_key);
  }
  if (find_use->child != NULL) {
    find_node(H, find_use->child, key, new_key);
  }
  if ((find_use->right_sibling->visited != true)) {
    find_node(H, find_use->right_sibling, key, new_key);
  }

  find_use->visited = false;
}

FIB_HEAP *insertion_procedure() {
  FIB_HEAP *temp;
  int no_of_nodes, ele, i;
  NODE *new_node;
  temp = (FIB_HEAP *)malloc(sizeof(FIB_HEAP));
  temp = NULL;
  if (temp == NULL) {
    temp = make_fib_heap();
  }
  printf("" \n enter number of nodes to be insert = "");
  scanf(""%d"", &no_of_nodes);
  for (i = 1; i <= no_of_nodes; i++) {
    printf(""\n node %d and its key value = "", i);
    scanf(""%d"", &ele);
    insertion(temp, new_node, ele);
  }
  return temp;
}
void Delete_Node(FIB_HEAP *H, int dec_key) {
  NODE *p = NULL;
  find_node(H, H->min, dec_key, -5000);
  p = extract_min(H);
  if (p != NULL)
    printf(""\n Node deleted"");
  else
    printf(""\n Node not deleted:some error"");
}

int main(int argc, char **argv) {
  NODE *new_node, *min_node, *extracted_min, *node_to_be_decrease, *find_use;
  FIB_HEAP *heap, *h1, *h2;
  int operation_no, new_key, dec_key, ele, i, no_of_nodes;
  heap = (FIB_HEAP *)malloc(sizeof(FIB_HEAP));
  heap = NULL;
  while (1) {
    printf("" \n Operations \n 1. Create Fibonacci heap \n 2. Insert nodes into fibonacci heap \n 3. Find min \n 4. Union \n 5. Extract min \n 6. Decrease key \n 7.Delete node \n 8. print heap \n 9. exit \n enter operation_no = "");
    scanf(""%d"", &operation_no);

    switch (operation_no) {
      case 1:
        heap = make_fib_heap();
        break;

      case 2:
        if (heap == NULL) {
          heap = make_fib_heap();
        }
        printf("" enter number of nodes to be insert = "");
        scanf(""%d"", &no_of_nodes);
        for (i = 1; i <= no_of_nodes; i++) {
          printf(""\n node %d and its key value = "", i);
          scanf(""%d"", &ele);
          insertion(heap, new_node, ele);
        }
        break;

      case 3:
        min_node = find_min_node(heap);
        if (min_node == NULL)
          printf(""No minimum value"");
        else
          printf(""\n min value = %d"", min_node->key);
        break;

      case 4:
        if (heap == NULL) {
          printf(""\n no FIbonacci heap created \n "");
          break;
        }
        h1 = insertion_procedure();
        heap = unionHeap(heap, h1);
        printf(""Unified Heap:\n"");
        print_heap(heap->min);
        break;

      case 5:
        if (heap == NULL)
          printf(""Empty Fibonacci heap"");
        else {
          extracted_min = extract_min(heap);
          printf(""\n min value = %d"", extracted_min->key);
          printf(""\n Updated heap: \n"");
          print_heap(heap->min);
        }
        break;

      case 6:
        if (heap == NULL)
          printf(""Fibonacci heap is empty"");
        else {
          printf("" \n node to be decreased = "");
          scanf(""%d"", &dec_key);
          printf("" \n enter the new key = "");
          scanf(""%d"", &new_key);
          find_use = heap->min;
          find_node(heap, find_use, dec_key, new_key);
          printf(""\n Key decreased- Corresponding heap:\n"");
          print_heap(heap->min);
        }
        break;
      case 7:
        if (heap == NULL)
          printf(""Fibonacci heap is empty"");
        else {
          printf("" \n Enter node key to be deleted = "");
          scanf(""%d"", &dec_key);
          Delete_Node(heap, dec_key);
          printf(""\n Node Deleted- Corresponding heap:\n"");
          print_heap(heap->min);
          break;
        }
      case 8:
        print_heap(heap->min);
        break;

      case 9:
        free(new_node);
        free(heap);
        exit(0);

      default:
        printf(""Invalid choice "");
    }
  }
}"
Fibonacci Heap,"A fibonacci heap is a data structure that consists of a collection of trees which follow min heap or max heap property. We have already discussed min heap and max heap property in the Heap Data Structure article. These two properties are the characteristics of the trees present on a fibonacci heap. In a fibonacci heap, a node can have more than two children or no children at all. Also, it has more efficient heap operations than that supported by the binomial and binary heaps. The fibonacci heap is called a fibonacci heap because the trees are constructed in a way such that a tree of order n has at least Fn+2 nodes in it, where Fn+2 is the (n + 2)th Fibonacci number. Important properties of a Fibonacci heap are: It is a set of min heap-ordered trees. (i.e. The parent is always smaller than the children.) A pointer is maintained at the minimum element node. It consists of a set of marked nodes. (Decrease key operation) The trees within a Fibonacci heap are unordered but rooted. The roots of all the trees are linked together for faster access. The child nodes of a parent node are connected to each other through a circular doubly linked list as shown below. There are two main advantages of using a circular doubly linked list. Deleting a node from the tree takes O(1) time. The concatenation of two such lists takes O(1) time. Algorithm Inserting a node into an already existing heap follows the steps below. Create a new node for the element. Check if the heap is empty. If the heap is empty, set the new node as a root node and mark it min. Else, insert the node into the root list and update min.  The minimum element is always given by the min pointer. Union of two fibonacci heaps consists of following steps. Concatenate the roots of both the heaps. Update min by selecting a minimum key from the new root lists. It is the most important operation on a fibonacci heap. In this operation, the node with minimum value is removed from the heap and the tree is re-adjusted. The following steps are followed: Delete the min node. Set the min-pointer to the next root in the root list. Create an array of size equal to the maximum degree of the trees in the heap before deletion. Do the following (steps 5-7) until there are no multiple roots with the same degree. Map the degree of current root (min-pointer) to the degree in the array. Map the degree of next root to the degree in array. If there are more than two mappings for the same degree, then apply union operation to those roots such that the min-heap property is maintained (i.e. the minimum is at the root). An implementation of the above steps can be understood in the example below. We will perform an extract-min operation on the heap below.
		
			Fibonacci Heap Delete the min node, add all its child nodes to the root list and set the min-pointer to the next root in the root list.
		
			Delete the min node The maximum degree in the tree is 3. Create an array of size 4 and map degree of the next roots with the array.
		
			Create an array Here, 23 and 7 have the same degrees, so unite them.
		
			Unite those having the same degrees Again, 7 and 17 have the same degrees, so unite them as well.
		
			Unite those having the same degrees Again 7 and 24 have the same degree, so unite them.
		
			Unite those having the same degrees Map the next nodes.
		
			Map the remaining nodes Again, 52 and 21 have the same degree, so unite them
		
			Unite those having the same degrees Similarly, unite 21 and 18.
		
			Unite those having the same degrees Map the remaining root.
		
			Map the remaining nodes The final heap is.
		
			Final fibonacci heap These are the most important operations which are discussed in Decrease Key and Delete Node Operations. To improve the asymptotic running time of Dijkstra's algorithm.","// Operations on a Fibonacci heap in C++

#include <cmath>
#include <cstdlib>
#include <iostream>

using namespace std;

// Node creation
struct node {
  int n;
  int degree;
  node *parent;
  node *child;
  node *left;
  node *right;
  char mark;

  char C;
};

// Implementation of Fibonacci heap
class FibonacciHeap {
   private:
  int nH;

  node *H;

   public:
  node *InitializeHeap();
  int Fibonnaci_link(node *, node *, node *);
  node *Create_node(int);
  node *Insert(node *, node *);
  node *Union(node *, node *);
  node *Extract_Min(node *);
  int Consolidate(node *);
  int Display(node *);
  node *Find(node *, int);
  int Decrease_key(node *, int, int);
  int Delete_key(node *, int);
  int Cut(node *, node *, node *);
  int Cascase_cut(node *, node *);
  FibonacciHeap() { H = InitializeHeap(); }
};

// Initialize heap
node *FibonacciHeap::InitializeHeap() {
  node *np;
  np = NULL;
  return np;
}

// Create node
node *FibonacciHeap::Create_node(int value) {
  node *x = new node;
  x->n = value;
  return x;
}

// Insert node
node *FibonacciHeap::Insert(node *H, node *x) {
  x->degree = 0;
  x->parent = NULL;
  x->child = NULL;
  x->left = x;
  x->right = x;
  x->mark = 'F';
  x->C = 'N';
  if (H != NULL) {
    (H->left)->right = x;
    x->right = H;
    x->left = H->left;
    H->left = x;
    if (x->n < H->n)
      H = x;
  } else {
    H = x;
  }
  nH = nH + 1;
  return H;
}

// Create linking
int FibonacciHeap::Fibonnaci_link(node *H1, node *y, node *z) {
  (y->left)->right = y->right;
  (y->right)->left = y->left;
  if (z->right == z)
    H1 = z;
  y->left = y;
  y->right = y;
  y->parent = z;

  if (z->child == NULL)
    z->child = y;

  y->right = z->child;
  y->left = (z->child)->left;
  ((z->child)->left)->right = y;
  (z->child)->left = y;

  if (y->n < (z->child)->n)
    z->child = y;
  z->degree++;
}

// Union Operation
node *FibonacciHeap::Union(node *H1, node *H2) {
  node *np;
  node *H = InitializeHeap();
  H = H1;
  (H->left)->right = H2;
  (H2->left)->right = H;
  np = H->left;
  H->left = H2->left;
  H2->left = np;
  return H;
}

// Display the heap
int FibonacciHeap::Display(node *H) {
  node *p = H;
  if (p == NULL) {
    cout << ""Empty Heap"" << endl;
    return 0;
  }
  cout << ""Root Nodes: "" << endl;

  do {
    cout << p->n;
    p = p->right;
    if (p != H) {
      cout << ""-->"";
    }
  } while (p != H && p->right != NULL);
  cout << endl;
}

// Extract min
node *FibonacciHeap::Extract_Min(node *H1) {
  node *p;
  node *ptr;
  node *z = H1;
  p = z;
  ptr = z;
  if (z == NULL)
    return z;

  node *x;
  node *np;

  x = NULL;

  if (z->child != NULL)
    x = z->child;

  if (x != NULL) {
    ptr = x;
    do {
      np = x->right;
      (H1->left)->right = x;
      x->right = H1;
      x->left = H1->left;
      H1->left = x;
      if (x->n < H1->n)
        H1 = x;

      x->parent = NULL;
      x = np;
    } while (np != ptr);
  }

  (z->left)->right = z->right;
  (z->right)->left = z->left;
  H1 = z->right;

  if (z == z->right && z->child == NULL)
    H = NULL;

  else {
    H1 = z->right;
    Consolidate(H1);
  }
  nH = nH - 1;
  return p;
}

// Consolidation Function
int FibonacciHeap::Consolidate(node *H1) {
  int d, i;
  float f = (log(nH)) / (log(2));
  int D = f;
  node *A[D];

  for (i = 0; i <= D; i++)
    A[i] = NULL;

  node *x = H1;
  node *y;
  node *np;
  node *pt = x;

  do {
    pt = pt->right;

    d = x->degree;

    while (A[d] != NULL)

    {
      y = A[d];

      if (x->n > y->n)

      {
        np = x;

        x = y;

        y = np;
      }

      if (y == H1)
        H1 = x;
      Fibonnaci_link(H1, y, x);
      if (x->right == x)
        H1 = x;
      A[d] = NULL;
      d = d + 1;
    }

    A[d] = x;
    x = x->right;

  }

  while (x != H1);
  H = NULL;
  for (int j = 0; j <= D; j++) {
    if (A[j] != NULL) {
      A[j]->left = A[j];
      A[j]->right = A[j];
      if (H != NULL) {
        (H->left)->right = A[j];
        A[j]->right = H;
        A[j]->left = H->left;
        H->left = A[j];
        if (A[j]->n < H->n)
          H = A[j];
      } else {
        H = A[j];
      }
      if (H == NULL)
        H = A[j];
      else if (A[j]->n < H->n)
        H = A[j];
    }
  }
}

// Decrease Key Operation
int FibonacciHeap::Decrease_key(node *H1, int x, int k) {
  node *y;
  if (H1 == NULL) {
    cout << ""The Heap is Empty"" << endl;
    return 0;
  }
  node *ptr = Find(H1, x);
  if (ptr == NULL) {
    cout << ""Node not found in the Heap"" << endl;
    return 1;
  }

  if (ptr->n < k) {
    cout << ""Entered key greater than current key"" << endl;
    return 0;
  }
  ptr->n = k;
  y = ptr->parent;
  if (y != NULL && ptr->n < y->n) {
    Cut(H1, ptr, y);
    Cascase_cut(H1, y);
  }

  if (ptr->n < H->n)
    H = ptr;

  return 0;
}

// Cutting Function
int FibonacciHeap::Cut(node *H1, node *x, node *y)

{
  if (x == x->right)
    y->child = NULL;
  (x->left)->right = x->right;
  (x->right)->left = x->left;
  if (x == y->child)
    y->child = x->right;
  y->degree = y->degree - 1;
  x->right = x;
  x->left = x;
  (H1->left)->right = x;
  x->right = H1;
  x->left = H1->left;
  H1->left = x;
  x->parent = NULL;
  x->mark = 'F';
}

// Cascade cut
int FibonacciHeap::Cascase_cut(node *H1, node *y) {
  node *z = y->parent;
  if (z != NULL) {
    if (y->mark == 'F') {
      y->mark = 'T';
    } else

    {
      Cut(H1, y, z);
      Cascase_cut(H1, z);
    }
  }
}

// Search function
node *FibonacciHeap::Find(node *H, int k) {
  node *x = H;
  x->C = 'Y';
  node *p = NULL;
  if (x->n == k) {
    p = x;
    x->C = 'N';
    return p;
  }

  if (p == NULL) {
    if (x->child != NULL)
      p = Find(x->child, k);
    if ((x->right)->C != 'Y')
      p = Find(x->right, k);
  }

  x->C = 'N';
  return p;
}

// Deleting key
int FibonacciHeap::Delete_key(node *H1, int k) {
  node *np = NULL;
  int t;
  t = Decrease_key(H1, k, -5000);
  if (!t)
    np = Extract_Min(H);
  if (np != NULL)
    cout << ""Key Deleted"" << endl;
  else
    cout << ""Key not Deleted"" << endl;
  return 0;
}

int main() {
  int n, m, l;
  FibonacciHeap fh;
  node *p;
  node *H;
  H = fh.InitializeHeap();

  p = fh.Create_node(7);
  H = fh.Insert(H, p);
  p = fh.Create_node(3);
  H = fh.Insert(H, p);
  p = fh.Create_node(17);
  H = fh.Insert(H, p);
  p = fh.Create_node(24);
  H = fh.Insert(H, p);

  fh.Display(H);

  p = fh.Extract_Min(H);
  if (p != NULL)
    cout << ""The node with minimum key: "" << p->n << endl;
  else
    cout << ""Heap is empty"" << endl;

  m = 26;
  l = 16;
  fh.Decrease_key(H, m, l);

  m = 16;
  fh.Delete_key(H, m);
}"
Decrease Key and Delete Node Operations on a Fibonacci Heap,"A fibonacci heap is a tree based data structure which consists of a collection of trees with min heap or max heap property. Its operations are more efficient in terms of time complexity than those of its similar data structures like binomial heap and binary heap. Now, we will discuss two of its important operations. Decrease a key: decreases the value of a the key to any lower value Delete a node: deletes the given node In decreasing a key operation, the value of a key is decreased to a lower value.  Following functions are used for decreasing the key. Select the node to be decreased, x, and change its value to the new value k. If the parent of x, y, is not null and the key of parent is greater than that of the k then call Cut(x) and Cascading-Cut(y) subsequently. If the key of x is smaller than the key of min, then mark x as min. Remove x from the current position and add it to the root list. If x is marked, then mark it as false. If the parent of y is not null then follow the following steps. If y is unmarked, then mark y. Else, call Cut(y) and Cascading-Cut(parent of y). The above operations can be understood in the examples below. Decrease the value 46 to 15.
		
			Decrease 46 to 15 Cut part: Since 24 ≠ nill and 15 < its parent, cut it and add it to the root list. Cascading-Cut part: mark 24.
		
			Add 15 to root list and mark 24 Decrease the value 35 to 5.
		
			Decrease 35 to 5 Cut part: Since 26 ≠ nill and 5<its parent, cut it and add it to the root list.
		
			Cut 5 and add it to root list Cascading-Cut part: Since 26 is marked, the flow goes to Cut and Cascading-Cut.
		Cut(26): Cut 26 and add it to the root list and mark it as false.
		
			Cut 26 and add it to root list
		
		
		Cascading-Cut(24):
		Since the 24 is also marked, again call Cut(24) and Cascading-Cut(7). These operations result in the tree below.

		
			Cut 24 and add it to root list Since 5 < 7, mark 5 as min.
		
			Mark 5 as min This process makes use of decrease-key and extract-min operations. The following steps are followed for deleting a node. Let k be the node to be deleted. Apply decrease-key operation to decrease the value of k to the lowest possible value (i.e. -∞). Apply extract-min operation to remove this node.","# Fibonacci Heap in python

import math

class FibonacciTree:
    def __init__(self, key):
        self.key = key
        self.children = []
        self.order = 0

    def add_at_end(self, t):
        self.children.append(t)
        self.order = self.order + 1


class FibonacciHeap:
    def __init__(self):
        self.trees = []
        self.least = None
        self.count = 0

    def insert(self, key):
        new_tree = FibonacciTree(key)
        self.trees.append(new_tree)
        if (self.least is None or key < self.least.key):
            self.least = new_tree
        self.count = self.count + 1

    def get_min(self):
        if self.least is None:
            return None
        return self.least.key

    def extract_min(self):
        smallest = self.least
        if smallest is not None:
            for child in smallest.children:
                self.trees.append(child)
            self.trees.remove(smallest)
            if self.trees == []:
                self.least = None
            else:
                self.least = self.trees[0]
                self.consolidate()
            self.count = self.count - 1
            return smallest.key

    def consolidate(self):
        aux = (floor_log2(self.count) + 1) * [None]

        while self.trees != []:
            x = self.trees[0]
            order = x.order
            self.trees.remove(x)
            while aux[order] is not None:
                y = aux[order]
                if x.key > y.key:
                    x, y = y, x
                x.add_at_end(y)
                aux[order] = None
                order = order + 1
            aux[order] = x

        self.least = None
        for k in aux:
            if k is not None:
                self.trees.append(k)
                if (self.least is None
                        or k.key < self.least.key):
                    self.least = k


def floor_log2(x):
    return math.frexp(x)[1] - 1


fheap = FibonacciHeap()

fheap.insert(11)
fheap.insert(10)
fheap.insert(39)
fheap.insert(26)
fheap.insert(24)

print('Minimum value: {}'.format(fheap.get_min()))

print('Minimum value removed: {}'.format(fheap.extract_min()))"
Decrease Key and Delete Node Operations on a Fibonacci Heap,"A fibonacci heap is a tree based data structure which consists of a collection of trees with min heap or max heap property. Its operations are more efficient in terms of time complexity than those of its similar data structures like binomial heap and binary heap. Now, we will discuss two of its important operations. Decrease a key: decreases the value of a the key to any lower value Delete a node: deletes the given node In decreasing a key operation, the value of a key is decreased to a lower value.  Following functions are used for decreasing the key. Select the node to be decreased, x, and change its value to the new value k. If the parent of x, y, is not null and the key of parent is greater than that of the k then call Cut(x) and Cascading-Cut(y) subsequently. If the key of x is smaller than the key of min, then mark x as min. Remove x from the current position and add it to the root list. If x is marked, then mark it as false. If the parent of y is not null then follow the following steps. If y is unmarked, then mark y. Else, call Cut(y) and Cascading-Cut(parent of y). The above operations can be understood in the examples below. Decrease the value 46 to 15.
		
			Decrease 46 to 15 Cut part: Since 24 ≠ nill and 15 < its parent, cut it and add it to the root list. Cascading-Cut part: mark 24.
		
			Add 15 to root list and mark 24 Decrease the value 35 to 5.
		
			Decrease 35 to 5 Cut part: Since 26 ≠ nill and 5<its parent, cut it and add it to the root list.
		
			Cut 5 and add it to root list Cascading-Cut part: Since 26 is marked, the flow goes to Cut and Cascading-Cut.
		Cut(26): Cut 26 and add it to the root list and mark it as false.
		
			Cut 26 and add it to root list
		
		
		Cascading-Cut(24):
		Since the 24 is also marked, again call Cut(24) and Cascading-Cut(7). These operations result in the tree below.

		
			Cut 24 and add it to root list Since 5 < 7, mark 5 as min.
		
			Mark 5 as min This process makes use of decrease-key and extract-min operations. The following steps are followed for deleting a node. Let k be the node to be deleted. Apply decrease-key operation to decrease the value of k to the lowest possible value (i.e. -∞). Apply extract-min operation to remove this node.","// Operations on Fibonacci Heap in Java

class node {
  node parent;
  node left;
  node right;
  node child;
  int degree;
  boolean mark;
  int key;

  public node() {
    this.degree = 0;
    this.mark = false;
    this.parent = null;
    this.left = this;
    this.right = this;
    this.child = null;
    this.key = Integer.MAX_VALUE;
  }

  node(int x) {
    this();
    this.key = x;
  }

  void set_parent(node x) {
    this.parent = x;
  }

  node get_parent() {
    return this.parent;
  }

  void set_left(node x) {
    this.left = x;
  }

  node get_left() {
    return this.left;
  }

  void set_right(node x) {
    this.right = x;
  }

  node get_right() {
    return this.right;
  }

  void set_child(node x) {
    this.child = x;
  }

  node get_child() {
    return this.child;
  }

  void set_degree(int x) {
    this.degree = x;
  }

  int get_degree() {
    return this.degree;
  }

  void set_mark(boolean m) {
    this.mark = m;
  }

  boolean get_mark() {
    return this.mark;
  }

  void set_key(int x) {
    this.key = x;
  }

  int get_key() {
    return this.key;
  }
}

public class fibHeap {
  node min;
  int n;
  boolean trace;
  node found;

  public boolean get_trace() {
    return trace;
  }

  public void set_trace(boolean t) {
    this.trace = t;
  }

  public static fibHeap create_heap() {
    return new fibHeap();
  }

  fibHeap() {
    min = null;
    n = 0;
    trace = false;
  }

  private void insert(node x) {
    if (min == null) {
      min = x;
      x.set_left(min);
      x.set_right(min);
    } else {
      x.set_right(min);
      x.set_left(min.get_left());
      min.get_left().set_right(x);
      min.set_left(x);
      if (x.get_key() < min.get_key())
        min = x;
    }
    n += 1;
  }

  public void insert(int key) {
    insert(new node(key));
  }

  public void display() {
    display(min);
    System.out.println();
  }

  private void display(node c) {
    System.out.print(""("");
    if (c == null) {
      System.out.print("")"");
      return;
    } else {
      node temp = c;
      do {
        System.out.print(temp.get_key());
        node k = temp.get_child();
        display(k);
        System.out.print(""->"");
        temp = temp.get_right();
      } while (temp != c);
      System.out.print("")"");
    }
  }

  public static void merge_heap(fibHeap H1, fibHeap H2, fibHeap H3) {
    H3.min = H1.min;

    if (H1.min != null && H2.min != null) {
      node t1 = H1.min.get_left();
      node t2 = H2.min.get_left();
      H1.min.set_left(t2);
      t1.set_right(H2.min);
      H2.min.set_left(t1);
      t2.set_right(H1.min);
    }
    if (H1.min == null || (H2.min != null && H2.min.get_key() < H1.min.get_key()))
      H3.min = H2.min;
    H3.n = H1.n + H2.n;
  }

  public int find_min() {
    return this.min.get_key();
  }

  private void display_node(node z) {
    System.out.println(""right: "" + ((z.get_right() == null) ? ""-1"" : z.get_right().get_key()));
    System.out.println(""left: "" + ((z.get_left() == null) ? ""-1"" : z.get_left().get_key()));
    System.out.println(""child: "" + ((z.get_child() == null) ? ""-1"" : z.get_child().get_key()));
    System.out.println(""degree "" + z.get_degree());
  }

  public int extract_min() {
    node z = this.min;
    if (z != null) {
      node c = z.get_child();
      node k = c, p;
      if (c != null) {
        do {
          p = c.get_right();
          insert(c);
          c.set_parent(null);
          c = p;
        } while (c != null && c != k);
      }
      z.get_left().set_right(z.get_right());
      z.get_right().set_left(z.get_left());
      z.set_child(null);
      if (z == z.get_right())
        this.min = null;
      else {
        this.min = z.get_right();
        this.consolidate();
      }
      this.n -= 1;
      return z.get_key();
    }
    return Integer.MAX_VALUE;
  }

  public void consolidate() {
    double phi = (1 + Math.sqrt(5)) / 2;
    int Dofn = (int) (Math.log(this.n) / Math.log(phi));
    node[] A = new node[Dofn + 1];
    for (int i = 0; i <= Dofn; ++i)
      A[i] = null;
    node w = min;
    if (w != null) {
      node check = min;
      do {
        node x = w;
        int d = x.get_degree();
        while (A[d] != null) {
          node y = A[d];
          if (x.get_key() > y.get_key()) {
            node temp = x;
            x = y;
            y = temp;
            w = x;
          }
          fib_heap_link(y, x);
          check = x;
          A[d] = null;
          d += 1;
        }
        A[d] = x;
        w = w.get_right();
      } while (w != null && w != check);
      this.min = null;
      for (int i = 0; i <= Dofn; ++i) {
        if (A[i] != null) {
          insert(A[i]);
        }
      }
    }
  }

  private void fib_heap_link(node y, node x) {
    y.get_left().set_right(y.get_right());
    y.get_right().set_left(y.get_left());

    node p = x.get_child();
    if (p == null) {
      y.set_right(y);
      y.set_left(y);
    } else {
      y.set_right(p);
      y.set_left(p.get_left());
      p.get_left().set_right(y);
      p.set_left(y);
    }
    y.set_parent(x);
    x.set_child(y);
    x.set_degree(x.get_degree() + 1);
    y.set_mark(false);
  }

  private void find(int key, node c) {
    if (found != null || c == null)
      return;
    else {
      node temp = c;
      do {
        if (key == temp.get_key())
          found = temp;
        else {
          node k = temp.get_child();
          find(key, k);
          temp = temp.get_right();
        }
      } while (temp != c && found == null);
    }
  }

  public node find(int k) {
    found = null;
    find(k, this.min);
    return found;
  }

  public void decrease_key(int key, int nval) {
    node x = find(key);
    decrease_key(x, nval);
  }

  private void decrease_key(node x, int k) {
    if (k > x.get_key())
      return;
    x.set_key(k);
    node y = x.get_parent();
    if (y != null && x.get_key() < y.get_key()) {
      cut(x, y);
      cascading_cut(y);
    }
    if (x.get_key() < min.get_key())
      min = x;
  }

  private void cut(node x, node y) {
    x.get_right().set_left(x.get_left());
    x.get_left().set_right(x.get_right());

    y.set_degree(y.get_degree() - 1);

    x.set_right(null);
    x.set_left(null);
    insert(x);
    x.set_parent(null);
    x.set_mark(false);
  }

  private void cascading_cut(node y) {
    node z = y.get_parent();
    if (z != null) {
      if (y.get_mark() == false)
        y.set_mark(true);
      else {
        cut(y, z);
        cascading_cut(z);
      }
    }
  }

  public void delete(node x) {
    decrease_key(x, Integer.MIN_VALUE);
    int p = extract_min();
  }

  public static void main(String[] args) {
    fibHeap obj = create_heap();
    obj.insert(7);
    obj.insert(26);
    obj.insert(30);
    obj.insert(39);
    obj.insert(10);
    obj.display();

    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
    System.out.println(obj.extract_min());
    obj.display();
  }
}"
Decrease Key and Delete Node Operations on a Fibonacci Heap,"A fibonacci heap is a tree based data structure which consists of a collection of trees with min heap or max heap property. Its operations are more efficient in terms of time complexity than those of its similar data structures like binomial heap and binary heap. Now, we will discuss two of its important operations. Decrease a key: decreases the value of a the key to any lower value Delete a node: deletes the given node In decreasing a key operation, the value of a key is decreased to a lower value.  Following functions are used for decreasing the key. Select the node to be decreased, x, and change its value to the new value k. If the parent of x, y, is not null and the key of parent is greater than that of the k then call Cut(x) and Cascading-Cut(y) subsequently. If the key of x is smaller than the key of min, then mark x as min. Remove x from the current position and add it to the root list. If x is marked, then mark it as false. If the parent of y is not null then follow the following steps. If y is unmarked, then mark y. Else, call Cut(y) and Cascading-Cut(parent of y). The above operations can be understood in the examples below. Decrease the value 46 to 15.
		
			Decrease 46 to 15 Cut part: Since 24 ≠ nill and 15 < its parent, cut it and add it to the root list. Cascading-Cut part: mark 24.
		
			Add 15 to root list and mark 24 Decrease the value 35 to 5.
		
			Decrease 35 to 5 Cut part: Since 26 ≠ nill and 5<its parent, cut it and add it to the root list.
		
			Cut 5 and add it to root list Cascading-Cut part: Since 26 is marked, the flow goes to Cut and Cascading-Cut.
		Cut(26): Cut 26 and add it to the root list and mark it as false.
		
			Cut 26 and add it to root list
		
		
		Cascading-Cut(24):
		Since the 24 is also marked, again call Cut(24) and Cascading-Cut(7). These operations result in the tree below.

		
			Cut 24 and add it to root list Since 5 < 7, mark 5 as min.
		
			Mark 5 as min This process makes use of decrease-key and extract-min operations. The following steps are followed for deleting a node. Let k be the node to be deleted. Apply decrease-key operation to decrease the value of k to the lowest possible value (i.e. -∞). Apply extract-min operation to remove this node.","// Operations on a Fibonacci heap in C

#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <math.h>

typedef struct _NODE
{
  int key;
  int degree;
  struct _NODE *left_sibling;
  struct _NODE *right_sibling;
  struct _NODE *parent;
  struct _NODE *child;
  bool mark;
  bool visited;
} NODE;

typedef struct fibanocci_heap
{
  int n;
  NODE *min;
  int phi;
  int degree;
} FIB_HEAP;

FIB_HEAP *make_fib_heap();
void insertion(FIB_HEAP *H, NODE *new, int val);
NODE *extract_min(FIB_HEAP *H);
void consolidate(FIB_HEAP *H);
void fib_heap_link(FIB_HEAP *H, NODE *y, NODE *x);
NODE *find_min_node(FIB_HEAP *H);
void decrease_key(FIB_HEAP *H, NODE *node, int key);
void cut(FIB_HEAP *H, NODE *node_to_be_decrease, NODE *parent_node);
void cascading_cut(FIB_HEAP *H, NODE *parent_node);
void Delete_Node(FIB_HEAP *H, int dec_key);

FIB_HEAP *make_fib_heap()
{
  FIB_HEAP *H;
  H = (FIB_HEAP *)malloc(sizeof(FIB_HEAP));
  H->n = 0;
  H->min = NULL;
  H->phi = 0;
  H->degree = 0;
  return H;
}
void new_print_heap(NODE *n)
{
  NODE *x;
  for (x = n;; x = x->right_sibling)
  {

    if (x->child == NULL)
    {
      printf(""node with no child (%d) \n"", x->key);
    }
    else
    {

      printf(""NODE(%d) with child (%d)\n"", x->key, x->child->key);
      new_print_heap(x->child);
    }
    if (x->right_sibling == n)
    {
      break;
    }
  }
}

void insertion(FIB_HEAP *H, NODE *new, int val)
{
  new = (NODE *)malloc(sizeof(NODE));
  new->key = val;
  new->degree = 0;
  new->mark = false;
  new->parent = NULL;
  new->child = NULL;
  new->visited = false;
  new->left_sibling = new;
  new->right_sibling = new;
  if (H->min == NULL)
  {
    H->min = new;
  }
  else
  {
    H->min->left_sibling->right_sibling = new;
    new->right_sibling = H->min;
    new->left_sibling = H->min->left_sibling;
    H->min->left_sibling = new;
    if (new->key < H->min->key)
    {
      H->min = new;
    }
  }
  (H->n)++;
}

NODE *find_min_node(FIB_HEAP *H)
{
  if (H == NULL)
  {
    printf("" \n Fibonacci heap not yet created \n"");
    return NULL;
  }
  else
    return H->min;
}

FIB_HEAP *unionHeap(FIB_HEAP *H1, FIB_HEAP *H2)
{
  FIB_HEAP *Hnew;
  Hnew = make_fib_heap();
  Hnew->min = H1->min;

  NODE *temp1, *temp2;
  temp1 = Hnew->min->right_sibling;
  temp2 = H2->min->left_sibling;

  Hnew->min->right_sibling->left_sibling = H2->min->left_sibling;
  Hnew->min->right_sibling = H2->min;
  H2->min->left_sibling = Hnew->min;
  temp2->right_sibling = temp1;

  if ((H1->min == NULL) || (H2->min != NULL && H2->min->key < H1->min->key))
    Hnew->min = H2->min;
  Hnew->n = H1->n + H2->n;
  return Hnew;
}

int cal_degree(int n)
{
  int count = 0;
  while (n > 0)
  {
    n = n / 2;
    count++;
  }
  return count;
}
void consolidate(FIB_HEAP *H)
{
  int degree, i, d;
  degree = cal_degree(H->n);
  NODE *A[degree], *x, *y, *z;
  for (i = 0; i <= degree; i++)
  {
    A[i] = NULL;
  }
  x = H->min;
  do
  {
    d = x->degree;
    while (A[d] != NULL)
    {
      y = A[d];
      if (x->key > y->key)
      {
        NODE *exchange_help;
        exchange_help = x;
        x = y;
        y = exchange_help;
      }
      if (y == H->min)
        H->min = x;
      fib_heap_link(H, y, x);
      if (y->right_sibling == x)
        H->min = x;
      A[d] = NULL;
      d++;
    }
    A[d] = x;
    x = x->right_sibling;
  } while (x != H->min);

  H->min = NULL;
  for (i = 0; i < degree; i++)
  {
    if (A[i] != NULL)
    {
      A[i]->left_sibling = A[i];
      A[i]->right_sibling = A[i];
      if (H->min == NULL)
      {
        H->min = A[i];
      }
      else
      {
        H->min->left_sibling->right_sibling = A[i];
        A[i]->right_sibling = H->min;
        A[i]->left_sibling = H->min->left_sibling;
        H->min->left_sibling = A[i];
        if (A[i]->key < H->min->key)
        {
          H->min = A[i];
        }
      }
      if (H->min == NULL)
      {
        H->min = A[i];
      }
      else if (A[i]->key < H->min->key)
      {
        H->min = A[i];
      }
    }
  }
}

void fib_heap_link(FIB_HEAP *H, NODE *y, NODE *x)
{
  y->right_sibling->left_sibling = y->left_sibling;
  y->left_sibling->right_sibling = y->right_sibling;

  if (x->right_sibling == x)
    H->min = x;

  y->left_sibling = y;
  y->right_sibling = y;
  y->parent = x;

  if (x->child == NULL)
  {
    x->child = y;
  }
  y->right_sibling = x->child;
  y->left_sibling = x->child->left_sibling;
  x->child->left_sibling->right_sibling = y;
  x->child->left_sibling = y;
  if ((y->key) < (x->child->key))
    x->child = y;

  (x->degree)++;
}
NODE *extract_min(FIB_HEAP *H)
{

  if (H->min == NULL)
    printf(""\n The heap is empty"");
  else
  {
    NODE *temp = H->min;
    NODE *pntr;
    pntr = temp;
    NODE *x = NULL;
    if (temp->child != NULL)
    {

      x = temp->child;
      do
      {
        pntr = x->right_sibling;
        (H->min->left_sibling)->right_sibling = x;
        x->right_sibling = H->min;
        x->left_sibling = H->min->left_sibling;
        H->min->left_sibling = x;
        if (x->key < H->min->key)
          H->min = x;
        x->parent = NULL;
        x = pntr;
      } while (pntr != temp->child);
    }

    (temp->left_sibling)->right_sibling = temp->right_sibling;
    (temp->right_sibling)->left_sibling = temp->left_sibling;
    H->min = temp->right_sibling;

    if (temp == temp->right_sibling && temp->child == NULL)
      H->min = NULL;
    else
    {
      H->min = temp->right_sibling;
      consolidate(H);
    }
    H->n = H->n - 1;
    return temp;
  }
  return H->min;
}

void cut(FIB_HEAP *H, NODE *node_to_be_decrease, NODE *parent_node)
{
  NODE *temp_parent_check;

  if (node_to_be_decrease == node_to_be_decrease->right_sibling)
    parent_node->child = NULL;

  node_to_be_decrease->left_sibling->right_sibling = node_to_be_decrease->right_sibling;
  node_to_be_decrease->right_sibling->left_sibling = node_to_be_decrease->left_sibling;
  if (node_to_be_decrease == parent_node->child)
    parent_node->child = node_to_be_decrease->right_sibling;
  (parent_node->degree)--;

  node_to_be_decrease->left_sibling = node_to_be_decrease;
  node_to_be_decrease->right_sibling = node_to_be_decrease;
  H->min->left_sibling->right_sibling = node_to_be_decrease;
  node_to_be_decrease->right_sibling = H->min;
  node_to_be_decrease->left_sibling = H->min->left_sibling;
  H->min->left_sibling = node_to_be_decrease;

  node_to_be_decrease->parent = NULL;
  node_to_be_decrease->mark = false;
}

void cascading_cut(FIB_HEAP *H, NODE *parent_node)
{
  NODE *aux;
  aux = parent_node->parent;
  if (aux != NULL)
  {
    if (parent_node->mark == false)
    {
      parent_node->mark = true;
    }
    else
    {
      cut(H, parent_node, aux);
      cascading_cut(H, aux);
    }
  }
}

void decrease_key(FIB_HEAP *H, NODE *node_to_be_decrease, int new_key)
{
  NODE *parent_node;
  if (H == NULL)
  {
    printf(""\n FIbonacci heap not created "");
    return;
  }
  if (node_to_be_decrease == NULL)
  {
    printf(""Node is not in the heap"");
  }

  else
  {
    if (node_to_be_decrease->key < new_key)
    {
      printf(""\n Invalid new key for decrease key operation \n "");
    }
    else
    {
      node_to_be_decrease->key = new_key;
      parent_node = node_to_be_decrease->parent;
      if ((parent_node != NULL) && (node_to_be_decrease->key < parent_node->key))
      {
        printf(""\n cut called"");
        cut(H, node_to_be_decrease, parent_node);
        printf(""\n cascading cut called"");
        cascading_cut(H, parent_node);
      }
      if (node_to_be_decrease->key < H->min->key)
      {
        H->min = node_to_be_decrease;
      }
    }
  }
}

void *find_node(FIB_HEAP *H, NODE *n, int key, int new_key)
{
  NODE *find_use = n;
  NODE *f = NULL;
  find_use->visited = true;
  if (find_use->key == key)
  {
    find_use->visited = false;
    f = find_use;
    decrease_key(H, f, new_key);
  }
  if (find_use->child != NULL)
  {
    find_node(H, find_use->child, key, new_key);
  }
  if ((find_use->right_sibling->visited != true))
  {
    find_node(H, find_use->right_sibling, key, new_key);
  }

  find_use->visited = false;
}

FIB_HEAP *insertion_procedure()
{
  FIB_HEAP *temp;
  int no_of_nodes, ele, i;
  NODE *new_node;
  temp = (FIB_HEAP *)malloc(sizeof(FIB_HEAP));
  temp = NULL;
  if (temp == NULL)
  {
    temp = make_fib_heap();
  }
  printf("" \n enter number of nodes to be insert = "");
  scanf(""%d"", &no_of_nodes);
  for (i = 1; i <= no_of_nodes; i++)
  {
    printf(""\n node %d and its key value = "", i);
    scanf(""%d"", &ele);
    insertion(temp, new_node, ele);
  }
  return temp;
}
void Delete_Node(FIB_HEAP *H, int dec_key)
{
  NODE *p = NULL;
  find_node(H, H->min, dec_key, -5000);
  p = extract_min(H);
  if (p != NULL)
    printf(""\n Node deleted"");
  else
    printf(""\n Node not deleted:some error"");
}

int main(int argc, char **argv)
{
  NODE *new_node, *min_node, *extracted_min, *node_to_be_decrease, *find_use;
  FIB_HEAP *heap, *h1, *h2;
  int operation_no, new_key, dec_key, ele, i, no_of_nodes;
  heap = (FIB_HEAP *)malloc(sizeof(FIB_HEAP));
  heap = NULL;
  while (1)
  {

    printf("" \n choose below operations \n 1. Create Fibonacci heap \n 2. Insert nodes into fibonacci heap \n 3. Find min \n 4. Union \n 5. Extract min \n 6. Decrease key \n 7.Delete node \n 8. print heap \n 9. exit \n enter operation_no = "");
    scanf(""%d"", &operation_no);

    switch (operation_no)
    {
    case 1:
      heap = make_fib_heap();
      break;

    case 2:
      if (heap == NULL)
      {
        heap = make_fib_heap();
      }
      printf("" enter number of nodes to be insert = "");
      scanf(""%d"", &no_of_nodes);
      for (i = 1; i <= no_of_nodes; i++)
      {
        printf(""\n node %d and its key value = "", i);
        scanf(""%d"", &ele);
        insertion(heap, new_node, ele);
      }
      break;

    case 3:
      min_node = find_min_node(heap);
      if (min_node == NULL)
        printf(""No minimum value"");
      else
        printf(""\n min value = %d"", min_node->key);
      break;

    case 4:
      if (heap == NULL)
      {
        printf(""\n no FIbonacci heap is created please create fibonacci heap \n "");
        break;
      }
      h1 = insertion_procedure();
      heap = unionHeap(heap, h1);
      printf(""Unified Heap:\n"");
      new_print_heap(heap->min);
      break;

    case 5:
      if (heap == NULL)
        printf(""Fibonacci heap is empty"");
      else
      {
        extracted_min = extract_min(heap);
        printf(""\n min value = %d"", extracted_min->key);
        printf(""\n Updated heap: \n"");
        new_print_heap(heap->min);
      }
      break;

    case 6:
      if (heap == NULL)
        printf(""Fibonacci heap is empty"");
      else
      {
        printf("" \n node to be decreased = "");
        scanf(""%d"", &dec_key);
        printf("" \n enter the new key = "");
        scanf(""%d"", &new_key);
        find_use = heap->min;
        find_node(heap, find_use, dec_key, new_key);
        printf(""\n Key decreased- Corresponding heap:\n"");
        new_print_heap(heap->min);
      }
      break;
    case 7:
      if (heap == NULL)
        printf(""Fibonacci heap is empty"");
      else
      {
        printf("" \n Enter node key to be deleted = "");
        scanf(""%d"", &dec_key);
        Delete_Node(heap, dec_key);
        printf(""\n Node Deleted- Corresponding heap:\n"");
        new_print_heap(heap->min);
        break;
      }
    case 8:
      new_print_heap(heap->min);
      break;

    case 9:
      free(new_node);
      free(heap);
      exit(0);

    default:
      printf(""Invalid choice "");
    }
  }
}"
Decrease Key and Delete Node Operations on a Fibonacci Heap,"A fibonacci heap is a tree based data structure which consists of a collection of trees with min heap or max heap property. Its operations are more efficient in terms of time complexity than those of its similar data structures like binomial heap and binary heap. Now, we will discuss two of its important operations. Decrease a key: decreases the value of a the key to any lower value Delete a node: deletes the given node In decreasing a key operation, the value of a key is decreased to a lower value.  Following functions are used for decreasing the key. Select the node to be decreased, x, and change its value to the new value k. If the parent of x, y, is not null and the key of parent is greater than that of the k then call Cut(x) and Cascading-Cut(y) subsequently. If the key of x is smaller than the key of min, then mark x as min. Remove x from the current position and add it to the root list. If x is marked, then mark it as false. If the parent of y is not null then follow the following steps. If y is unmarked, then mark y. Else, call Cut(y) and Cascading-Cut(parent of y). The above operations can be understood in the examples below. Decrease the value 46 to 15.
		
			Decrease 46 to 15 Cut part: Since 24 ≠ nill and 15 < its parent, cut it and add it to the root list. Cascading-Cut part: mark 24.
		
			Add 15 to root list and mark 24 Decrease the value 35 to 5.
		
			Decrease 35 to 5 Cut part: Since 26 ≠ nill and 5<its parent, cut it and add it to the root list.
		
			Cut 5 and add it to root list Cascading-Cut part: Since 26 is marked, the flow goes to Cut and Cascading-Cut.
		Cut(26): Cut 26 and add it to the root list and mark it as false.
		
			Cut 26 and add it to root list
		
		
		Cascading-Cut(24):
		Since the 24 is also marked, again call Cut(24) and Cascading-Cut(7). These operations result in the tree below.

		
			Cut 24 and add it to root list Since 5 < 7, mark 5 as min.
		
			Mark 5 as min This process makes use of decrease-key and extract-min operations. The following steps are followed for deleting a node. Let k be the node to be deleted. Apply decrease-key operation to decrease the value of k to the lowest possible value (i.e. -∞). Apply extract-min operation to remove this node.","// Operations on a Fibonacci heap in C++

#include <iostream>
#include <cmath>
#include <cstdlib>

using namespace std;

struct node
{
  int n;
  int degree;
  node *parent;
  node *child;
  node *left;
  node *right;
  char mark;

  char C;
};

class FibonacciHeap
{
private:
  int nH;

  node *H;

public:
  node *InitializeHeap();
  int Fibonnaci_link(node *, node *, node *);
  node *Create_node(int);
  node *Insert(node *, node *);
  node *Union(node *, node *);
  node *Extract_Min(node *);
  int Consolidate(node *);
  int Display(node *);
  node *Find(node *, int);
  int Decrease_key(node *, int, int);
  int Delete_key(node *, int);
  int Cut(node *, node *, node *);
  int Cascase_cut(node *, node *);
  FibonacciHeap() { H = InitializeHeap(); }
};

node *FibonacciHeap::InitializeHeap()
{
  node *np;
  np = NULL;
  return np;
}

node *FibonacciHeap::Create_node(int value)
{
  node *x = new node;
  x->n = value;
  return x;
}

node *FibonacciHeap::Insert(node *H, node *x)
{
  x->degree = 0;
  x->parent = NULL;
  x->child = NULL;
  x->left = x;
  x->right = x;
  x->mark = 'F';
  x->C = 'N';
  if (H != NULL)
  {
    (H->left)->right = x;
    x->right = H;
    x->left = H->left;
    H->left = x;
    if (x->n < H->n)
      H = x;
  }
  else
  {
    H = x;
  }
  nH = nH + 1;
  return H;
}

int FibonacciHeap::Fibonnaci_link(node *H1, node *y, node *z)
{
  (y->left)->right = y->right;
  (y->right)->left = y->left;
  if (z->right == z)
    H1 = z;
  y->left = y;
  y->right = y;
  y->parent = z;

  if (z->child == NULL)
    z->child = y;

  y->right = z->child;
  y->left = (z->child)->left;
  ((z->child)->left)->right = y;
  (z->child)->left = y;

  if (y->n < (z->child)->n)
    z->child = y;
  z->degree++;
}

node *FibonacciHeap::Union(node *H1, node *H2)
{
  node *np;
  node *H = InitializeHeap();
  H = H1;
  (H->left)->right = H2;
  (H2->left)->right = H;
  np = H->left;
  H->left = H2->left;
  H2->left = np;
  return H;
}

int FibonacciHeap::Display(node *H)
{
  node *p = H;
  if (p == NULL)
  {
    cout << ""The Heap is Empty"" << endl;
    return 0;
  }
  cout << ""The root nodes of Heap are: "" << endl;

  do
  {
    cout << p->n;
    p = p->right;
    if (p != H)
    {
      cout << ""-->"";
    }
  } while (p != H && p->right != NULL);
  cout << endl;
}

node *FibonacciHeap::Extract_Min(node *H1)
{
  node *p;
  node *ptr;
  node *z = H1;
  p = z;
  ptr = z;
  if (z == NULL)
    return z;

  node *x;
  node *np;

  x = NULL;

  if (z->child != NULL)
    x = z->child;

  if (x != NULL)
  {
    ptr = x;
    do
    {
      np = x->right;
      (H1->left)->right = x;
      x->right = H1;
      x->left = H1->left;
      H1->left = x;
      if (x->n < H1->n)
        H1 = x;

      x->parent = NULL;
      x = np;
    } while (np != ptr);
  }

  (z->left)->right = z->right;
  (z->right)->left = z->left;
  H1 = z->right;

  if (z == z->right && z->child == NULL)
    H = NULL;

  else
  {
    H1 = z->right;
    Consolidate(H1);
  }
  nH = nH - 1;
  return p;
}

int FibonacciHeap::Consolidate(node *H1)
{
  int d, i;
  float f = (log(nH)) / (log(2));
  int D = f;
  node *A[D];

  for (i = 0; i <= D; i++)
    A[i] = NULL;

  node *x = H1;
  node *y;
  node *np;
  node *pt = x;

  do
  {
    pt = pt->right;

    d = x->degree;

    while (A[d] != NULL)

    {
      y = A[d];

      if (x->n > y->n)

      {
        np = x;

        x = y;

        y = np;
      }

      if (y == H1)
        H1 = x;
      Fibonnaci_link(H1, y, x);
      if (x->right == x)
        H1 = x;
      A[d] = NULL;
      d = d + 1;
    }

    A[d] = x;
    x = x->right;

  }

  while (x != H1);
  H = NULL;
  for (int j = 0; j <= D; j++)
  {
    if (A[j] != NULL)
    {
      A[j]->left = A[j];
      A[j]->right = A[j];
      if (H != NULL)
      {
        (H->left)->right = A[j];
        A[j]->right = H;
        A[j]->left = H->left;
        H->left = A[j];
        if (A[j]->n < H->n)
          H = A[j];
      }
      else
      {
        H = A[j];
      }
      if (H == NULL)
        H = A[j];
      else if (A[j]->n < H->n)
        H = A[j];
    }
  }
}

int FibonacciHeap::Decrease_key(node *H1, int x, int k)
{
  node *y;
  if (H1 == NULL)
  {
    cout << ""The Heap is Empty"" << endl;
    return 0;
  }
  node *ptr = Find(H1, x);
  if (ptr == NULL)
  {
    cout << ""Node not found in the Heap"" << endl;
    return 1;
  }

  if (ptr->n < k)
  {
    cout << ""Entered key greater than current key"" << endl;
    return 0;
  }
  ptr->n = k;
  y = ptr->parent;
  if (y != NULL && ptr->n < y->n)
  {
    Cut(H1, ptr, y);
    Cascase_cut(H1, y);
  }

  if (ptr->n < H->n)
    H = ptr;

  return 0;
}

int FibonacciHeap::Cut(node *H1, node *x, node *y)

{
  if (x == x->right)
    y->child = NULL;
  (x->left)->right = x->right;
  (x->right)->left = x->left;
  if (x == y->child)
    y->child = x->right;
  y->degree = y->degree - 1;
  x->right = x;
  x->left = x;
  (H1->left)->right = x;
  x->right = H1;
  x->left = H1->left;
  H1->left = x;
  x->parent = NULL;
  x->mark = 'F';
}

int FibonacciHeap::Cascase_cut(node *H1, node *y)
{
  node *z = y->parent;
  if (z != NULL)
  {
    if (y->mark == 'F')
    {
      y->mark = 'T';
    }
    else

    {
      Cut(H1, y, z);
      Cascase_cut(H1, z);
    }
  }
}

node *FibonacciHeap::Find(node *H, int k)
{
  node *x = H;
  x->C = 'Y';
  node *p = NULL;
  if (x->n == k)
  {
    p = x;
    x->C = 'N';
    return p;
  }

  if (p == NULL)
  {
    if (x->child != NULL)
      p = Find(x->child, k);
    if ((x->right)->C != 'Y')
      p = Find(x->right, k);
  }

  x->C = 'N';
  return p;
}

int FibonacciHeap::Delete_key(node *H1, int k)
{
  node *np = NULL;
  int t;
  t = Decrease_key(H1, k, -5000);
  if (!t)
    np = Extract_Min(H);
  if (np != NULL)
    cout << ""Key Deleted"" << endl;
  else
    cout << ""Key not Deleted"" << endl;
  return 0;
}

int main()
{
  int n, m, l;
  FibonacciHeap fh;
  node *p;
  node *H;
  H = fh.InitializeHeap();

  p = fh.Create_node(7);
  H = fh.Insert(H, p);
  p = fh.Create_node(17);
  H = fh.Insert(H, p);
  p = fh.Create_node(26);
  H = fh.Insert(H, p);
  p = fh.Create_node(1);
  H = fh.Insert(H, p);

  fh.Display(H);

  p = fh.Extract_Min(H);
  if (p != NULL)
    cout << ""The node with minimum key: "" << p->n << endl;
  else
    cout << ""Heap is empty"" << endl;

  m = 26;
  l = 16;
  fh.Decrease_key(H, m, l);

  m = 16;
  fh.Delete_key(H, m);
}"
Tree Traversal,"Traversing a tree means visiting every node in the tree. You might, for instance, want to add all the values in the tree or find the largest one. For all these operations, you will need to visit each node of the tree. Linear data structures like arrays, stacks, queues, and linked list have only one way to read the data. But a hierarchical data structure like a tree can be traversed in different ways. Let's think about how we can read the elements of the tree in the image shown above. Starting from top, Left to right Starting from bottom, Left to right Although this process is somewhat easy, it doesn't respect the hierarchy of the tree, only the depth of the nodes. Instead, we use traversal methods that take into account the basic structure of a tree i.e. The struct node pointed to by left and right might have other left and right children so we should think of them as sub-trees instead of sub-nodes.  According to this structure, every tree is a combination of A node carrying data
	Two subtrees A node carrying data Two subtrees Remember that our goal is to visit each node, so we need to visit all the nodes in the subtree, visit the root node and visit all the nodes in the right subtree as well. Depending on the order in which we do this, there can be three types of traversal. First, visit all the nodes in the left subtree Then the root node Visit all the nodes in the right subtree Visit root node Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit the root node Let's visualize in-order traversal. We start from the root node. We traverse the left subtree first. We also need to remember to visit the root node and the right subtree when this tree is done. Let's put all this in a stack so that we remember. Now we traverse to the subtree pointed on the TOP of the stack. Again, we follow the same rule of inorder After traversing the left subtree, we are left with Since the node ""5"" doesn't have any subtrees, we print it directly. After that we print its parent ""12"" and then the right child ""6"". Putting everything on a stack was helpful because now that the left-subtree of the root node has been traversed, we can print it and go to the right subtree. After going through all the elements, we get the inorder traversal as We don't have to create the stack ourselves because recursion maintains the correct order for us.","# Tree traversal in Python


class Node:
    def __init__(self, item):
        self.left = None
        self.right = None
        self.val = item


def inorder(root):

    if root:
        # Traverse left
        inorder(root.left)
        # Traverse root
        print(str(root.val) + ""->"", end='')
        # Traverse right
        inorder(root.right)


def postorder(root):

    if root:
        # Traverse left
        postorder(root.left)
        # Traverse right
        postorder(root.right)
        # Traverse root
        print(str(root.val) + ""->"", end='')


def preorder(root):

    if root:
        # Traverse root
        print(str(root.val) + ""->"", end='')
        # Traverse left
        preorder(root.left)
        # Traverse right
        preorder(root.right)


root = Node(1)
root.left = Node(2)
root.right = Node(3)
root.left.left = Node(4)
root.left.right = Node(5)

print(""Inorder traversal "")
inorder(root)

print(""\nPreorder traversal "")
preorder(root)

print(""\nPostorder traversal "")
postorder(root)"
Tree Traversal,"Traversing a tree means visiting every node in the tree. You might, for instance, want to add all the values in the tree or find the largest one. For all these operations, you will need to visit each node of the tree. Linear data structures like arrays, stacks, queues, and linked list have only one way to read the data. But a hierarchical data structure like a tree can be traversed in different ways. Let's think about how we can read the elements of the tree in the image shown above. Starting from top, Left to right Starting from bottom, Left to right Although this process is somewhat easy, it doesn't respect the hierarchy of the tree, only the depth of the nodes. Instead, we use traversal methods that take into account the basic structure of a tree i.e. The struct node pointed to by left and right might have other left and right children so we should think of them as sub-trees instead of sub-nodes.  According to this structure, every tree is a combination of A node carrying data
	Two subtrees A node carrying data Two subtrees Remember that our goal is to visit each node, so we need to visit all the nodes in the subtree, visit the root node and visit all the nodes in the right subtree as well. Depending on the order in which we do this, there can be three types of traversal. First, visit all the nodes in the left subtree Then the root node Visit all the nodes in the right subtree Visit root node Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit the root node Let's visualize in-order traversal. We start from the root node. We traverse the left subtree first. We also need to remember to visit the root node and the right subtree when this tree is done. Let's put all this in a stack so that we remember. Now we traverse to the subtree pointed on the TOP of the stack. Again, we follow the same rule of inorder After traversing the left subtree, we are left with Since the node ""5"" doesn't have any subtrees, we print it directly. After that we print its parent ""12"" and then the right child ""6"". Putting everything on a stack was helpful because now that the left-subtree of the root node has been traversed, we can print it and go to the right subtree. After going through all the elements, we get the inorder traversal as We don't have to create the stack ourselves because recursion maintains the correct order for us.","// Tree traversal in Java

class Node {
  int item;
  Node left, right;

  public Node(int key) {
  item = key;
  left = right = null;
  }
}

class BinaryTree {
  // Root of Binary Tree
  Node root;

  BinaryTree() {
  root = null;
  }

  void postorder(Node node) {
  if (node == null)
    return;

  // Traverse left
  postorder(node.left);
  // Traverse right
  postorder(node.right);
  // Traverse root
  System.out.print(node.item + ""->"");
  }

  void inorder(Node node) {
  if (node == null)
    return;

  // Traverse left
  inorder(node.left);
  // Traverse root
  System.out.print(node.item + ""->"");
  // Traverse right
  inorder(node.right);
  }

  void preorder(Node node) {
  if (node == null)
    return;

  // Traverse root
  System.out.print(node.item + ""->"");
  // Traverse left
  preorder(node.left);
  // Traverse right
  preorder(node.right);
  }

  public static void main(String[] args) {
  BinaryTree tree = new BinaryTree();
  tree.root = new Node(1);
  tree.root.left = new Node(12);
  tree.root.right = new Node(9);
  tree.root.left.left = new Node(5);
  tree.root.left.right = new Node(6);

  System.out.println(""Inorder traversal"");
  tree.inorder(tree.root);

  System.out.println(""\nPreorder traversal "");
  tree.preorder(tree.root);

  System.out.println(""\nPostorder traversal"");
  tree.postorder(tree.root);
  }
}"
Tree Traversal,"Traversing a tree means visiting every node in the tree. You might, for instance, want to add all the values in the tree or find the largest one. For all these operations, you will need to visit each node of the tree. Linear data structures like arrays, stacks, queues, and linked list have only one way to read the data. But a hierarchical data structure like a tree can be traversed in different ways. Let's think about how we can read the elements of the tree in the image shown above. Starting from top, Left to right Starting from bottom, Left to right Although this process is somewhat easy, it doesn't respect the hierarchy of the tree, only the depth of the nodes. Instead, we use traversal methods that take into account the basic structure of a tree i.e. The struct node pointed to by left and right might have other left and right children so we should think of them as sub-trees instead of sub-nodes.  According to this structure, every tree is a combination of A node carrying data
	Two subtrees A node carrying data Two subtrees Remember that our goal is to visit each node, so we need to visit all the nodes in the subtree, visit the root node and visit all the nodes in the right subtree as well. Depending on the order in which we do this, there can be three types of traversal. First, visit all the nodes in the left subtree Then the root node Visit all the nodes in the right subtree Visit root node Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit the root node Let's visualize in-order traversal. We start from the root node. We traverse the left subtree first. We also need to remember to visit the root node and the right subtree when this tree is done. Let's put all this in a stack so that we remember. Now we traverse to the subtree pointed on the TOP of the stack. Again, we follow the same rule of inorder After traversing the left subtree, we are left with Since the node ""5"" doesn't have any subtrees, we print it directly. After that we print its parent ""12"" and then the right child ""6"". Putting everything on a stack was helpful because now that the left-subtree of the root node has been traversed, we can print it and go to the right subtree. After going through all the elements, we get the inorder traversal as We don't have to create the stack ourselves because recursion maintains the correct order for us.","// Tree traversal in C

#include <stdio.h>
#include <stdlib.h>

struct node {
  int item;
  struct node* left;
  struct node* right;
};

// Inorder traversal
void inorderTraversal(struct node* root) {
  if (root == NULL) return;
  inorderTraversal(root->left);
  printf(""%d ->"", root->item);
  inorderTraversal(root->right);
}

// preorderTraversal traversal
void preorderTraversal(struct node* root) {
  if (root == NULL) return;
  printf(""%d ->"", root->item);
  preorderTraversal(root->left);
  preorderTraversal(root->right);
}

// postorderTraversal traversal
void postorderTraversal(struct node* root) {
  if (root == NULL) return;
  postorderTraversal(root->left);
  postorderTraversal(root->right);
  printf(""%d ->"", root->item);
}

// Create a new Node
struct node* createNode(value) {
  struct node* newNode = malloc(sizeof(struct node));
  newNode->item = value;
  newNode->left = NULL;
  newNode->right = NULL;

  return newNode;
}

// Insert on the left of the node
struct node* insertLeft(struct node* root, int value) {
  root->left = createNode(value);
  return root->left;
}

// Insert on the right of the node
struct node* insertRight(struct node* root, int value) {
  root->right = createNode(value);
  return root->right;
}

int main() {
  struct node* root = createNode(1);
  insertLeft(root, 12);
  insertRight(root, 9);

  insertLeft(root->left, 5);
  insertRight(root->left, 6);

  printf(""Inorder traversal \n"");
  inorderTraversal(root);

  printf(""\nPreorder traversal \n"");
  preorderTraversal(root);

  printf(""\nPostorder traversal \n"");
  postorderTraversal(root);
}"
Tree Traversal,"Traversing a tree means visiting every node in the tree. You might, for instance, want to add all the values in the tree or find the largest one. For all these operations, you will need to visit each node of the tree. Linear data structures like arrays, stacks, queues, and linked list have only one way to read the data. But a hierarchical data structure like a tree can be traversed in different ways. Let's think about how we can read the elements of the tree in the image shown above. Starting from top, Left to right Starting from bottom, Left to right Although this process is somewhat easy, it doesn't respect the hierarchy of the tree, only the depth of the nodes. Instead, we use traversal methods that take into account the basic structure of a tree i.e. The struct node pointed to by left and right might have other left and right children so we should think of them as sub-trees instead of sub-nodes.  According to this structure, every tree is a combination of A node carrying data
	Two subtrees A node carrying data Two subtrees Remember that our goal is to visit each node, so we need to visit all the nodes in the subtree, visit the root node and visit all the nodes in the right subtree as well. Depending on the order in which we do this, there can be three types of traversal. First, visit all the nodes in the left subtree Then the root node Visit all the nodes in the right subtree Visit root node Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit all the nodes in the left subtree Visit all the nodes in the right subtree Visit the root node Let's visualize in-order traversal. We start from the root node. We traverse the left subtree first. We also need to remember to visit the root node and the right subtree when this tree is done. Let's put all this in a stack so that we remember. Now we traverse to the subtree pointed on the TOP of the stack. Again, we follow the same rule of inorder After traversing the left subtree, we are left with Since the node ""5"" doesn't have any subtrees, we print it directly. After that we print its parent ""12"" and then the right child ""6"". Putting everything on a stack was helpful because now that the left-subtree of the root node has been traversed, we can print it and go to the right subtree. After going through all the elements, we get the inorder traversal as We don't have to create the stack ourselves because recursion maintains the correct order for us.","// Tree traversal in C++

#include <iostream>
using namespace std;

struct Node {
  int data;
  struct Node *left, *right;
  Node(int data) {
    this->data = data;
    left = right = NULL;
  }
};

// Preorder traversal
void preorderTraversal(struct Node* node) {
  if (node == NULL)
    return;

  cout << node->data << ""->"";
  preorderTraversal(node->left);
  preorderTraversal(node->right);
}

// Postorder traversal
void postorderTraversal(struct Node* node) {
  if (node == NULL)
    return;

  postorderTraversal(node->left);
  postorderTraversal(node->right);
  cout << node->data << ""->"";
}

// Inorder traversal
void inorderTraversal(struct Node* node) {
  if (node == NULL)
    return;

  inorderTraversal(node->left);
  cout << node->data << ""->"";
  inorderTraversal(node->right);
}

int main() {
  struct Node* root = new Node(1);
  root->left = new Node(12);
  root->right = new Node(9);
  root->left->left = new Node(5);
  root->left->right = new Node(6);

  cout << ""Inorder traversal "";
  inorderTraversal(root);

  cout << ""\nPreorder traversal "";
  preorderTraversal(root);

  cout << ""\nPostorder traversal "";
  postorderTraversal(root);"
Binary Tree,"A binary tree is a tree data structure in which each parent node can have at most two children. Each node of a binary tree consists of three items: data item
    

    
        address of left child
    

    
        address of right child data item data item address of left child address of left child address of right child address of right child A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. To learn more, please visit full binary tree. A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. To learn more, please visit perfect binary tree.  A complete binary tree is just like a full binary tree, but with two major differences Every level must be completely filled All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. To learn more, please visit complete binary tree. A degenerate or pathological tree is the tree having a single child either left or right. A skewed binary tree is a pathological/degenerate tree in which the tree is either dominated by the left nodes or the right nodes. Thus, there are two types of skewed binary tree: left-skewed binary tree and right-skewed binary tree. It is a type of binary tree in which the difference between the height of the left and the right subtree for each node is either 0 or 1. To learn more, please visit balanced binary tree. A node of a binary tree is represented by a structure containing a data part and two pointers to other structures of the same type. For easy and quick access to data
	In router algorithms
	To implement heap data structure
	Syntax tree For easy and quick access to data In router algorithms To implement heap data structure Syntax tree","# Binary Tree in Python

class Node:
    def __init__(self, key):
        self.left = None
        self.right = None
        self.val = key

    # Traverse preorder
    def traversePreOrder(self):
        print(self.val, end=' ')
        if self.left:
            self.left.traversePreOrder()
        if self.right:
            self.right.traversePreOrder()

    # Traverse inorder
    def traverseInOrder(self):
        if self.left:
            self.left.traverseInOrder()
        print(self.val, end=' ')
        if self.right:
            self.right.traverseInOrder()

    # Traverse postorder
    def traversePostOrder(self):
        if self.left:
            self.left.traversePostOrder()
        if self.right:
            self.right.traversePostOrder()
        print(self.val, end=' ')


root = Node(1)

root.left = Node(2)
root.right = Node(3)

root.left.left = Node(4)

print(""Pre order Traversal: "", end="""")
root.traversePreOrder()
print(""\nIn order Traversal: "", end="""")
root.traverseInOrder()
print(""\nPost order Traversal: "", end="""")
root.traversePostOrder()"
Binary Tree,"A binary tree is a tree data structure in which each parent node can have at most two children. Each node of a binary tree consists of three items: data item
    

    
        address of left child
    

    
        address of right child data item data item address of left child address of left child address of right child address of right child A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. To learn more, please visit full binary tree. A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. To learn more, please visit perfect binary tree.  A complete binary tree is just like a full binary tree, but with two major differences Every level must be completely filled All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. To learn more, please visit complete binary tree. A degenerate or pathological tree is the tree having a single child either left or right. A skewed binary tree is a pathological/degenerate tree in which the tree is either dominated by the left nodes or the right nodes. Thus, there are two types of skewed binary tree: left-skewed binary tree and right-skewed binary tree. It is a type of binary tree in which the difference between the height of the left and the right subtree for each node is either 0 or 1. To learn more, please visit balanced binary tree. A node of a binary tree is represented by a structure containing a data part and two pointers to other structures of the same type. For easy and quick access to data
	In router algorithms
	To implement heap data structure
	Syntax tree For easy and quick access to data In router algorithms To implement heap data structure Syntax tree","// Binary Tree in Java

// Node creation
class Node {
  int key;
  Node left, right;

  public Node(int item) {
  key = item;
  left = right = null;
  }
}

class BinaryTree {
  Node root;

  BinaryTree(int key) {
  root = new Node(key);
  }

  BinaryTree() {
  root = null;
  }

  // Traverse Inorder
  public void traverseInOrder(Node node) {
  if (node != null) {
    traverseInOrder(node.left);
    System.out.print("" "" + node.key);
    traverseInOrder(node.right);
  }
  }

  // Traverse Postorder
  public void traversePostOrder(Node node) {
  if (node != null) {
    traversePostOrder(node.left);
    traversePostOrder(node.right);
    System.out.print("" "" + node.key);
  }
  }

  // Traverse Preorder
  public void traversePreOrder(Node node) {
  if (node != null) {
    System.out.print("" "" + node.key);
    traversePreOrder(node.left);
    traversePreOrder(node.right);
  }
  }

  public static void main(String[] args) {
  BinaryTree tree = new BinaryTree();

  tree.root = new Node(1);
  tree.root.left = new Node(2);
  tree.root.right = new Node(3);
  tree.root.left.left = new Node(4);

  System.out.print(""Pre order Traversal: "");
  tree.traversePreOrder(tree.root);
  System.out.print(""\nIn order Traversal: "");
  tree.traverseInOrder(tree.root);
  System.out.print(""\nPost order Traversal: "");
  tree.traversePostOrder(tree.root);
  }
}"
Binary Tree,"A binary tree is a tree data structure in which each parent node can have at most two children. Each node of a binary tree consists of three items: data item
    

    
        address of left child
    

    
        address of right child data item data item address of left child address of left child address of right child address of right child A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. To learn more, please visit full binary tree. A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. To learn more, please visit perfect binary tree.  A complete binary tree is just like a full binary tree, but with two major differences Every level must be completely filled All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. To learn more, please visit complete binary tree. A degenerate or pathological tree is the tree having a single child either left or right. A skewed binary tree is a pathological/degenerate tree in which the tree is either dominated by the left nodes or the right nodes. Thus, there are two types of skewed binary tree: left-skewed binary tree and right-skewed binary tree. It is a type of binary tree in which the difference between the height of the left and the right subtree for each node is either 0 or 1. To learn more, please visit balanced binary tree. A node of a binary tree is represented by a structure containing a data part and two pointers to other structures of the same type. For easy and quick access to data
	In router algorithms
	To implement heap data structure
	Syntax tree For easy and quick access to data In router algorithms To implement heap data structure Syntax tree","// Tree traversal in C

#include <stdio.h>
#include <stdlib.h>

struct node {
  int item;
  struct node* left;
  struct node* right;
};

// Inorder traversal
void inorderTraversal(struct node* root) {
  if (root == NULL) return;
  inorderTraversal(root->left);
  printf(""%d ->"", root->item);
  inorderTraversal(root->right);
}

// Preorder traversal
void preorderTraversal(struct node* root) {
  if (root == NULL) return;
  printf(""%d ->"", root->item);
  preorderTraversal(root->left);
  preorderTraversal(root->right);
}

// Postorder traversal
void postorderTraversal(struct node* root) {
  if (root == NULL) return;
  postorderTraversal(root->left);
  postorderTraversal(root->right);
  printf(""%d ->"", root->item);
}

// Create a new Node
struct node* createNode(value) {
  struct node* newNode = malloc(sizeof(struct node));
  newNode->item = value;
  newNode->left = NULL;
  newNode->right = NULL;

  return newNode;
}

// Insert on the left of the node
struct node* insertLeft(struct node* root, int value) {
  root->left = createNode(value);
  return root->left;
}

// Insert on the right of the node
struct node* insertRight(struct node* root, int value) {
  root->right = createNode(value);
  return root->right;
}

int main() {
  struct node* root = createNode(1);
  insertLeft(root, 2);
  insertRight(root, 3);
  insertLeft(root->left, 4);

  printf(""Inorder traversal \n"");
  inorderTraversal(root);

  printf(""\nPreorder traversal \n"");
  preorderTraversal(root);

  printf(""\nPostorder traversal \n"");
  postorderTraversal(root);
}"
Binary Tree,"A binary tree is a tree data structure in which each parent node can have at most two children. Each node of a binary tree consists of three items: data item
    

    
        address of left child
    

    
        address of right child data item data item address of left child address of left child address of right child address of right child A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. To learn more, please visit full binary tree. A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. To learn more, please visit perfect binary tree.  A complete binary tree is just like a full binary tree, but with two major differences Every level must be completely filled All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. To learn more, please visit complete binary tree. A degenerate or pathological tree is the tree having a single child either left or right. A skewed binary tree is a pathological/degenerate tree in which the tree is either dominated by the left nodes or the right nodes. Thus, there are two types of skewed binary tree: left-skewed binary tree and right-skewed binary tree. It is a type of binary tree in which the difference between the height of the left and the right subtree for each node is either 0 or 1. To learn more, please visit balanced binary tree. A node of a binary tree is represented by a structure containing a data part and two pointers to other structures of the same type. For easy and quick access to data
	In router algorithms
	To implement heap data structure
	Syntax tree For easy and quick access to data In router algorithms To implement heap data structure Syntax tree","// Binary Tree in C++

#include <stdlib.h>

#include <iostream>

using namespace std;

struct node {
  int data;
  struct node *left;
  struct node *right;
};

// New node creation
struct node *newNode(int data) {
  struct node *node = (struct node *)malloc(sizeof(struct node));

  node->data = data;

  node->left = NULL;
  node->right = NULL;
  return (node);
}

// Traverse Preorder
void traversePreOrder(struct node *temp) {
  if (temp != NULL) {
    cout << "" "" << temp->data;
    traversePreOrder(temp->left);
    traversePreOrder(temp->right);
  }
}

// Traverse Inorder
void traverseInOrder(struct node *temp) {
  if (temp != NULL) {
    traverseInOrder(temp->left);
    cout << "" "" << temp->data;
    traverseInOrder(temp->right);
  }
}

// Traverse Postorder
void traversePostOrder(struct node *temp) {
  if (temp != NULL) {
    traversePostOrder(temp->left);
    traversePostOrder(temp->right);
    cout << "" "" << temp->data;
  }
}

int main() {
  struct node *root = newNode(1);
  root->left = newNode(2);
  root->right = newNode(3);
  root->left->left = newNode(4);

  cout << ""preorder traversal: "";
  traversePreOrder(root);
  cout << ""\nInorder traversal: "";
  traverseInOrder(root);
  cout << ""\nPostorder traversal: "";
  traversePostOrder(root);
}"
Full Binary Tree,A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. It is also known as a proper binary tree. The number of leaves is i + 1. The total number of nodes is 2i + 1. The number of internal nodes is (n – 1) / 2. The number of leaves is (n + 1) / 2. The total number of nodes is 2l – 1. The number of internal nodes is l – 1. The number of leaves is at most 2λ - 1. The following code is for checking if a tree is a full binary tree.,"# Checking if a binary tree is a full binary tree in Python


# Creating a node
class Node:

    def __init__(self, item):
        self.item = item
        self.leftChild = None
        self.rightChild = None


# Checking full binary tree
def isFullTree(root):

    # Tree empty case
    if root is None:
        return True

    # Checking whether child is present
    if root.leftChild is None and root.rightChild is None:
        return True

    if root.leftChild is not None and root.rightChild is not None:
        return (isFullTree(root.leftChild) and isFullTree(root.rightChild))

    return False


root = Node(1)
root.rightChild = Node(3)
root.leftChild = Node(2)

root.leftChild.leftChild = Node(4)
root.leftChild.rightChild = Node(5)
root.leftChild.rightChild.leftChild = Node(6)
root.leftChild.rightChild.rightChild = Node(7)

if isFullTree(root):
    print(""The tree is a full binary tree"")
else:
    print(""The tree is not a full binary tree"")
"
Full Binary Tree,A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. It is also known as a proper binary tree. The number of leaves is i + 1. The total number of nodes is 2i + 1. The number of internal nodes is (n – 1) / 2. The number of leaves is (n + 1) / 2. The total number of nodes is 2l – 1. The number of internal nodes is l – 1. The number of leaves is at most 2λ - 1. The following code is for checking if a tree is a full binary tree.,"// Checking if a binary tree is a full binary tree in Java

class Node {
  int data;
  Node leftChild, rightChild;

  Node(int item) {
  data = item;
  leftChild = rightChild = null;
  }
}

class BinaryTree {
  Node root;

  // Check for Full Binary Tree
  boolean isFullBinaryTree(Node node) {

  // Checking tree emptiness
  if (node == null)
    return true;

  // Checking the children
  if (node.leftChild == null && node.rightChild == null)
    return true;

  if ((node.leftChild != null) && (node.rightChild != null))
    return (isFullBinaryTree(node.leftChild) && isFullBinaryTree(node.rightChild));

  return false;
  }

  public static void main(String args[]) {
    BinaryTree tree = new BinaryTree();
    tree.root = new Node(1);
    tree.root.leftChild = new Node(2);
    tree.root.rightChild = new Node(3);
    tree.root.leftChild.leftChild = new Node(4);
    tree.root.leftChild.rightChild = new Node(5);
    tree.root.rightChild.leftChild = new Node(6);
    tree.root.rightChild.rightChild = new Node(7);

    if (tree.isFullBinaryTree(tree.root))
      System.out.print(""The tree is a full binary tree"");
    else
      System.out.print(""The tree is not a full binary tree"");
  }
}"
Full Binary Tree,A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. It is also known as a proper binary tree. The number of leaves is i + 1. The total number of nodes is 2i + 1. The number of internal nodes is (n – 1) / 2. The number of leaves is (n + 1) / 2. The total number of nodes is 2l – 1. The number of internal nodes is l – 1. The number of leaves is at most 2λ - 1. The following code is for checking if a tree is a full binary tree.,"// Checking if a binary tree is a full binary tree in C

#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>

struct Node {
  int item;
  struct Node *left, *right;
};

// Creation of new Node
struct Node *createNewNode(char k) {
  struct Node *node = (struct Node *)malloc(sizeof(struct Node));
  node->item = k;
  node->right = node->left = NULL;
  return node;
}

bool isFullBinaryTree(struct Node *root) {
  // Checking tree emptiness
  if (root == NULL)
    return true;

  // Checking the presence of children
  if (root->left == NULL && root->right == NULL)
    return true;

  if ((root->left) && (root->right))
    return (isFullBinaryTree(root->left) && isFullBinaryTree(root->right));

  return false;
}

int main() {
  struct Node *root = NULL;
  root = createNewNode(1);
  root->left = createNewNode(2);
  root->right = createNewNode(3);

  root->left->left = createNewNode(4);
  root->left->right = createNewNode(5);
  root->left->right->left = createNewNode(6);
  root->left->right->right = createNewNode(7);

  if (isFullBinaryTree(root))
    printf(""The tree is a full binary tree\n"");
  else
    printf(""The tree is not a full binary tree\n"");
}"
Full Binary Tree,A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children. It is also known as a proper binary tree. The number of leaves is i + 1. The total number of nodes is 2i + 1. The number of internal nodes is (n – 1) / 2. The number of leaves is (n + 1) / 2. The total number of nodes is 2l – 1. The number of internal nodes is l – 1. The number of leaves is at most 2λ - 1. The following code is for checking if a tree is a full binary tree.,"// Checking if a binary tree is a full binary tree in C++

#include <iostream>
using namespace std;

struct Node {
  int key;
  struct Node *left, *right;
};

// New node creation
struct Node *newNode(char k) {
  struct Node *node = (struct Node *)malloc(sizeof(struct Node));
  node->key = k;
  node->right = node->left = NULL;
  return node;
}

bool isFullBinaryTree(struct Node *root) {
  
  // Checking for emptiness
  if (root == NULL)
    return true;

  // Checking for the presence of children
  if (root->left == NULL && root->right == NULL)
    return true;

  if ((root->left) && (root->right))
    return (isFullBinaryTree(root->left) && isFullBinaryTree(root->right));

  return false;
}

int main() {
  struct Node *root = NULL;
  root = newNode(1);
  root->left = newNode(2);
  root->right = newNode(3);
  root->left->left = newNode(4);
  root->left->right = newNode(5);
  root->left->right->left = newNode(6);
  root->left->right->right = newNode(7);

  if (isFullBinaryTree(root))
    cout << ""The tree is a full binary tree\n"";
  else
    cout << ""The tree is not a full binary tree\n"";
}"
Perfect Binary Tree,"A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. All the internal nodes have a degree of 2. Recursively, a perfect binary tree can be defined as: If a single node has no children, it is a perfect binary tree of height h = 0, If a node has h > 0, it is a perfect binary tree if both of its subtrees are of height h - 1 and are non-overlapping.  The following code is for checking whether a tree is a perfect binary tree. A perfect binary tree of height h has 2h + 1 – 1 node. A perfect binary tree with n nodes has height log(n + 1) – 1 = Θ(ln(n)). A perfect binary tree of height h has 2h leaf nodes. The average depth of a node in a perfect binary tree is Θ(ln(n)).","# Checking if a binary tree is a perfect binary tree in Python


class newNode:
    def __init__(self, k):
        self.key = k
        self.right = self.left = None


# Calculate the depth
def calculateDepth(node):
    d = 0
    while (node is not None):
        d += 1
        node = node.left
    return d


# Check if the tree is perfect binary tree
def is_perfect(root, d, level=0):

    # Check if the tree is empty
    if (root is None):
        return True

    # Check the presence of trees
    if (root.left is None and root.right is None):
        return (d == level + 1)

    if (root.left is None or root.right is None):
        return False

    return (is_perfect(root.left, d, level + 1) and
            is_perfect(root.right, d, level + 1))


root = None
root = newNode(1)
root.left = newNode(2)
root.right = newNode(3)
root.left.left = newNode(4)
root.left.right = newNode(5)

if (is_perfect(root, calculateDepth(root))):
    print(""The tree is a perfect binary tree"")
else:
    print(""The tree is not a perfect binary tree"")"
Perfect Binary Tree,"A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. All the internal nodes have a degree of 2. Recursively, a perfect binary tree can be defined as: If a single node has no children, it is a perfect binary tree of height h = 0, If a node has h > 0, it is a perfect binary tree if both of its subtrees are of height h - 1 and are non-overlapping.  The following code is for checking whether a tree is a perfect binary tree. A perfect binary tree of height h has 2h + 1 – 1 node. A perfect binary tree with n nodes has height log(n + 1) – 1 = Θ(ln(n)). A perfect binary tree of height h has 2h leaf nodes. The average depth of a node in a perfect binary tree is Θ(ln(n)).","// Checking if a binary tree is a perfect binary tree in Java

class PerfectBinaryTree {

  static class Node {
    int key;
    Node left, right;
  }

  // Calculate the depth
  static int depth(Node node) {
    int d = 0;
    while (node != null) {
      d++;
      node = node.left;
    }
    return d;
  }

  // Check if the tree is perfect binary tree
  static boolean is_perfect(Node root, int d, int level) {

    // Check if the tree is empty
    if (root == null)
      return true;

    // If for children
    if (root.left == null && root.right == null)
      return (d == level + 1);

    if (root.left == null || root.right == null)
      return false;

    return is_perfect(root.left, d, level + 1) && is_perfect(root.right, d, level + 1);
  }

  // Wrapper function
  static boolean is_Perfect(Node root) {
    int d = depth(root);
    return is_perfect(root, d, 0);
  }

  // Create a new node
  static Node newNode(int k) {
    Node node = new Node();
    node.key = k;
    node.right = null;
    node.left = null;
    return node;
  }

  public static void main(String args[]) {
    Node root = null;
    root = newNode(1);
    root.left = newNode(2);
    root.right = newNode(3);
    root.left.left = newNode(4);
    root.left.right = newNode(5);

    if (is_Perfect(root) == true)
      System.out.println(""The tree is a perfect binary tree"");
    else
      System.out.println(""The tree is not a perfect binary tree"");
  }
}"
Perfect Binary Tree,"A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. All the internal nodes have a degree of 2. Recursively, a perfect binary tree can be defined as: If a single node has no children, it is a perfect binary tree of height h = 0, If a node has h > 0, it is a perfect binary tree if both of its subtrees are of height h - 1 and are non-overlapping.  The following code is for checking whether a tree is a perfect binary tree. A perfect binary tree of height h has 2h + 1 – 1 node. A perfect binary tree with n nodes has height log(n + 1) – 1 = Θ(ln(n)). A perfect binary tree of height h has 2h leaf nodes. The average depth of a node in a perfect binary tree is Θ(ln(n)).","// Checking if a binary tree is a perfect binary tree in C

#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>

struct node {
  int data;
  struct node *left;
  struct node *right;
};

// Creating a new node
struct node *newnode(int data) {
  struct node *node = (struct node *)malloc(sizeof(struct node));
  node->data = data;
  node->left = NULL;
  node->right = NULL;

  return (node);
}

// Calculate the depth
int depth(struct node *node) {
  int d = 0;
  while (node != NULL) {
    d++;
    node = node->left;
  }
  return d;
}

// Check if the tree is perfect
bool is_perfect(struct node *root, int d, int level) {
    // Check if the tree is empty
  if (root == NULL)
    return true;

  // Check the presence of children
  if (root->left == NULL && root->right == NULL)
    return (d == level + 1);

  if (root->left == NULL || root->right == NULL)
    return false;

  return is_perfect(root->left, d, level + 1) &&
       is_perfect(root->right, d, level + 1);
}

// Wrapper function
bool is_Perfect(struct node *root) {
  int d = depth(root);
  return is_perfect(root, d, 0);
}

int main() {
  struct node *root = NULL;
  root = newnode(1);
  root->left = newnode(2);
  root->right = newnode(3);
  root->left->left = newnode(4);
  root->left->right = newnode(5);
  root->right->left = newnode(6);

  if (is_Perfect(root))
    printf(""The tree is a perfect binary tree\n"");
  else
    printf(""The tree is not a perfect binary tree\n"");
}"
Perfect Binary Tree,"A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level. All the internal nodes have a degree of 2. Recursively, a perfect binary tree can be defined as: If a single node has no children, it is a perfect binary tree of height h = 0, If a node has h > 0, it is a perfect binary tree if both of its subtrees are of height h - 1 and are non-overlapping.  The following code is for checking whether a tree is a perfect binary tree. A perfect binary tree of height h has 2h + 1 – 1 node. A perfect binary tree with n nodes has height log(n + 1) – 1 = Θ(ln(n)). A perfect binary tree of height h has 2h leaf nodes. The average depth of a node in a perfect binary tree is Θ(ln(n)).","// Checking if a binary tree is a perfect binary tree in C++

#include <iostream>
using namespace std;

struct Node {
  int key;
  struct Node *left, *right;
};

int depth(Node *node) {
  int d = 0;
  while (node != NULL) {
    d++;
    node = node->left;
  }
  return d;
}

bool isPerfectR(struct Node *root, int d, int level = 0) {
  if (root == NULL)
    return true;

  if (root->left == NULL && root->right == NULL)
    return (d == level + 1);

  if (root->left == NULL || root->right == NULL)
    return false;

  return isPerfectR(root->left, d, level + 1) &&
       isPerfectR(root->right, d, level + 1);
}

bool isPerfect(Node *root) {
  int d = depth(root);
  return isPerfectR(root, d);
}

struct Node *newNode(int k) {
  struct Node *node = new Node;
  node->key = k;
  node->right = node->left = NULL;
  return node;
}

int main() {
  struct Node *root = NULL;
  root = newNode(1);
  root->left = newNode(2);
  root->right = newNode(3);
  root->left->left = newNode(4);
  root->left->right = newNode(5);
  root->right->left = newNode(6);

  if (isPerfect(root))
    cout << ""The tree is a perfect binary tree\n"";
  else
    cout << ""The tree is not a perfect binary tree\n"";
}"
Complete Binary Tree,"A complete binary tree is a binary tree in which all the levels are completely filled except possibly the lowest one, which is filled from the left. A complete binary tree is just like a full binary tree, but with two major differences All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. Select the first element of the list to be the root node. (no. of elements on level-I: 1)
		
			Select the first element as root Put the second element as a left child of the root node and the third element as the right child. (no. of elements on level-II: 2)
		
			12 as a left child and 9 as a right child Put the next two elements as children of the left node of the second level. Again, put the next two elements as children of the right node of the second level (no. of elements on level-III: 4) elements). Keep repeating until you reach the last element.
		
			5 as a left child and 6 as a right child A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2.  Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap-based data structures
	Heap sort Heap-based data structures Heap sort","# Checking if a binary tree is a complete binary tree in C


class Node:

    def __init__(self, item):
        self.item = item
        self.left = None
        self.right = None


# Count the number of nodes
def count_nodes(root):
    if root is None:
        return 0
    return (1 + count_nodes(root.left) + count_nodes(root.right))


# Check if the tree is complete binary tree
def is_complete(root, index, numberNodes):

    # Check if the tree is empty
    if root is None:
        return True

    if index >= numberNodes:
        return False

    return (is_complete(root.left, 2 * index + 1, numberNodes)
            and is_complete(root.right, 2 * index + 2, numberNodes))


root = Node(1)
root.left = Node(2)
root.right = Node(3)
root.left.left = Node(4)
root.left.right = Node(5)
root.right.left = Node(6)

node_count = count_nodes(root)
index = 0

if is_complete(root, index, node_count):
    print(""The tree is a complete binary tree"")
else:
    print(""The tree is not a complete binary tree"")
"
Complete Binary Tree,"A complete binary tree is a binary tree in which all the levels are completely filled except possibly the lowest one, which is filled from the left. A complete binary tree is just like a full binary tree, but with two major differences All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. Select the first element of the list to be the root node. (no. of elements on level-I: 1)
		
			Select the first element as root Put the second element as a left child of the root node and the third element as the right child. (no. of elements on level-II: 2)
		
			12 as a left child and 9 as a right child Put the next two elements as children of the left node of the second level. Again, put the next two elements as children of the right node of the second level (no. of elements on level-III: 4) elements). Keep repeating until you reach the last element.
		
			5 as a left child and 6 as a right child A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2.  Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap-based data structures
	Heap sort Heap-based data structures Heap sort","// Checking if a binary tree is a complete binary tree in Java

// Node creation
class Node {
  int data;
  Node left, right;

  Node(int item) {
    data = item;
    left = right = null;
  }
}

class BinaryTree {
  Node root;

  // Count the number of nodes
  int countNumNodes(Node root) {
    if (root == null)
      return (0);
    return (1 + countNumNodes(root.left) + countNumNodes(root.right));
  }

  // Check for complete binary tree
  boolean checkComplete(Node root, int index, int numberNodes) {

    // Check if the tree is empty
    if (root == null)
      return true;

    if (index >= numberNodes)
      return false;

    return (checkComplete(root.left, 2 * index + 1, numberNodes)
        && checkComplete(root.right, 2 * index + 2, numberNodes));
  }

  public static void main(String args[]) {
    BinaryTree tree = new BinaryTree();

    tree.root = new Node(1);
    tree.root.left = new Node(2);
    tree.root.right = new Node(3);
    tree.root.left.right = new Node(5);
    tree.root.left.left = new Node(4);
    tree.root.right.left = new Node(6);

    int node_count = tree.countNumNodes(tree.root);
    int index = 0;

    if (tree.checkComplete(tree.root, index, node_count))
      System.out.println(""The tree is a complete binary tree"");
    else
      System.out.println(""The tree is not a complete binary tree"");
  }
}"
Complete Binary Tree,"A complete binary tree is a binary tree in which all the levels are completely filled except possibly the lowest one, which is filled from the left. A complete binary tree is just like a full binary tree, but with two major differences All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. Select the first element of the list to be the root node. (no. of elements on level-I: 1)
		
			Select the first element as root Put the second element as a left child of the root node and the third element as the right child. (no. of elements on level-II: 2)
		
			12 as a left child and 9 as a right child Put the next two elements as children of the left node of the second level. Again, put the next two elements as children of the right node of the second level (no. of elements on level-III: 4) elements). Keep repeating until you reach the last element.
		
			5 as a left child and 6 as a right child A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2.  Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap-based data structures
	Heap sort Heap-based data structures Heap sort","// Checking if a binary tree is a complete binary tree in C

#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>

struct Node {
  int key;
  struct Node *left, *right;
};

// Node creation
struct Node *newNode(char k) {
  struct Node *node = (struct Node *)malloc(sizeof(struct Node));
  node->key = k;
  node->right = node->left = NULL;
  return node;
}

// Count the number of nodes
int countNumNodes(struct Node *root) {
  if (root == NULL)
    return (0);
  return (1 + countNumNodes(root->left) + countNumNodes(root->right));
}

// Check if the tree is a complete binary tree
bool checkComplete(struct Node *root, int index, int numberNodes) {
  // Check if the tree is complete
  if (root == NULL)
    return true;

  if (index >= numberNodes)
    return false;

  return (checkComplete(root->left, 2 * index + 1, numberNodes) && checkComplete(root->right, 2 * index + 2, numberNodes));
}

int main() {
  struct Node *root = NULL;
  root = newNode(1);
  root->left = newNode(2);
  root->right = newNode(3);
  root->left->left = newNode(4);
  root->left->right = newNode(5);
  root->right->left = newNode(6);

  int node_count = countNumNodes(root);
  int index = 0;

  if (checkComplete(root, index, node_count))
    printf(""The tree is a complete binary tree\n"");
  else
    printf(""The tree is not a complete binary tree\n"");
}"
Complete Binary Tree,"A complete binary tree is a binary tree in which all the levels are completely filled except possibly the lowest one, which is filled from the left. A complete binary tree is just like a full binary tree, but with two major differences All the leaf elements must lean towards the left. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree. Select the first element of the list to be the root node. (no. of elements on level-I: 1)
		
			Select the first element as root Put the second element as a left child of the root node and the third element as the right child. (no. of elements on level-II: 2)
		
			12 as a left child and 9 as a right child Put the next two elements as children of the left node of the second level. Again, put the next two elements as children of the right node of the second level (no. of elements on level-III: 4) elements). Keep repeating until you reach the last element.
		
			5 as a left child and 6 as a right child A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2.  Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap-based data structures
	Heap sort Heap-based data structures Heap sort","// Checking if a binary tree is a complete binary tree in C++

#include <iostream>

using namespace std;

struct Node {
  int key;
  struct Node *left, *right;
};

// Create node
struct Node *newNode(char k) {
  struct Node *node = (struct Node *)malloc(sizeof(struct Node));
  node->key = k;
  node->right = node->left = NULL;
  return node;
}

// Count the number of nodes
int countNumNodes(struct Node *root) {
  if (root == NULL)
    return (0);
  return (1 + countNumNodes(root->left) + countNumNodes(root->right));
}

// Check if the tree is a complete binary tree
bool checkComplete(struct Node *root, int index, int numberNodes) {
  
  // Check if the tree is empty
  if (root == NULL)
    return true;

  if (index >= numberNodes)
    return false;

  return (checkComplete(root->left, 2 * index + 1, numberNodes) && checkComplete(root->right, 2 * index + 2, numberNodes));
}

int main() {
  struct Node *root = NULL;
  root = newNode(1);
  root->left = newNode(2);
  root->right = newNode(3);
  root->left->left = newNode(4);
  root->left->right = newNode(5);
  root->right->left = newNode(6);

  int node_count = countNumNodes(root);
  int index = 0;

  if (checkComplete(root, index, node_count))
    cout << ""The tree is a complete binary tree\n"";
  else
    cout << ""The tree is not a complete binary tree\n"";
}
"
Balanced Binary Tree,"A balanced binary tree, also referred to as a height-balanced binary tree, is defined as a binary tree in which the height of the left and right subtree of any node differ by not more than 1. To learn more about the height of a tree/node, visit Tree Data Structure.Following are the conditions for a height-balanced binary tree: difference between the left and the right subtree for any node is not more than one the left subtree is balanced the right subtree is balanced The following code is for checking whether a tree is height-balanced. AVL tree
	Balanced Binary Search Tree AVL tree Balanced Binary Search Tree","# Checking if a binary tree is height balanced in Python


class Node:

    def __init__(self, data):
        self.data = data
        self.left = self.right = None


class Height:
    def __init__(self):
        self.height = 0


def isHeightBalanced(root, height):

    left_height = Height()
    right_height = Height()

    if root is None:
        return True

    l = isHeightBalanced(root.left, left_height)
    r = isHeightBalanced(root.right, right_height)

    height.height = max(left_height.height, right_height.height) + 1

    if abs(left_height.height - right_height.height) <= 1:
        return l and r

    return False


height = Height()

root = Node(1)
root.left = Node(2)
root.right = Node(3)
root.left.left = Node(4)
root.left.right = Node(5)

if isHeightBalanced(root, height):
    print('The tree is balanced')
else:
    print('The tree is not balanced')
"
Balanced Binary Tree,"A balanced binary tree, also referred to as a height-balanced binary tree, is defined as a binary tree in which the height of the left and right subtree of any node differ by not more than 1. To learn more about the height of a tree/node, visit Tree Data Structure.Following are the conditions for a height-balanced binary tree: difference between the left and the right subtree for any node is not more than one the left subtree is balanced the right subtree is balanced The following code is for checking whether a tree is height-balanced. AVL tree
	Balanced Binary Search Tree AVL tree Balanced Binary Search Tree","// Checking if a binary tree is height balanced in Java

// Node creation
class Node {

  int data;
  Node left, right;

  Node(int d) {
    data = d;
    left = right = null;
  }
}

// Calculate height
class Height {
  int height = 0;
}

class BinaryTree {

  Node root;

  // Check height balance
  boolean checkHeightBalance(Node root, Height height) {

    // Check for emptiness
    if (root == null) {
      height.height = 0;
      return true;
    }

    Height leftHeighteight = new Height(), rightHeighteight = new Height();
    boolean l = checkHeightBalance(root.left, leftHeighteight);
    boolean r = checkHeightBalance(root.right, rightHeighteight);
    int leftHeight = leftHeighteight.height, rightHeight = rightHeighteight.height;

    height.height = (leftHeight > rightHeight ? leftHeight : rightHeight) + 1;

    if ((leftHeight - rightHeight >= 2) || (rightHeight - leftHeight >= 2))
      return false;

    else
      return l && r;
  }

  public static void main(String args[]) {
    Height height = new Height();

    BinaryTree tree = new BinaryTree();
    tree.root = new Node(1);
    tree.root.left = new Node(2);
    tree.root.right = new Node(3);
    tree.root.left.left = new Node(4);
    tree.root.left.right = new Node(5);

    if (tree.checkHeightBalance(tree.root, height))
      System.out.println(""The tree is balanced"");
    else
      System.out.println(""The tree is not balanced"");
  }
}"
Balanced Binary Tree,"A balanced binary tree, also referred to as a height-balanced binary tree, is defined as a binary tree in which the height of the left and right subtree of any node differ by not more than 1. To learn more about the height of a tree/node, visit Tree Data Structure.Following are the conditions for a height-balanced binary tree: difference between the left and the right subtree for any node is not more than one the left subtree is balanced the right subtree is balanced The following code is for checking whether a tree is height-balanced. AVL tree
	Balanced Binary Search Tree AVL tree Balanced Binary Search Tree","// Checking if a binary tree is height balanced in C

#include <stdio.h>
#include <stdlib.h>
#define bool int

// Node creation
struct node {
  int item;
  struct node *left;
  struct node *right;
};

// Create a new node
struct node *newNode(int item) {
  struct node *node = (struct node *)malloc(sizeof(struct node));
  node->item = item;
  node->left = NULL;
  node->right = NULL;

  return (node);
}

// Check for height balance
bool checkHeightBalance(struct node *root, int *height) {
  // Check for emptiness
  int leftHeight = 0, rightHeight = 0;
  int l = 0, r = 0;

  if (root == NULL) {
    *height = 0;
    return 1;
  }

  l = checkHeightBalance(root->left, &leftHeight);
  r = checkHeightBalance(root->right, &rightHeight);

  *height = (leftHeight > rightHeight ? leftHeight : rightHeight) + 1;

  if ((leftHeight - rightHeight >= 2) || (rightHeight - leftHeight >= 2))
    return 0;

  else
    return l && r;
}

int main() {
  int height = 0;

  struct node *root = newNode(1);
  root->left = newNode(2);
  root->right = newNode(3);
  root->left->left = newNode(4);
  root->left->right = newNode(5);

  if (checkHeightBalance(root, &height))
    printf(""The tree is balanced"");
  else
    printf(""The tree is not balanced"");
}"
Balanced Binary Tree,"A balanced binary tree, also referred to as a height-balanced binary tree, is defined as a binary tree in which the height of the left and right subtree of any node differ by not more than 1. To learn more about the height of a tree/node, visit Tree Data Structure.Following are the conditions for a height-balanced binary tree: difference between the left and the right subtree for any node is not more than one the left subtree is balanced the right subtree is balanced The following code is for checking whether a tree is height-balanced. AVL tree
	Balanced Binary Search Tree AVL tree Balanced Binary Search Tree","// Checking if a binary tree is height balanced in C++

#include 
using namespace std;

#define bool int

class node {
   public:
  int item;
  node *left;
  node *right;
};

// Create anew node
node *newNode(int item) {
  node *Node = new node();
  Node->item = item;
  Node->left = NULL;
  Node->right = NULL;

  return (Node);
}

// Check height balance
bool checkHeightBalance(node *root, int *height) {
  // Check for emptiness
  int leftHeight = 0, rightHeight = 0;

  int l = 0, r = 0;

  if (root == NULL) {
    *height = 0;
    return 1;
  }

  l = checkHeightBalance(root->left, &leftHeight);
  r = checkHeightBalance(root->right, &rightHeight);

  *height = (leftHeight > rightHeight ? leftHeight : rightHeight) + 1;

  if (std::abs(leftHeight - rightHeight >= 2))
    return 0;

  else
    return l && r;
}

int main() {
  int height = 0;

  node *root = newNode(1);
  root->left = newNode(2);
  root->right = newNode(3);
  root->left->left = newNode(4);
  root->left->right = newNode(5);

  if (checkHeightBalance(root, &height))
    cout << ""The tree is balanced"";
  else
    cout << ""The tree is not balanced"";
}"
Binary Search Tree,"Binary search tree is a data structure that quickly allows us to maintain a sorted list of numbers. It is called a binary tree because each tree node has a maximum of two children.
	It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. It is called a binary tree because each tree node has a maximum of two children. It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. The properties that separate a binary search tree from a regular binary tree is All nodes of left subtree are less than the root node All nodes of right subtree are more than the root node Both subtrees of each node are also BSTs i.e. they have the above two properties The binary tree on the right isn't a binary search tree because the right subtree of the node ""3"" contains a value smaller than it. There are two basic operations that you can perform on a binary search tree: The algorithm depends on the property of BST that if each left subtree has values below root and each right subtree has values above the root. If the value is below the root, we can say for sure that the value is not in the right subtree; we need to only search in the left subtree and if the value is above the root, we can say for sure that the value is not in the left subtree; we need to only search in the right subtree. Algorithm: Let us try to visualize this with a diagram. If the value is found, we return the value so that it gets propagated in each recursion step as shown in the image below.  If you might have noticed, we have called return search(struct node*) four times. When we return either the new node or NULL, the value gets returned again and again until search(root) returns the final result. If the value is not found, we eventually reach the left or right child of a leaf node which is NULL and it gets propagated and returned. Inserting a value in the correct position is similar to searching because we try to maintain the rule that the left subtree is lesser than root and the right subtree is larger than root. We keep going to either right subtree or left subtree depending on the value and when we reach a point left or right subtree is null, we put the new node there. Algorithm: The algorithm isn't as simple as it looks. Let's try to visualize how we add a number to an existing BST. We have attached the node but we still have to exit from the function without doing any damage to the rest of the tree. This is where the return node; at the end comes in handy. In the case of NULL, the newly created node is returned and attached to the parent node, otherwise the same node is returned without any change as we go up until we return to the root. This makes sure that as we move back up the tree, the other node connections aren't changed. There are three cases for deleting a node from a binary search tree. In the first case, the node to be deleted is the leaf node. In such a case, simply delete the node from the tree. In the second case, the node to be deleted lies has a single child node. In such a case follow the steps below: Replace that node with its child node. Remove the child node from its original position. In the third case, the node to be deleted has two children. In such a case follow the steps below: Get the inorder successor of that node. Replace the node with the inorder successor. Remove the inorder successor from its original position. Here, n is the number of nodes in the tree. The space complexity for all the operations is O(n). In multilevel indexing in the database For dynamic sorting For managing virtual memory areas in Unix kernel","# Binary Search Tree operations in Python


# Create a node
class Node:
    def __init__(self, key):
        self.key = key
        self.left = None
        self.right = None


# Inorder traversal
def inorder(root):
    if root is not None:
        # Traverse left
        inorder(root.left)

        # Traverse root
        print(str(root.key) + ""->"", end=' ')

        # Traverse right
        inorder(root.right)


# Insert a node
def insert(node, key):

    # Return a new node if the tree is empty
    if node is None:
        return Node(key)

    # Traverse to the right place and insert the node
    if key < node.key:
        node.left = insert(node.left, key)
    else:
        node.right = insert(node.right, key)

    return node


# Find the inorder successor
def minValueNode(node):
    current = node

    # Find the leftmost leaf
    while(current.left is not None):
        current = current.left

    return current


# Deleting a node
def deleteNode(root, key):

    # Return if the tree is empty
    if root is None:
        return root

    # Find the node to be deleted
    if key < root.key:
        root.left = deleteNode(root.left, key)
    elif(key > root.key):
        root.right = deleteNode(root.right, key)
    else:
        # If the node is with only one child or no child
        if root.left is None:
            temp = root.right
            root = None
            return temp

        elif root.right is None:
            temp = root.left
            root = None
            return temp

        # If the node has two children,
        # place the inorder successor in position of the node to be deleted
        temp = minValueNode(root.right)

        root.key = temp.key

        # Delete the inorder successor
        root.right = deleteNode(root.right, temp.key)

    return root


root = None
root = insert(root, 8)
root = insert(root, 3)
root = insert(root, 1)
root = insert(root, 6)
root = insert(root, 7)
root = insert(root, 10)
root = insert(root, 14)
root = insert(root, 4)

print(""Inorder traversal: "", end=' ')
inorder(root)

print(""\nDelete 10"")
root = deleteNode(root, 10)
print(""Inorder traversal: "", end=' ')
inorder(root)"
Binary Search Tree,"Binary search tree is a data structure that quickly allows us to maintain a sorted list of numbers. It is called a binary tree because each tree node has a maximum of two children.
	It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. It is called a binary tree because each tree node has a maximum of two children. It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. The properties that separate a binary search tree from a regular binary tree is All nodes of left subtree are less than the root node All nodes of right subtree are more than the root node Both subtrees of each node are also BSTs i.e. they have the above two properties The binary tree on the right isn't a binary search tree because the right subtree of the node ""3"" contains a value smaller than it. There are two basic operations that you can perform on a binary search tree: The algorithm depends on the property of BST that if each left subtree has values below root and each right subtree has values above the root. If the value is below the root, we can say for sure that the value is not in the right subtree; we need to only search in the left subtree and if the value is above the root, we can say for sure that the value is not in the left subtree; we need to only search in the right subtree. Algorithm: Let us try to visualize this with a diagram. If the value is found, we return the value so that it gets propagated in each recursion step as shown in the image below.  If you might have noticed, we have called return search(struct node*) four times. When we return either the new node or NULL, the value gets returned again and again until search(root) returns the final result. If the value is not found, we eventually reach the left or right child of a leaf node which is NULL and it gets propagated and returned. Inserting a value in the correct position is similar to searching because we try to maintain the rule that the left subtree is lesser than root and the right subtree is larger than root. We keep going to either right subtree or left subtree depending on the value and when we reach a point left or right subtree is null, we put the new node there. Algorithm: The algorithm isn't as simple as it looks. Let's try to visualize how we add a number to an existing BST. We have attached the node but we still have to exit from the function without doing any damage to the rest of the tree. This is where the return node; at the end comes in handy. In the case of NULL, the newly created node is returned and attached to the parent node, otherwise the same node is returned without any change as we go up until we return to the root. This makes sure that as we move back up the tree, the other node connections aren't changed. There are three cases for deleting a node from a binary search tree. In the first case, the node to be deleted is the leaf node. In such a case, simply delete the node from the tree. In the second case, the node to be deleted lies has a single child node. In such a case follow the steps below: Replace that node with its child node. Remove the child node from its original position. In the third case, the node to be deleted has two children. In such a case follow the steps below: Get the inorder successor of that node. Replace the node with the inorder successor. Remove the inorder successor from its original position. Here, n is the number of nodes in the tree. The space complexity for all the operations is O(n). In multilevel indexing in the database For dynamic sorting For managing virtual memory areas in Unix kernel","// Binary Search Tree operations in Java

class BinarySearchTree {
  class Node {
    int key;
    Node left, right;

    public Node(int item) {
      key = item;
      left = right = null;
    }
  }

  Node root;

  BinarySearchTree() {
    root = null;
  }

  void insert(int key) {
    root = insertKey(root, key);
  }

  // Insert key in the tree
  Node insertKey(Node root, int key) {
    // Return a new node if the tree is empty
    if (root == null) {
      root = new Node(key);
      return root;
    }

    // Traverse to the right place and insert the node
    if (key < root.key)
      root.left = insertKey(root.left, key);
    else if (key > root.key)
      root.right = insertKey(root.right, key);

    return root;
  }

  void inorder() {
    inorderRec(root);
  }

  // Inorder Traversal
  void inorderRec(Node root) {
    if (root != null) {
      inorderRec(root.left);
      System.out.print(root.key + "" -> "");
      inorderRec(root.right);
    }
  }

  void deleteKey(int key) {
    root = deleteRec(root, key);
  }

  Node deleteRec(Node root, int key) {
    // Return if the tree is empty
    if (root == null)
      return root;

    // Find the node to be deleted
    if (key < root.key)
      root.left = deleteRec(root.left, key);
    else if (key > root.key)
      root.right = deleteRec(root.right, key);
    else {
      // If the node is with only one child or no child
      if (root.left == null)
        return root.right;
      else if (root.right == null)
        return root.left;

      // If the node has two children
      // Place the inorder successor in position of the node to be deleted
      root.key = minValue(root.right);

      // Delete the inorder successor
      root.right = deleteRec(root.right, root.key);
    }

    return root;
  }

  // Find the inorder successor
  int minValue(Node root) {
    int minv = root.key;
    while (root.left != null) {
      minv = root.left.key;
      root = root.left;
    }
    return minv;
  }

  // Driver Program to test above functions
  public static void main(String[] args) {
    BinarySearchTree tree = new BinarySearchTree();

    tree.insert(8);
    tree.insert(3);
    tree.insert(1);
    tree.insert(6);
    tree.insert(7);
    tree.insert(10);
    tree.insert(14);
    tree.insert(4);

    System.out.print(""Inorder traversal: "");
    tree.inorder();

    System.out.println(""\n\nAfter deleting 10"");
    tree.deleteKey(10);
    System.out.print(""Inorder traversal: "");
    tree.inorder();
  }
}"
Binary Search Tree,"Binary search tree is a data structure that quickly allows us to maintain a sorted list of numbers. It is called a binary tree because each tree node has a maximum of two children.
	It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. It is called a binary tree because each tree node has a maximum of two children. It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. The properties that separate a binary search tree from a regular binary tree is All nodes of left subtree are less than the root node All nodes of right subtree are more than the root node Both subtrees of each node are also BSTs i.e. they have the above two properties The binary tree on the right isn't a binary search tree because the right subtree of the node ""3"" contains a value smaller than it. There are two basic operations that you can perform on a binary search tree: The algorithm depends on the property of BST that if each left subtree has values below root and each right subtree has values above the root. If the value is below the root, we can say for sure that the value is not in the right subtree; we need to only search in the left subtree and if the value is above the root, we can say for sure that the value is not in the left subtree; we need to only search in the right subtree. Algorithm: Let us try to visualize this with a diagram. If the value is found, we return the value so that it gets propagated in each recursion step as shown in the image below.  If you might have noticed, we have called return search(struct node*) four times. When we return either the new node or NULL, the value gets returned again and again until search(root) returns the final result. If the value is not found, we eventually reach the left or right child of a leaf node which is NULL and it gets propagated and returned. Inserting a value in the correct position is similar to searching because we try to maintain the rule that the left subtree is lesser than root and the right subtree is larger than root. We keep going to either right subtree or left subtree depending on the value and when we reach a point left or right subtree is null, we put the new node there. Algorithm: The algorithm isn't as simple as it looks. Let's try to visualize how we add a number to an existing BST. We have attached the node but we still have to exit from the function without doing any damage to the rest of the tree. This is where the return node; at the end comes in handy. In the case of NULL, the newly created node is returned and attached to the parent node, otherwise the same node is returned without any change as we go up until we return to the root. This makes sure that as we move back up the tree, the other node connections aren't changed. There are three cases for deleting a node from a binary search tree. In the first case, the node to be deleted is the leaf node. In such a case, simply delete the node from the tree. In the second case, the node to be deleted lies has a single child node. In such a case follow the steps below: Replace that node with its child node. Remove the child node from its original position. In the third case, the node to be deleted has two children. In such a case follow the steps below: Get the inorder successor of that node. Replace the node with the inorder successor. Remove the inorder successor from its original position. Here, n is the number of nodes in the tree. The space complexity for all the operations is O(n). In multilevel indexing in the database For dynamic sorting For managing virtual memory areas in Unix kernel","// Binary Search Tree operations in C

#include <stdio.h>
#include <stdlib.h>

struct node {
  int key;
  struct node *left, *right;
};

// Create a node
struct node *newNode(int item) {
  struct node *temp = (struct node *)malloc(sizeof(struct node));
  temp->key = item;
  temp->left = temp->right = NULL;
  return temp;
}

// Inorder Traversal
void inorder(struct node *root) {
  if (root != NULL) {
    // Traverse left
    inorder(root->left);

    // Traverse root
    printf(""%d -> "", root->key);

    // Traverse right
    inorder(root->right);
  }
}

// Insert a node
struct node *insert(struct node *node, int key) {
  // Return a new node if the tree is empty
  if (node == NULL) return newNode(key);

  // Traverse to the right place and insert the node
  if (key < node->key)
    node->left = insert(node->left, key);
  else
    node->right = insert(node->right, key);

  return node;
}

// Find the inorder successor
struct node *minValueNode(struct node *node) {
  struct node *current = node;

  // Find the leftmost leaf
  while (current && current->left != NULL)
    current = current->left;

  return current;
}

// Deleting a node
struct node *deleteNode(struct node *root, int key) {
  // Return if the tree is empty
  if (root == NULL) return root;

  // Find the node to be deleted
  if (key < root->key)
    root->left = deleteNode(root->left, key);
  else if (key > root->key)
    root->right = deleteNode(root->right, key);

  else {
    // If the node is with only one child or no child
    if (root->left == NULL) {
      struct node *temp = root->right;
      free(root);
      return temp;
    } else if (root->right == NULL) {
      struct node *temp = root->left;
      free(root);
      return temp;
    }

    // If the node has two children
    struct node *temp = minValueNode(root->right);

    // Place the inorder successor in position of the node to be deleted
    root->key = temp->key;

    // Delete the inorder successor
    root->right = deleteNode(root->right, temp->key);
  }
  return root;
}

// Driver code
int main() {
  struct node *root = NULL;
  root = insert(root, 8);
  root = insert(root, 3);
  root = insert(root, 1);
  root = insert(root, 6);
  root = insert(root, 7);
  root = insert(root, 10);
  root = insert(root, 14);
  root = insert(root, 4);

  printf(""Inorder traversal: "");
  inorder(root);

  printf(""\nAfter deleting 10\n"");
  root = deleteNode(root, 10);
  printf(""Inorder traversal: "");
  inorder(root);
}"
Binary Search Tree,"Binary search tree is a data structure that quickly allows us to maintain a sorted list of numbers. It is called a binary tree because each tree node has a maximum of two children.
	It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. It is called a binary tree because each tree node has a maximum of two children. It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time. The properties that separate a binary search tree from a regular binary tree is All nodes of left subtree are less than the root node All nodes of right subtree are more than the root node Both subtrees of each node are also BSTs i.e. they have the above two properties The binary tree on the right isn't a binary search tree because the right subtree of the node ""3"" contains a value smaller than it. There are two basic operations that you can perform on a binary search tree: The algorithm depends on the property of BST that if each left subtree has values below root and each right subtree has values above the root. If the value is below the root, we can say for sure that the value is not in the right subtree; we need to only search in the left subtree and if the value is above the root, we can say for sure that the value is not in the left subtree; we need to only search in the right subtree. Algorithm: Let us try to visualize this with a diagram. If the value is found, we return the value so that it gets propagated in each recursion step as shown in the image below.  If you might have noticed, we have called return search(struct node*) four times. When we return either the new node or NULL, the value gets returned again and again until search(root) returns the final result. If the value is not found, we eventually reach the left or right child of a leaf node which is NULL and it gets propagated and returned. Inserting a value in the correct position is similar to searching because we try to maintain the rule that the left subtree is lesser than root and the right subtree is larger than root. We keep going to either right subtree or left subtree depending on the value and when we reach a point left or right subtree is null, we put the new node there. Algorithm: The algorithm isn't as simple as it looks. Let's try to visualize how we add a number to an existing BST. We have attached the node but we still have to exit from the function without doing any damage to the rest of the tree. This is where the return node; at the end comes in handy. In the case of NULL, the newly created node is returned and attached to the parent node, otherwise the same node is returned without any change as we go up until we return to the root. This makes sure that as we move back up the tree, the other node connections aren't changed. There are three cases for deleting a node from a binary search tree. In the first case, the node to be deleted is the leaf node. In such a case, simply delete the node from the tree. In the second case, the node to be deleted lies has a single child node. In such a case follow the steps below: Replace that node with its child node. Remove the child node from its original position. In the third case, the node to be deleted has two children. In such a case follow the steps below: Get the inorder successor of that node. Replace the node with the inorder successor. Remove the inorder successor from its original position. Here, n is the number of nodes in the tree. The space complexity for all the operations is O(n). In multilevel indexing in the database For dynamic sorting For managing virtual memory areas in Unix kernel","// Binary Search Tree operations in C++

#include <iostream>
using namespace std;

struct node {
  int key;
  struct node *left, *right;
};

// Create a node
struct node *newNode(int item) {
  struct node *temp = (struct node *)malloc(sizeof(struct node));
  temp->key = item;
  temp->left = temp->right = NULL;
  return temp;
}

// Inorder Traversal
void inorder(struct node *root) {
  if (root != NULL) {
    // Traverse left
    inorder(root->left);

    // Traverse root
    cout << root->key << "" -> "";

    // Traverse right
    inorder(root->right);
  }
}

// Insert a node
struct node *insert(struct node *node, int key) {
  // Return a new node if the tree is empty
  if (node == NULL) return newNode(key);

  // Traverse to the right place and insert the node
  if (key < node->key)
    node->left = insert(node->left, key);
  else
    node->right = insert(node->right, key);

  return node;
}

// Find the inorder successor
struct node *minValueNode(struct node *node) {
  struct node *current = node;

  // Find the leftmost leaf
  while (current && current->left != NULL)
    current = current->left;

  return current;
}

// Deleting a node
struct node *deleteNode(struct node *root, int key) {
  // Return if the tree is empty
  if (root == NULL) return root;

  // Find the node to be deleted
  if (key < root->key)
    root->left = deleteNode(root->left, key);
  else if (key > root->key)
    root->right = deleteNode(root->right, key);
  else {
    // If the node is with only one child or no child
    if (root->left == NULL) {
      struct node *temp = root->right;
      free(root);
      return temp;
    } else if (root->right == NULL) {
      struct node *temp = root->left;
      free(root);
      return temp;
    }

    // If the node has two children
    struct node *temp = minValueNode(root->right);

    // Place the inorder successor in position of the node to be deleted
    root->key = temp->key;

    // Delete the inorder successor
    root->right = deleteNode(root->right, temp->key);
  }
  return root;
}

// Driver code
int main() {
  struct node *root = NULL;
  root = insert(root, 8);
  root = insert(root, 3);
  root = insert(root, 1);
  root = insert(root, 6);
  root = insert(root, 7);
  root = insert(root, 10);
  root = insert(root, 14);
  root = insert(root, 4);

  cout << ""Inorder traversal: "";
  inorder(root);

  cout << ""\nAfter deleting 10\n"";
  root = deleteNode(root, 10);
  cout << ""Inorder traversal: "";
  inorder(root);
}"
AVL Tree,"AVL tree is a self-balancing binary search tree in which each node maintains extra information called a balance factor whose value is either -1, 0 or +1. AVL tree got its name after its inventor Georgy Adelson-Velsky and Landis. Balance factor of a node in an AVL tree is the difference between the height of the left subtree and that of the right subtree of that node. Balance Factor = (Height of Left Subtree - Height of Right Subtree) or (Height of Right Subtree - Height of Left Subtree) The self balancing property of an avl tree is maintained by the balance factor. The value of balance factor should always be -1, 0 or +1. An example of a balanced avl tree is:  Various operations that can be performed on an AVL tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node. Algorithm Let the initial tree be:
		
			Left rotate If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In left-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x. Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y A newNode is always inserted as a leaf node with balance factor equal to 0. Let the initial tree be:
		
			Initial tree for insertion
		
		
		Let the node to be inserted be:
		
			New node Go to the appropriate leaf node to insert a newNode using the following recursive steps. Compare newKey with rootKey of the current tree.
		
			If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached.
			Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached.
			Else, return leafNode.
				
					Finding the location to insert newNode If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached. Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached. Else, return leafNode.
				
					Finding the location to insert newNode Compare leafKey obtained from the above steps with newKey:
		
			If newKey < leafKey, make newNode as the leftChild of leafNode.
			Else, make newNode as rightChild of leafNode.
				
					Inserting the new node If newKey < leafKey, make newNode as the leftChild of leafNode. Else, make newNode as rightChild of leafNode.
				
					Inserting the new node Update balanceFactor of the nodes.
		
			Updating the balance factor after insertion If the nodes are unbalanced, then rebalance the node.
		
			If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation
						
					
				
			
			If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If newNodeKey < leftChildKey do right rotation. Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If newNodeKey > rightChildKey do left rotation. Else, do right-left rotation The final tree is:
		
			Final balanced tree A node is always deleted as a leaf node. After deleting a node, the balance factors of the nodes get changed. In order to rebalance the balance factor, suitable rotations are performed. Locate nodeToBeDeleted (recursion is used to find nodeToBeDeleted in the code used below).

		
			Locating the node to be deleted There are three cases for deleting a node:
		
			If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted.
			If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child.
			If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted. If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child. If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted Remove the leaf node w.
						
							Remove w Update balanceFactor of the nodes.
		
			Update bf Rebalance the tree if the balance factor of any of the nodes is not equal to -1, 0 or 1.
		
			If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation.
				
			
			If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation. If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree Else do left-right rotation. If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of rightChild <= 0, do left rotation. Else do right-left rotation. The final tree is:
		
			Avl tree final For indexing large records in databases
	For searching in large databases For indexing large records in databases For searching in large databases","# AVL tree implementation in Python


import sys

# Create a tree node
class TreeNode(object):
    def __init__(self, key):
        self.key = key
        self.left = None
        self.right = None
        self.height = 1


class AVLTree(object):

    # Function to insert a node
    def insert_node(self, root, key):

        # Find the correct location and insert the node
        if not root:
            return TreeNode(key)
        elif key < root.key:
            root.left = self.insert_node(root.left, key)
        else:
            root.right = self.insert_node(root.right, key)

        root.height = 1 + max(self.getHeight(root.left),
                              self.getHeight(root.right))

        # Update the balance factor and balance the tree
        balanceFactor = self.getBalance(root)
        if balanceFactor > 1:
            if key < root.left.key:
                return self.rightRotate(root)
            else:
                root.left = self.leftRotate(root.left)
                return self.rightRotate(root)

        if balanceFactor < -1:
            if key > root.right.key:
                return self.leftRotate(root)
            else:
                root.right = self.rightRotate(root.right)
                return self.leftRotate(root)

        return root

    # Function to delete a node
    def delete_node(self, root, key):

        # Find the node to be deleted and remove it
        if not root:
            return root
        elif key < root.key:
            root.left = self.delete_node(root.left, key)
        elif key > root.key:
            root.right = self.delete_node(root.right, key)
        else:
            if root.left is None:
                temp = root.right
                root = None
                return temp
            elif root.right is None:
                temp = root.left
                root = None
                return temp
            temp = self.getMinValueNode(root.right)
            root.key = temp.key
            root.right = self.delete_node(root.right,
                                          temp.key)
        if root is None:
            return root

        # Update the balance factor of nodes
        root.height = 1 + max(self.getHeight(root.left),
                              self.getHeight(root.right))

        balanceFactor = self.getBalance(root)

        # Balance the tree
        if balanceFactor > 1:
            if self.getBalance(root.left) >= 0:
                return self.rightRotate(root)
            else:
                root.left = self.leftRotate(root.left)
                return self.rightRotate(root)
        if balanceFactor < -1:
            if self.getBalance(root.right) <= 0:
                return self.leftRotate(root)
            else:
                root.right = self.rightRotate(root.right)
                return self.leftRotate(root)
        return root

    # Function to perform left rotation
    def leftRotate(self, z):
        y = z.right
        T2 = y.left
        y.left = z
        z.right = T2
        z.height = 1 + max(self.getHeight(z.left),
                           self.getHeight(z.right))
        y.height = 1 + max(self.getHeight(y.left),
                           self.getHeight(y.right))
        return y

    # Function to perform right rotation
    def rightRotate(self, z):
        y = z.left
        T3 = y.right
        y.right = z
        z.left = T3
        z.height = 1 + max(self.getHeight(z.left),
                           self.getHeight(z.right))
        y.height = 1 + max(self.getHeight(y.left),
                           self.getHeight(y.right))
        return y

    # Get the height of the node
    def getHeight(self, root):
        if not root:
            return 0
        return root.height

    # Get balance factore of the node
    def getBalance(self, root):
        if not root:
            return 0
        return self.getHeight(root.left) - self.getHeight(root.right)

    def getMinValueNode(self, root):
        if root is None or root.left is None:
            return root
        return self.getMinValueNode(root.left)

    def preOrder(self, root):
        if not root:
            return
        print(""{0} "".format(root.key), end="""")
        self.preOrder(root.left)
        self.preOrder(root.right)

    # Print the tree
    def printHelper(self, currPtr, indent, last):
        if currPtr != None:
            sys.stdout.write(indent)
            if last:
                sys.stdout.write(""R----"")
                indent += ""     ""
            else:
                sys.stdout.write(""L----"")
                indent += ""|    ""
            print(currPtr.key)
            self.printHelper(currPtr.left, indent, False)
            self.printHelper(currPtr.right, indent, True)


myTree = AVLTree()
root = None
nums = [33, 13, 52, 9, 21, 61, 8, 11]
for num in nums:
    root = myTree.insert_node(root, num)
myTree.printHelper(root, """", True)
key = 13
root = myTree.delete_node(root, key)
print(""After Deletion: "")
myTree.printHelper(root, """", True)"
AVL Tree,"AVL tree is a self-balancing binary search tree in which each node maintains extra information called a balance factor whose value is either -1, 0 or +1. AVL tree got its name after its inventor Georgy Adelson-Velsky and Landis. Balance factor of a node in an AVL tree is the difference between the height of the left subtree and that of the right subtree of that node. Balance Factor = (Height of Left Subtree - Height of Right Subtree) or (Height of Right Subtree - Height of Left Subtree) The self balancing property of an avl tree is maintained by the balance factor. The value of balance factor should always be -1, 0 or +1. An example of a balanced avl tree is:  Various operations that can be performed on an AVL tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node. Algorithm Let the initial tree be:
		
			Left rotate If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In left-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x. Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y A newNode is always inserted as a leaf node with balance factor equal to 0. Let the initial tree be:
		
			Initial tree for insertion
		
		
		Let the node to be inserted be:
		
			New node Go to the appropriate leaf node to insert a newNode using the following recursive steps. Compare newKey with rootKey of the current tree.
		
			If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached.
			Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached.
			Else, return leafNode.
				
					Finding the location to insert newNode If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached. Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached. Else, return leafNode.
				
					Finding the location to insert newNode Compare leafKey obtained from the above steps with newKey:
		
			If newKey < leafKey, make newNode as the leftChild of leafNode.
			Else, make newNode as rightChild of leafNode.
				
					Inserting the new node If newKey < leafKey, make newNode as the leftChild of leafNode. Else, make newNode as rightChild of leafNode.
				
					Inserting the new node Update balanceFactor of the nodes.
		
			Updating the balance factor after insertion If the nodes are unbalanced, then rebalance the node.
		
			If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation
						
					
				
			
			If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If newNodeKey < leftChildKey do right rotation. Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If newNodeKey > rightChildKey do left rotation. Else, do right-left rotation The final tree is:
		
			Final balanced tree A node is always deleted as a leaf node. After deleting a node, the balance factors of the nodes get changed. In order to rebalance the balance factor, suitable rotations are performed. Locate nodeToBeDeleted (recursion is used to find nodeToBeDeleted in the code used below).

		
			Locating the node to be deleted There are three cases for deleting a node:
		
			If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted.
			If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child.
			If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted. If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child. If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted Remove the leaf node w.
						
							Remove w Update balanceFactor of the nodes.
		
			Update bf Rebalance the tree if the balance factor of any of the nodes is not equal to -1, 0 or 1.
		
			If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation.
				
			
			If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation. If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree Else do left-right rotation. If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of rightChild <= 0, do left rotation. Else do right-left rotation. The final tree is:
		
			Avl tree final For indexing large records in databases
	For searching in large databases For indexing large records in databases For searching in large databases","// AVL tree implementation in Java

// Create node
class Node {
  int item, height;
  Node left, right;

  Node(int d) {
    item = d;
    height = 1;
  }
}

// Tree class
class AVLTree {
  Node root;

  int height(Node N) {
    if (N == null)
      return 0;
    return N.height;
  }

  int max(int a, int b) {
    return (a > b) ? a : b;
  }

  Node rightRotate(Node y) {
    Node x = y.left;
    Node T2 = x.right;
    x.right = y;
    y.left = T2;
    y.height = max(height(y.left), height(y.right)) + 1;
    x.height = max(height(x.left), height(x.right)) + 1;
    return x;
  }

  Node leftRotate(Node x) {
    Node y = x.right;
    Node T2 = y.left;
    y.left = x;
    x.right = T2;
    x.height = max(height(x.left), height(x.right)) + 1;
    y.height = max(height(y.left), height(y.right)) + 1;
    return y;
  }

  // Get balance factor of a node
  int getBalanceFactor(Node N) {
    if (N == null)
      return 0;
    return height(N.left) - height(N.right);
  }

  // Insert a node
  Node insertNode(Node node, int item) {

    // Find the position and insert the node
    if (node == null)
      return (new Node(item));
    if (item < node.item)
      node.left = insertNode(node.left, item);
    else if (item > node.item)
      node.right = insertNode(node.right, item);
    else
      return node;

    // Update the balance factor of each node
    // And, balance the tree
    node.height = 1 + max(height(node.left), height(node.right));
    int balanceFactor = getBalanceFactor(node);
    if (balanceFactor > 1) {
      if (item < node.left.item) {
        return rightRotate(node);
      } else if (item > node.left.item) {
        node.left = leftRotate(node.left);
        return rightRotate(node);
      }
    }
    if (balanceFactor < -1) {
      if (item > node.right.item) {
        return leftRotate(node);
      } else if (item < node.right.item) {
        node.right = rightRotate(node.right);
        return leftRotate(node);
      }
    }
    return node;
  }

  Node nodeWithMimumValue(Node node) {
    Node current = node;
    while (current.left != null)
      current = current.left;
    return current;
  }

  // Delete a node
  Node deleteNode(Node root, int item) {

    // Find the node to be deleted and remove it
    if (root == null)
      return root;
    if (item < root.item)
      root.left = deleteNode(root.left, item);
    else if (item > root.item)
      root.right = deleteNode(root.right, item);
    else {
      if ((root.left == null) || (root.right == null)) {
        Node temp = null;
        if (temp == root.left)
          temp = root.right;
        else
          temp = root.left;
        if (temp == null) {
          temp = root;
          root = null;
        } else
          root = temp;
      } else {
        Node temp = nodeWithMimumValue(root.right);
        root.item = temp.item;
        root.right = deleteNode(root.right, temp.item);
      }
    }
    if (root == null)
      return root;

    // Update the balance factor of each node and balance the tree
    root.height = max(height(root.left), height(root.right)) + 1;
    int balanceFactor = getBalanceFactor(root);
    if (balanceFactor > 1) {
      if (getBalanceFactor(root.left) >= 0) {
        return rightRotate(root);
      } else {
        root.left = leftRotate(root.left);
        return rightRotate(root);
      }
    }
    if (balanceFactor < -1) {
      if (getBalanceFactor(root.right) <= 0) {
        return leftRotate(root);
      } else {
        root.right = rightRotate(root.right);
        return leftRotate(root);
      }
    }
    return root;
  }

  void preOrder(Node node) {
    if (node != null) {
      System.out.print(node.item + "" "");
      preOrder(node.left);
      preOrder(node.right);
    }
  }

  // Print the tree
  private void printTree(Node currPtr, String indent, boolean last) {
    if (currPtr != null) {
      System.out.print(indent);
      if (last) {
        System.out.print(""R----"");
        indent += ""   "";
      } else {
        System.out.print(""L----"");
        indent += ""|  "";
      }
      System.out.println(currPtr.item);
      printTree(currPtr.left, indent, false);
      printTree(currPtr.right, indent, true);
    }
  }

  // Driver code
  public static void main(String[] args) {
    AVLTree tree = new AVLTree();
    tree.root = tree.insertNode(tree.root, 33);
    tree.root = tree.insertNode(tree.root, 13);
    tree.root = tree.insertNode(tree.root, 53);
    tree.root = tree.insertNode(tree.root, 9);
    tree.root = tree.insertNode(tree.root, 21);
    tree.root = tree.insertNode(tree.root, 61);
    tree.root = tree.insertNode(tree.root, 8);
    tree.root = tree.insertNode(tree.root, 11);
    tree.printTree(tree.root, """", true);
    tree.root = tree.deleteNode(tree.root, 13);
    System.out.println(""After Deletion: "");
    tree.printTree(tree.root, """", true);
  }
}"
AVL Tree,"AVL tree is a self-balancing binary search tree in which each node maintains extra information called a balance factor whose value is either -1, 0 or +1. AVL tree got its name after its inventor Georgy Adelson-Velsky and Landis. Balance factor of a node in an AVL tree is the difference between the height of the left subtree and that of the right subtree of that node. Balance Factor = (Height of Left Subtree - Height of Right Subtree) or (Height of Right Subtree - Height of Left Subtree) The self balancing property of an avl tree is maintained by the balance factor. The value of balance factor should always be -1, 0 or +1. An example of a balanced avl tree is:  Various operations that can be performed on an AVL tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node. Algorithm Let the initial tree be:
		
			Left rotate If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In left-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x. Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y A newNode is always inserted as a leaf node with balance factor equal to 0. Let the initial tree be:
		
			Initial tree for insertion
		
		
		Let the node to be inserted be:
		
			New node Go to the appropriate leaf node to insert a newNode using the following recursive steps. Compare newKey with rootKey of the current tree.
		
			If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached.
			Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached.
			Else, return leafNode.
				
					Finding the location to insert newNode If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached. Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached. Else, return leafNode.
				
					Finding the location to insert newNode Compare leafKey obtained from the above steps with newKey:
		
			If newKey < leafKey, make newNode as the leftChild of leafNode.
			Else, make newNode as rightChild of leafNode.
				
					Inserting the new node If newKey < leafKey, make newNode as the leftChild of leafNode. Else, make newNode as rightChild of leafNode.
				
					Inserting the new node Update balanceFactor of the nodes.
		
			Updating the balance factor after insertion If the nodes are unbalanced, then rebalance the node.
		
			If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation
						
					
				
			
			If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If newNodeKey < leftChildKey do right rotation. Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If newNodeKey > rightChildKey do left rotation. Else, do right-left rotation The final tree is:
		
			Final balanced tree A node is always deleted as a leaf node. After deleting a node, the balance factors of the nodes get changed. In order to rebalance the balance factor, suitable rotations are performed. Locate nodeToBeDeleted (recursion is used to find nodeToBeDeleted in the code used below).

		
			Locating the node to be deleted There are three cases for deleting a node:
		
			If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted.
			If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child.
			If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted. If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child. If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted Remove the leaf node w.
						
							Remove w Update balanceFactor of the nodes.
		
			Update bf Rebalance the tree if the balance factor of any of the nodes is not equal to -1, 0 or 1.
		
			If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation.
				
			
			If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation. If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree Else do left-right rotation. If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of rightChild <= 0, do left rotation. Else do right-left rotation. The final tree is:
		
			Avl tree final For indexing large records in databases
	For searching in large databases For indexing large records in databases For searching in large databases","// AVL tree implementation in C

#include <stdio.h>
#include <stdlib.h>

// Create Node
struct Node {
  int key;
  struct Node *left;
  struct Node *right;
  int height;
};

int max(int a, int b);

// Calculate height
int height(struct Node *N) {
  if (N == NULL)
    return 0;
  return N->height;
}

int max(int a, int b) {
  return (a > b) ? a : b;
}

// Create a node
struct Node *newNode(int key) {
  struct Node *node = (struct Node *)
    malloc(sizeof(struct Node));
  node->key = key;
  node->left = NULL;
  node->right = NULL;
  node->height = 1;
  return (node);
}

// Right rotate
struct Node *rightRotate(struct Node *y) {
  struct Node *x = y->left;
  struct Node *T2 = x->right;

  x->right = y;
  y->left = T2;

  y->height = max(height(y->left), height(y->right)) + 1;
  x->height = max(height(x->left), height(x->right)) + 1;

  return x;
}

// Left rotate
struct Node *leftRotate(struct Node *x) {
  struct Node *y = x->right;
  struct Node *T2 = y->left;

  y->left = x;
  x->right = T2;

  x->height = max(height(x->left), height(x->right)) + 1;
  y->height = max(height(y->left), height(y->right)) + 1;

  return y;
}

// Get the balance factor
int getBalance(struct Node *N) {
  if (N == NULL)
    return 0;
  return height(N->left) - height(N->right);
}

// Insert node
struct Node *insertNode(struct Node *node, int key) {
  // Find the correct position to insertNode the node and insertNode it
  if (node == NULL)
    return (newNode(key));

  if (key < node->key)
    node->left = insertNode(node->left, key);
  else if (key > node->key)
    node->right = insertNode(node->right, key);
  else
    return node;

  // Update the balance factor of each node and
  // Balance the tree
  node->height = 1 + max(height(node->left),
               height(node->right));

  int balance = getBalance(node);
  if (balance > 1 && key < node->left->key)
    return rightRotate(node);

  if (balance < -1 && key > node->right->key)
    return leftRotate(node);

  if (balance > 1 && key > node->left->key) {
    node->left = leftRotate(node->left);
    return rightRotate(node);
  }

  if (balance < -1 && key < node->right->key) {
    node->right = rightRotate(node->right);
    return leftRotate(node);
  }

  return node;
}

struct Node *minValueNode(struct Node *node) {
  struct Node *current = node;

  while (current->left != NULL)
    current = current->left;

  return current;
}

// Delete a nodes
struct Node *deleteNode(struct Node *root, int key) {
  // Find the node and delete it
  if (root == NULL)
    return root;

  if (key < root->key)
    root->left = deleteNode(root->left, key);

  else if (key > root->key)
    root->right = deleteNode(root->right, key);

  else {
    if ((root->left == NULL) || (root->right == NULL)) {
      struct Node *temp = root->left ? root->left : root->right;

      if (temp == NULL) {
        temp = root;
        root = NULL;
      } else
        *root = *temp;
      free(temp);
    } else {
      struct Node *temp = minValueNode(root->right);

      root->key = temp->key;

      root->right = deleteNode(root->right, temp->key);
    }
  }

  if (root == NULL)
    return root;

  // Update the balance factor of each node and
  // balance the tree
  root->height = 1 + max(height(root->left),
               height(root->right));

  int balance = getBalance(root);
  if (balance > 1 && getBalance(root->left) >= 0)
    return rightRotate(root);

  if (balance > 1 && getBalance(root->left) < 0) {
    root->left = leftRotate(root->left);
    return rightRotate(root);
  }

  if (balance < -1 && getBalance(root->right) <= 0)
    return leftRotate(root);

  if (balance < -1 && getBalance(root->right) > 0) {
    root->right = rightRotate(root->right);
    return leftRotate(root);
  }

  return root;
}

// Print the tree
void printPreOrder(struct Node *root) {
  if (root != NULL) {
    printf(""%d "", root->key);
    printPreOrder(root->left);
    printPreOrder(root->right);
  }
}

int main() {
  struct Node *root = NULL;

  root = insertNode(root, 2);
  root = insertNode(root, 1);
  root = insertNode(root, 7);
  root = insertNode(root, 4);
  root = insertNode(root, 5);
  root = insertNode(root, 3);
  root = insertNode(root, 8);

  printPreOrder(root);

  root = deleteNode(root, 3);

  printf(""\nAfter deletion: "");
  printPreOrder(root);

  return 0;
}"
AVL Tree,"AVL tree is a self-balancing binary search tree in which each node maintains extra information called a balance factor whose value is either -1, 0 or +1. AVL tree got its name after its inventor Georgy Adelson-Velsky and Landis. Balance factor of a node in an AVL tree is the difference between the height of the left subtree and that of the right subtree of that node. Balance Factor = (Height of Left Subtree - Height of Right Subtree) or (Height of Right Subtree - Height of Left Subtree) The self balancing property of an avl tree is maintained by the balance factor. The value of balance factor should always be -1, 0 or +1. An example of a balanced avl tree is:  Various operations that can be performed on an AVL tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node. Algorithm Let the initial tree be:
		
			Left rotate If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In left-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x. Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y A newNode is always inserted as a leaf node with balance factor equal to 0. Let the initial tree be:
		
			Initial tree for insertion
		
		
		Let the node to be inserted be:
		
			New node Go to the appropriate leaf node to insert a newNode using the following recursive steps. Compare newKey with rootKey of the current tree.
		
			If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached.
			Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached.
			Else, return leafNode.
				
					Finding the location to insert newNode If newKey < rootKey, call insertion algorithm on the left subtree of the current node until the leaf node is reached. Else if newKey > rootKey, call insertion algorithm on the right subtree of current node until the leaf node is reached. Else, return leafNode.
				
					Finding the location to insert newNode Compare leafKey obtained from the above steps with newKey:
		
			If newKey < leafKey, make newNode as the leftChild of leafNode.
			Else, make newNode as rightChild of leafNode.
				
					Inserting the new node If newKey < leafKey, make newNode as the leftChild of leafNode. Else, make newNode as rightChild of leafNode.
				
					Inserting the new node Update balanceFactor of the nodes.
		
			Updating the balance factor after insertion If the nodes are unbalanced, then rebalance the node.
		
			If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation
						
					
				
			
			If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If balanceFactor > 1, it means the height of the left subtree is greater than that of the right subtree. So, do a right rotation or left-right rotation
				
					If newNodeKey < leftChildKey do right rotation.
					Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If newNodeKey < leftChildKey do right rotation. Else, do left-right rotation.
						
							Balancing the tree with rotation
						

						
							Balancing the tree with rotation If balanceFactor < -1, it means the height of the right subtree is greater than that of the left subtree. So, do right rotation or right-left rotation
				
					If newNodeKey > rightChildKey do left rotation.
					Else, do right-left rotation If newNodeKey > rightChildKey do left rotation. Else, do right-left rotation The final tree is:
		
			Final balanced tree A node is always deleted as a leaf node. After deleting a node, the balance factors of the nodes get changed. In order to rebalance the balance factor, suitable rotations are performed. Locate nodeToBeDeleted (recursion is used to find nodeToBeDeleted in the code used below).

		
			Locating the node to be deleted There are three cases for deleting a node:
		
			If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted.
			If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child.
			If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w If nodeToBeDeleted is the leaf node (ie. does not have any child), then remove nodeToBeDeleted. If nodeToBeDeleted has one child, then substitute the contents of nodeToBeDeleted with that of the child. Remove the child. If nodeToBeDeleted has two children, find the inorder successor w of nodeToBeDeleted (ie. node with a minimum value of key in the right subtree).
				
					Finding the successor
				

				
					Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted
						
					
					Remove the leaf node w.
						
							Remove w Substitute the contents of nodeToBeDeleted with that of w.

						
							Substitute the node to be deleted Remove the leaf node w.
						
							Remove w Update balanceFactor of the nodes.
		
			Update bf Rebalance the tree if the balance factor of any of the nodes is not equal to -1, 0 or 1.
		
			If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation.
				
			
			If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of currentNode > 1,
				
					If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree
						
					
					Else do left-right rotation. If balanceFactor of leftChild >= 0, do right rotation.
						
							Right-rotate for balancing the tree Else do left-right rotation. If balanceFactor of currentNode < -1,
				
					If balanceFactor of rightChild <= 0, do left rotation.
					Else do right-left rotation. If balanceFactor of rightChild <= 0, do left rotation. Else do right-left rotation. The final tree is:
		
			Avl tree final For indexing large records in databases
	For searching in large databases For indexing large records in databases For searching in large databases","// AVL tree implementation in C++

#include <iostream>
using namespace std;

class Node {
   public:
  int key;
  Node *left;
  Node *right;
  int height;
};

int max(int a, int b);

// Calculate height
int height(Node *N) {
  if (N == NULL)
    return 0;
  return N->height;
}

int max(int a, int b) {
  return (a > b) ? a : b;
}

// New node creation
Node *newNode(int key) {
  Node *node = new Node();
  node->key = key;
  node->left = NULL;
  node->right = NULL;
  node->height = 1;
  return (node);
}

// Rotate right
Node *rightRotate(Node *y) {
  Node *x = y->left;
  Node *T2 = x->right;
  x->right = y;
  y->left = T2;
  y->height = max(height(y->left),
          height(y->right)) +
        1;
  x->height = max(height(x->left),
          height(x->right)) +
        1;
  return x;
}

// Rotate left
Node *leftRotate(Node *x) {
  Node *y = x->right;
  Node *T2 = y->left;
  y->left = x;
  x->right = T2;
  x->height = max(height(x->left),
          height(x->right)) +
        1;
  y->height = max(height(y->left),
          height(y->right)) +
        1;
  return y;
}

// Get the balance factor of each node
int getBalanceFactor(Node *N) {
  if (N == NULL)
    return 0;
  return height(N->left) -
       height(N->right);
}

// Insert a node
Node *insertNode(Node *node, int key) {
  // Find the correct postion and insert the node
  if (node == NULL)
    return (newNode(key));
  if (key < node->key)
    node->left = insertNode(node->left, key);
  else if (key > node->key)
    node->right = insertNode(node->right, key);
  else
    return node;

  // Update the balance factor of each node and
  // balance the tree
  node->height = 1 + max(height(node->left),
               height(node->right));
  int balanceFactor = getBalanceFactor(node);
  if (balanceFactor > 1) {
    if (key < node->left->key) {
      return rightRotate(node);
    } else if (key > node->left->key) {
      node->left = leftRotate(node->left);
      return rightRotate(node);
    }
  }
  if (balanceFactor < -1) {
    if (key > node->right->key) {
      return leftRotate(node);
    } else if (key < node->right->key) {
      node->right = rightRotate(node->right);
      return leftRotate(node);
    }
  }
  return node;
}

// Node with minimum value
Node *nodeWithMimumValue(Node *node) {
  Node *current = node;
  while (current->left != NULL)
    current = current->left;
  return current;
}

// Delete a node
Node *deleteNode(Node *root, int key) {
  // Find the node and delete it
  if (root == NULL)
    return root;
  if (key < root->key)
    root->left = deleteNode(root->left, key);
  else if (key > root->key)
    root->right = deleteNode(root->right, key);
  else {
    if ((root->left == NULL) ||
      (root->right == NULL)) {
      Node *temp = root->left ? root->left : root->right;
      if (temp == NULL) {
        temp = root;
        root = NULL;
      } else
        *root = *temp;
      free(temp);
    } else {
      Node *temp = nodeWithMimumValue(root->right);
      root->key = temp->key;
      root->right = deleteNode(root->right,
                   temp->key);
    }
  }

  if (root == NULL)
    return root;

  // Update the balance factor of each node and
  // balance the tree
  root->height = 1 + max(height(root->left),
               height(root->right));
  int balanceFactor = getBalanceFactor(root);
  if (balanceFactor > 1) {
    if (getBalanceFactor(root->left) >= 0) {
      return rightRotate(root);
    } else {
      root->left = leftRotate(root->left);
      return rightRotate(root);
    }
  }
  if (balanceFactor < -1) {
    if (getBalanceFactor(root->right) <= 0) {
      return leftRotate(root);
    } else {
      root->right = rightRotate(root->right);
      return leftRotate(root);
    }
  }
  return root;
}

// Print the tree
void printTree(Node *root, string indent, bool last) {
  if (root != nullptr) {
    cout << indent;
    if (last) {
      cout << ""R----"";
      indent += ""   "";
    } else {
      cout << ""L----"";
      indent += ""|  "";
    }
    cout << root->key << endl;
    printTree(root->left, indent, false);
    printTree(root->right, indent, true);
  }
}

int main() {
  Node *root = NULL;
  root = insertNode(root, 33);
  root = insertNode(root, 13);
  root = insertNode(root, 53);
  root = insertNode(root, 9);
  root = insertNode(root, 21);
  root = insertNode(root, 61);
  root = insertNode(root, 8);
  root = insertNode(root, 11);
  printTree(root, """", true);
  root = deleteNode(root, 13);
  cout << ""After deleting "" << endl;
  printTree(root, """", true);
}"
B-tree,"B-tree is a special type of self-balancing search tree in which each node can contain more than one key and can have more than two children. It is a generalized form of the binary search tree. It is also known as a height-balanced m-way tree. The need for B-tree arose with the rise in the need for lesser time in accessing physical storage media like a hard disk. The secondary storage devices are slower with a larger capacity. There was a need for such types of data structures that minimize the disk access. Other data structures such as a binary search tree, avl tree, red-black tree, etc can store only one key in one node. If you have to store a large number of keys, then the height of such trees becomes very large, and the access time increases. However, B-tree can store many keys in a single node and can have multiple child nodes. This decreases the height significantly allowing faster disk accesses. For each node x, the keys are stored in increasing order. In each node, there is a boolean value x.leaf which is true if x is a leaf. If n is the order of the tree, each internal node can contain at most n - 1 keys along with a pointer to each child. Each node except root can have at most n children and at least n/2 children. All leaves have the same depth (i.e. height-h of the tree). The root has at least 2 children and contains a minimum of 1 key. If n ≥ 1, then for any n-key B-tree of height h and minimum degree t ≥ 2, h ≥ logt (n+1)/2. Searching for an element in a B-tree is the generalized form of searching an element in a Binary Search Tree. The following steps are followed. Starting from the root node, compare k with the first key of the node.
		If k = the first key of the node, return the node and the index. If k.leaf = true, return NULL (i.e. not found). If k < the first key of the root node, search the left child of this key recursively. If there is more than one key in the current node and k > the first key, compare k with the next key in the node.
		If k < next key, search the left child of this key (ie. k lies in between the first and the second keys).
		Else, search the right child of the key. Repeat steps 1 to 4 until the leaf is reached. Let us search key k = 17 in the tree below of degree 3.

		
			B-tree k is not found in the root so, compare it with the root key.
		
			k is not found on the root node Since k > 11, go to the right child of the root node.
		
			Go to the right subtree Compare k with 16. Since k > 16, compare k with the next key 18.
		
			Compare with the keys from left to right Since k < 18, k lies between 16 and 18. Search in the right child of 16 or the left child of 18.
		
			k lies in between 16 and 18 k is found.
		
			k is found To learn more about different B-tree operations, please visit Insertion on B-tree
	Deletion on B-tree Insertion on B-tree Deletion on B-tree  Worst case Time complexity: Θ(log n) Average case Time complexity: Θ(log n) Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) databases and file systems
	to store blocks of data (secondary storage media)
	multilevel indexing databases and file systems to store blocks of data (secondary storage media) multilevel indexing","# Searching a key on a B-tree in Python


# Create a node
class BTreeNode:
  def __init__(self, leaf=False):
    self.leaf = leaf
    self.keys = []
    self.child = []


# Tree
class BTree:
  def __init__(self, t):
    self.root = BTreeNode(True)
    self.t = t

    # Insert node
  def insert(self, k):
    root = self.root
    if len(root.keys) == (2 * self.t) - 1:
      temp = BTreeNode()
      self.root = temp
      temp.child.insert(0, root)
      self.split_child(temp, 0)
      self.insert_non_full(temp, k)
    else:
      self.insert_non_full(root, k)

    # Insert nonfull
  def insert_non_full(self, x, k):
    i = len(x.keys) - 1
    if x.leaf:
      x.keys.append((None, None))
      while i >= 0 and k[0] < x.keys[i][0]:
        x.keys[i + 1] = x.keys[i]
        i -= 1
      x.keys[i + 1] = k
    else:
      while i >= 0 and k[0] < x.keys[i][0]:
        i -= 1
      i += 1
      if len(x.child[i].keys) == (2 * self.t) - 1:
        self.split_child(x, i)
        if k[0] > x.keys[i][0]:
          i += 1
      self.insert_non_full(x.child[i], k)

    # Split the child
  def split_child(self, x, i):
    t = self.t
    y = x.child[i]
    z = BTreeNode(y.leaf)
    x.child.insert(i + 1, z)
    x.keys.insert(i, y.keys[t - 1])
    z.keys = y.keys[t: (2 * t) - 1]
    y.keys = y.keys[0: t - 1]
    if not y.leaf:
      z.child = y.child[t: 2 * t]
      y.child = y.child[0: t - 1]

  # Print the tree
  def print_tree(self, x, l=0):
    print(""Level "", l, "" "", len(x.keys), end="":"")
    for i in x.keys:
      print(i, end="" "")
    print()
    l += 1
    if len(x.child) > 0:
      for i in x.child:
        self.print_tree(i, l)

  # Search key in the tree
  def search_key(self, k, x=None):
    if x is not None:
      i = 0
      while i < len(x.keys) and k > x.keys[i][0]:
        i += 1
      if i < len(x.keys) and k == x.keys[i][0]:
        return (x, i)
      elif x.leaf:
        return None
      else:
        return self.search_key(k, x.child[i])
      
    else:
      return self.search_key(k, self.root)


def main():
  B = BTree(3)

  for i in range(10):
    B.insert((i, 2 * i))

  B.print_tree(B.root)

  if B.search_key(8) is not None:
    print(""\nFound"")
  else:
    print(""\nNot Found"")


if __name__ == '__main__':
  main()"
B-tree,"B-tree is a special type of self-balancing search tree in which each node can contain more than one key and can have more than two children. It is a generalized form of the binary search tree. It is also known as a height-balanced m-way tree. The need for B-tree arose with the rise in the need for lesser time in accessing physical storage media like a hard disk. The secondary storage devices are slower with a larger capacity. There was a need for such types of data structures that minimize the disk access. Other data structures such as a binary search tree, avl tree, red-black tree, etc can store only one key in one node. If you have to store a large number of keys, then the height of such trees becomes very large, and the access time increases. However, B-tree can store many keys in a single node and can have multiple child nodes. This decreases the height significantly allowing faster disk accesses. For each node x, the keys are stored in increasing order. In each node, there is a boolean value x.leaf which is true if x is a leaf. If n is the order of the tree, each internal node can contain at most n - 1 keys along with a pointer to each child. Each node except root can have at most n children and at least n/2 children. All leaves have the same depth (i.e. height-h of the tree). The root has at least 2 children and contains a minimum of 1 key. If n ≥ 1, then for any n-key B-tree of height h and minimum degree t ≥ 2, h ≥ logt (n+1)/2. Searching for an element in a B-tree is the generalized form of searching an element in a Binary Search Tree. The following steps are followed. Starting from the root node, compare k with the first key of the node.
		If k = the first key of the node, return the node and the index. If k.leaf = true, return NULL (i.e. not found). If k < the first key of the root node, search the left child of this key recursively. If there is more than one key in the current node and k > the first key, compare k with the next key in the node.
		If k < next key, search the left child of this key (ie. k lies in between the first and the second keys).
		Else, search the right child of the key. Repeat steps 1 to 4 until the leaf is reached. Let us search key k = 17 in the tree below of degree 3.

		
			B-tree k is not found in the root so, compare it with the root key.
		
			k is not found on the root node Since k > 11, go to the right child of the root node.
		
			Go to the right subtree Compare k with 16. Since k > 16, compare k with the next key 18.
		
			Compare with the keys from left to right Since k < 18, k lies between 16 and 18. Search in the right child of 16 or the left child of 18.
		
			k lies in between 16 and 18 k is found.
		
			k is found To learn more about different B-tree operations, please visit Insertion on B-tree
	Deletion on B-tree Insertion on B-tree Deletion on B-tree  Worst case Time complexity: Θ(log n) Average case Time complexity: Θ(log n) Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) databases and file systems
	to store blocks of data (secondary storage media)
	multilevel indexing databases and file systems to store blocks of data (secondary storage media) multilevel indexing","
// Searching a key on a B-tree in Java 

public class BTree {

  private int T;

  // Node creation
  public class Node {
    int n;
    int key[] = new int[2 * T - 1];
    Node child[] = new Node[2 * T];
    boolean leaf = true;

    public int Find(int k) {
      for (int i = 0; i < this.n; i++) {
        if (this.key[i] == k) {
          return i;
        }
      }
      return -1;
    };
  }

  public BTree(int t) {
    T = t;
    root = new Node();
    root.n = 0;
    root.leaf = true;
  }

  private Node root;

  // Search key
  private Node Search(Node x, int key) {
    int i = 0;
    if (x == null)
      return x;
    for (i = 0; i < x.n; i++) {
      if (key < x.key[i]) {
        break;
      }
      if (key == x.key[i]) {
        return x;
      }
    }
    if (x.leaf) {
      return null;
    } else {
      return Search(x.child[i], key);
    }
  }

  // Splitting the node
  private void Split(Node x, int pos, Node y) {
    Node z = new Node();
    z.leaf = y.leaf;
    z.n = T - 1;
    for (int j = 0; j < T - 1; j++) {
      z.key[j] = y.key[j + T];
    }
    if (!y.leaf) {
      for (int j = 0; j < T; j++) {
        z.child[j] = y.child[j + T];
      }
    }
    y.n = T - 1;
    for (int j = x.n; j >= pos + 1; j--) {
      x.child[j + 1] = x.child[j];
    }
    x.child[pos + 1] = z;

    for (int j = x.n - 1; j >= pos; j--) {
      x.key[j + 1] = x.key[j];
    }
    x.key[pos] = y.key[T - 1];
    x.n = x.n + 1;
  }

  // Inserting a value
  public void Insert(final int key) {
    Node r = root;
    if (r.n == 2 * T - 1) {
      Node s = new Node();
      root = s;
      s.leaf = false;
      s.n = 0;
      s.child[0] = r;
      Split(s, 0, r);
      insertValue(s, key);
    } else {
      insertValue(r, key);
    }
  }

  // Insert the node
  final private void insertValue(Node x, int k) {

    if (x.leaf) {
      int i = 0;
      for (i = x.n - 1; i >= 0 && k < x.key[i]; i--) {
        x.key[i + 1] = x.key[i];
      }
      x.key[i + 1] = k;
      x.n = x.n + 1;
    } else {
      int i = 0;
      for (i = x.n - 1; i >= 0 && k < x.key[i]; i--) {
      }
      ;
      i++;
      Node tmp = x.child[i];
      if (tmp.n == 2 * T - 1) {
        Split(x, i, tmp);
        if (k > x.key[i]) {
          i++;
        }
      }
      insertValue(x.child[i], k);
    }

  }

  public void Show() {
    Show(root);
  }

  // Display
  private void Show(Node x) {
    assert (x == null);
    for (int i = 0; i < x.n; i++) {
      System.out.print(x.key[i] + "" "");
    }
    if (!x.leaf) {
      for (int i = 0; i < x.n + 1; i++) {
        Show(x.child[i]);
      }
    }
  }

  // Check if present
  public boolean Contain(int k) {
    if (this.Search(root, k) != null) {
      return true;
    } else {
      return false;
    }
  }

  public static void main(String[] args) {
    BTree b = new BTree(3);
    b.Insert(8);
    b.Insert(9);
    b.Insert(10);
    b.Insert(11);
    b.Insert(15);
    b.Insert(20);
    b.Insert(17);

    b.Show();

    if (b.Contain(12)) {
      System.out.println(""\nfound"");
    } else {
      System.out.println(""\nnot found"");
    }
    ;
  }
}"
B-tree,"B-tree is a special type of self-balancing search tree in which each node can contain more than one key and can have more than two children. It is a generalized form of the binary search tree. It is also known as a height-balanced m-way tree. The need for B-tree arose with the rise in the need for lesser time in accessing physical storage media like a hard disk. The secondary storage devices are slower with a larger capacity. There was a need for such types of data structures that minimize the disk access. Other data structures such as a binary search tree, avl tree, red-black tree, etc can store only one key in one node. If you have to store a large number of keys, then the height of such trees becomes very large, and the access time increases. However, B-tree can store many keys in a single node and can have multiple child nodes. This decreases the height significantly allowing faster disk accesses. For each node x, the keys are stored in increasing order. In each node, there is a boolean value x.leaf which is true if x is a leaf. If n is the order of the tree, each internal node can contain at most n - 1 keys along with a pointer to each child. Each node except root can have at most n children and at least n/2 children. All leaves have the same depth (i.e. height-h of the tree). The root has at least 2 children and contains a minimum of 1 key. If n ≥ 1, then for any n-key B-tree of height h and minimum degree t ≥ 2, h ≥ logt (n+1)/2. Searching for an element in a B-tree is the generalized form of searching an element in a Binary Search Tree. The following steps are followed. Starting from the root node, compare k with the first key of the node.
		If k = the first key of the node, return the node and the index. If k.leaf = true, return NULL (i.e. not found). If k < the first key of the root node, search the left child of this key recursively. If there is more than one key in the current node and k > the first key, compare k with the next key in the node.
		If k < next key, search the left child of this key (ie. k lies in between the first and the second keys).
		Else, search the right child of the key. Repeat steps 1 to 4 until the leaf is reached. Let us search key k = 17 in the tree below of degree 3.

		
			B-tree k is not found in the root so, compare it with the root key.
		
			k is not found on the root node Since k > 11, go to the right child of the root node.
		
			Go to the right subtree Compare k with 16. Since k > 16, compare k with the next key 18.
		
			Compare with the keys from left to right Since k < 18, k lies between 16 and 18. Search in the right child of 16 or the left child of 18.
		
			k lies in between 16 and 18 k is found.
		
			k is found To learn more about different B-tree operations, please visit Insertion on B-tree
	Deletion on B-tree Insertion on B-tree Deletion on B-tree  Worst case Time complexity: Θ(log n) Average case Time complexity: Θ(log n) Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) databases and file systems
	to store blocks of data (secondary storage media)
	multilevel indexing databases and file systems to store blocks of data (secondary storage media) multilevel indexing","// Searching a key on a B-tree in C

#include <stdio.h>
#include <stdlib.h>

#define MAX 3
#define MIN 2

struct BTreeNode {
  int val[MAX + 1], count;
  struct BTreeNode *link[MAX + 1];
};

struct BTreeNode *root;

// Create a node
struct BTreeNode *createNode(int val, struct BTreeNode *child) {
  struct BTreeNode *newNode;
  newNode = (struct BTreeNode *)malloc(sizeof(struct BTreeNode));
  newNode->val[1] = val;
  newNode->count = 1;
  newNode->link[0] = root;
  newNode->link[1] = child;
  return newNode;
}

// Insert node
void insertNode(int val, int pos, struct BTreeNode *node,
        struct BTreeNode *child) {
  int j = node->count;
  while (j > pos) {
    node->val[j + 1] = node->val[j];
    node->link[j + 1] = node->link[j];
    j--;
  }
  node->val[j + 1] = val;
  node->link[j + 1] = child;
  node->count++;
}

// Split node
void splitNode(int val, int *pval, int pos, struct BTreeNode *node,
         struct BTreeNode *child, struct BTreeNode **newNode) {
  int median, j;

  if (pos > MIN)
    median = MIN + 1;
  else
    median = MIN;

  *newNode = (struct BTreeNode *)malloc(sizeof(struct BTreeNode));
  j = median + 1;
  while (j <= MAX) {
    (*newNode)->val[j - median] = node->val[j];
    (*newNode)->link[j - median] = node->link[j];
    j++;
  }
  node->count = median;
  (*newNode)->count = MAX - median;

  if (pos <= MIN) {
    insertNode(val, pos, node, child);
  } else {
    insertNode(val, pos - median, *newNode, child);
  }
  *pval = node->val[node->count];
  (*newNode)->link[0] = node->link[node->count];
  node->count--;
}

// Set the value
int setValue(int val, int *pval,
           struct BTreeNode *node, struct BTreeNode **child) {
  int pos;
  if (!node) {
    *pval = val;
    *child = NULL;
    return 1;
  }

  if (val < node->val[1]) {
    pos = 0;
  } else {
    for (pos = node->count;
       (val < node->val[pos] && pos > 1); pos--)
      ;
    if (val == node->val[pos]) {
      printf(""Duplicates are not permitted\n"");
      return 0;
    }
  }
  if (setValue(val, pval, node->link[pos], child)) {
    if (node->count < MAX) {
      insertNode(*pval, pos, node, *child);
    } else {
      splitNode(*pval, pval, pos, node, *child, child);
      return 1;
    }
  }
  return 0;
}

// Insert the value
void insert(int val) {
  int flag, i;
  struct BTreeNode *child;

  flag = setValue(val, &i, root, &child);
  if (flag)
    root = createNode(i, child);
}

// Search node
void search(int val, int *pos, struct BTreeNode *myNode) {
  if (!myNode) {
    return;
  }

  if (val < myNode->val[1]) {
    *pos = 0;
  } else {
    for (*pos = myNode->count;
       (val < myNode->val[*pos] && *pos > 1); (*pos)--)
      ;
    if (val == myNode->val[*pos]) {
      printf(""%d is found"", val);
      return;
    }
  }
  search(val, pos, myNode->link[*pos]);

  return;
}

// Traverse then nodes
void traversal(struct BTreeNode *myNode) {
  int i;
  if (myNode) {
    for (i = 0; i < myNode->count; i++) {
      traversal(myNode->link[i]);
      printf(""%d "", myNode->val[i + 1]);
    }
    traversal(myNode->link[i]);
  }
}

int main() {
  int val, ch;

  insert(8);
  insert(9);
  insert(10);
  insert(11);
  insert(15);
  insert(16);
  insert(17);
  insert(18);
  insert(20);
  insert(23);

  traversal(root);

  printf(""\n"");
  search(11, &ch, root);
}"
B-tree,"B-tree is a special type of self-balancing search tree in which each node can contain more than one key and can have more than two children. It is a generalized form of the binary search tree. It is also known as a height-balanced m-way tree. The need for B-tree arose with the rise in the need for lesser time in accessing physical storage media like a hard disk. The secondary storage devices are slower with a larger capacity. There was a need for such types of data structures that minimize the disk access. Other data structures such as a binary search tree, avl tree, red-black tree, etc can store only one key in one node. If you have to store a large number of keys, then the height of such trees becomes very large, and the access time increases. However, B-tree can store many keys in a single node and can have multiple child nodes. This decreases the height significantly allowing faster disk accesses. For each node x, the keys are stored in increasing order. In each node, there is a boolean value x.leaf which is true if x is a leaf. If n is the order of the tree, each internal node can contain at most n - 1 keys along with a pointer to each child. Each node except root can have at most n children and at least n/2 children. All leaves have the same depth (i.e. height-h of the tree). The root has at least 2 children and contains a minimum of 1 key. If n ≥ 1, then for any n-key B-tree of height h and minimum degree t ≥ 2, h ≥ logt (n+1)/2. Searching for an element in a B-tree is the generalized form of searching an element in a Binary Search Tree. The following steps are followed. Starting from the root node, compare k with the first key of the node.
		If k = the first key of the node, return the node and the index. If k.leaf = true, return NULL (i.e. not found). If k < the first key of the root node, search the left child of this key recursively. If there is more than one key in the current node and k > the first key, compare k with the next key in the node.
		If k < next key, search the left child of this key (ie. k lies in between the first and the second keys).
		Else, search the right child of the key. Repeat steps 1 to 4 until the leaf is reached. Let us search key k = 17 in the tree below of degree 3.

		
			B-tree k is not found in the root so, compare it with the root key.
		
			k is not found on the root node Since k > 11, go to the right child of the root node.
		
			Go to the right subtree Compare k with 16. Since k > 16, compare k with the next key 18.
		
			Compare with the keys from left to right Since k < 18, k lies between 16 and 18. Search in the right child of 16 or the left child of 18.
		
			k lies in between 16 and 18 k is found.
		
			k is found To learn more about different B-tree operations, please visit Insertion on B-tree
	Deletion on B-tree Insertion on B-tree Deletion on B-tree  Worst case Time complexity: Θ(log n) Average case Time complexity: Θ(log n) Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) databases and file systems
	to store blocks of data (secondary storage media)
	multilevel indexing databases and file systems to store blocks of data (secondary storage media) multilevel indexing","// Searching a key on a B-tree in C++

#include <iostream>
using namespace std;

class TreeNode {
  int *keys;
  int t;
  TreeNode **C;
  int n;
  bool leaf;

   public:
  TreeNode(int temp, bool bool_leaf);

  void insertNonFull(int k);
  void splitChild(int i, TreeNode *y);
  void traverse();

  TreeNode *search(int k);

  friend class BTree;
};

class BTree {
  TreeNode *root;
  int t;

   public:
  BTree(int temp) {
    root = NULL;
    t = temp;
  }

  void traverse() {
    if (root != NULL)
      root->traverse();
  }

  TreeNode *search(int k) {
    return (root == NULL) ? NULL : root->search(k);
  }

  void insert(int k);
};

TreeNode::TreeNode(int t1, bool leaf1) {
  t = t1;
  leaf = leaf1;

  keys = new int[2 * t - 1];
  C = new TreeNode *[2 * t];

  n = 0;
}

void TreeNode::traverse() {
  int i;
  for (i = 0; i < n; i++) {
    if (leaf == false)
      C[i]->traverse();
    cout << "" "" << keys[i];
  }

  if (leaf == false)
    C[i]->traverse();
}

TreeNode *TreeNode::search(int k) {
  int i = 0;
  while (i < n && k > keys[i])
    i++;

  if (keys[i] == k)
    return this;

  if (leaf == true)
    return NULL;

  return C[i]->search(k);
}

void BTree::insert(int k) {
  if (root == NULL) {
    root = new TreeNode(t, true);
    root->keys[0] = k;
    root->n = 1;
  } else {
    if (root->n == 2 * t - 1) {
      TreeNode *s = new TreeNode(t, false);

      s->C[0] = root;

      s->splitChild(0, root);

      int i = 0;
      if (s->keys[0] < k)
        i++;
      s->C[i]->insertNonFull(k);

      root = s;
    } else
      root->insertNonFull(k);
  }
}

void TreeNode::insertNonFull(int k) {
  int i = n - 1;

  if (leaf == true) {
    while (i >= 0 && keys[i] > k) {
      keys[i + 1] = keys[i];
      i--;
    }

    keys[i + 1] = k;
    n = n + 1;
  } else {
    while (i >= 0 && keys[i] > k)
      i--;

    if (C[i + 1]->n == 2 * t - 1) {
      splitChild(i + 1, C[i + 1]);

      if (keys[i + 1] < k)
        i++;
    }
    C[i + 1]->insertNonFull(k);
  }
}

void TreeNode::splitChild(int i, TreeNode *y) {
  TreeNode *z = new TreeNode(y->t, y->leaf);
  z->n = t - 1;

  for (int j = 0; j < t - 1; j++)
    z->keys[j] = y->keys[j + t];

  if (y->leaf == false) {
    for (int j = 0; j < t; j++)
      z->C[j] = y->C[j + t];
  }

  y->n = t - 1;
  for (int j = n; j >= i + 1; j--)
    C[j + 1] = C[j];

  C[i + 1] = z;

  for (int j = n - 1; j >= i; j--)
    keys[j + 1] = keys[j];

  keys[i] = y->keys[t - 1];
  n = n + 1;
}

int main() {
  BTree t(3);
  t.insert(8);
  t.insert(9);
  t.insert(10);
  t.insert(11);
  t.insert(15);
  t.insert(16);
  t.insert(17);
  t.insert(18);
  t.insert(20);
  t.insert(23);

  cout << ""The B-tree is: "";
  t.traverse();

  int k = 10;
  (t.search(k) != NULL) ? cout << endl
                 << k << "" is found""
              : cout << endl
                 << k << "" is not Found"";

  k = 2;
  (t.search(k) != NULL) ? cout << endl
                 << k << "" is found""
              : cout << endl
                 << k << "" is not Found\n"";
}"
Insertion into a B-tree,"Inserting an element on a B-tree consists of two events: searching the appropriate node to insert the element and splitting the node if required.Insertion operation always takes place in the bottom-up approach. Let us understand these events below. If the tree is empty, allocate a root node and insert the key. Update the allowed number of keys in the node. Search the appropriate node for insertion. If the node is full, follow the steps below. Insert the elements in increasing order. Now, there are elements greater than its limit. So, split at the median. Push the median key upwards and make the left keys as a left child and the right keys as a right child. If the node is not full, follow the steps below. Insert the node in increasing order. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 8, 9, 10, 11, 15, 20, 17.","# Inserting a key on a B-tree in Python


# Create a node
class BTreeNode:
    def __init__(self, leaf=False):
        self.leaf = leaf
        self.keys = []
        self.child = []


# Tree
class BTree:
    def __init__(self, t):
        self.root = BTreeNode(True)
        self.t = t

    # Insert node
    def insert(self, k):
        root = self.root
        if len(root.keys) == (2 * self.t) - 1:
            temp = BTreeNode()
            self.root = temp
            temp.child.insert(0, root)
            self.split_child(temp, 0)
            self.insert_non_full(temp, k)
        else:
            self.insert_non_full(root, k)

    # Insert nonfull
    def insert_non_full(self, x, k):
        i = len(x.keys) - 1
        if x.leaf:
            x.keys.append((None, None))
            while i >= 0 and k[0] < x.keys[i][0]:
                x.keys[i + 1] = x.keys[i]
                i -= 1
            x.keys[i + 1] = k
        else:
            while i >= 0 and k[0] < x.keys[i][0]:
                i -= 1
            i += 1
            if len(x.child[i].keys) == (2 * self.t) - 1:
                self.split_child(x, i)
                if k[0] > x.keys[i][0]:
                    i += 1
            self.insert_non_full(x.child[i], k)

    # Split the child
    def split_child(self, x, i):
        t = self.t
        y = x.child[i]
        z = BTreeNode(y.leaf)
        x.child.insert(i + 1, z)
        x.keys.insert(i, y.keys[t - 1])
        z.keys = y.keys[t: (2 * t) - 1]
        y.keys = y.keys[0: t - 1]
        if not y.leaf:
            z.child = y.child[t: 2 * t]
            y.child = y.child[0: t - 1]

    # Print the tree
    def print_tree(self, x, l=0):
        print(""Level "", l, "" "", len(x.keys), end="":"")
        for i in x.keys:
            print(i, end="" "")
        print()
        l += 1
        if len(x.child) > 0:
            for i in x.child:
                self.print_tree(i, l)


def main():
    B = BTree(3)

    for i in range(10):
        B.insert((i, 2 * i))

    B.print_tree(B.root)


if __name__ == '__main__':
    main()"
Insertion into a B-tree,"Inserting an element on a B-tree consists of two events: searching the appropriate node to insert the element and splitting the node if required.Insertion operation always takes place in the bottom-up approach. Let us understand these events below. If the tree is empty, allocate a root node and insert the key. Update the allowed number of keys in the node. Search the appropriate node for insertion. If the node is full, follow the steps below. Insert the elements in increasing order. Now, there are elements greater than its limit. So, split at the median. Push the median key upwards and make the left keys as a left child and the right keys as a right child. If the node is not full, follow the steps below. Insert the node in increasing order. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 8, 9, 10, 11, 15, 20, 17.","// Inserting a key on a B-tree in Java 

public class BTree {

  private int T;

  // Node Creation
  public class Node {
    int n;
    int key[] = new int[2 * T - 1];
    Node child[] = new Node[2 * T];
    boolean leaf = true;

    public int Find(int k) {
      for (int i = 0; i < this.n; i++) {
        if (this.key[i] == k) {
          return i;
        }
      }
      return -1;
    };
  }

  public BTree(int t) {
    T = t;
    root = new Node();
    root.n = 0;
    root.leaf = true;
  }

  private Node root;

  // split
  private void split(Node x, int pos, Node y) {
    Node z = new Node();
    z.leaf = y.leaf;
    z.n = T - 1;
    for (int j = 0; j < T - 1; j++) {
      z.key[j] = y.key[j + T];
    }
    if (!y.leaf) {
      for (int j = 0; j < T; j++) {
        z.child[j] = y.child[j + T];
      }
    }
    y.n = T - 1;
    for (int j = x.n; j >= pos + 1; j--) {
      x.child[j + 1] = x.child[j];
    }
    x.child[pos + 1] = z;

    for (int j = x.n - 1; j >= pos; j--) {
      x.key[j + 1] = x.key[j];
    }
    x.key[pos] = y.key[T - 1];
    x.n = x.n + 1;
  }

  // insert key
  public void insert(final int key) {
    Node r = root;
    if (r.n == 2 * T - 1) {
      Node s = new Node();
      root = s;
      s.leaf = false;
      s.n = 0;
      s.child[0] = r;
      split(s, 0, r);
      _insert(s, key);
    } else {
      _insert(r, key);
    }
  }

  // insert node
  final private void _insert(Node x, int k) {

    if (x.leaf) {
      int i = 0;
      for (i = x.n - 1; i >= 0 && k < x.key[i]; i--) {
        x.key[i + 1] = x.key[i];
      }
      x.key[i + 1] = k;
      x.n = x.n + 1;
    } else {
      int i = 0;
      for (i = x.n - 1; i >= 0 && k < x.key[i]; i--) {
      }
      ;
      i++;
      Node tmp = x.child[i];
      if (tmp.n == 2 * T - 1) {
        split(x, i, tmp);
        if (k > x.key[i]) {
          i++;
        }
      }
      _insert(x.child[i], k);
    }

  }

  public void display() {
    display(root);
  }

  // Display the tree
  private void display(Node x) {
    assert (x == null);
    for (int i = 0; i < x.n; i++) {
      System.out.print(x.key[i] + "" "");
    }
    if (!x.leaf) {
      for (int i = 0; i < x.n + 1; i++) {
        display(x.child[i]);
      }
    }
  }

  public static void main(String[] args) {
    BTree b = new BTree(3);
    b.insert(8);
    b.insert(9);
    b.insert(10);
    b.insert(11);
    b.insert(15);
    b.insert(20);
    b.insert(17);

    b.display();
  }
}"
Insertion into a B-tree,"Inserting an element on a B-tree consists of two events: searching the appropriate node to insert the element and splitting the node if required.Insertion operation always takes place in the bottom-up approach. Let us understand these events below. If the tree is empty, allocate a root node and insert the key. Update the allowed number of keys in the node. Search the appropriate node for insertion. If the node is full, follow the steps below. Insert the elements in increasing order. Now, there are elements greater than its limit. So, split at the median. Push the median key upwards and make the left keys as a left child and the right keys as a right child. If the node is not full, follow the steps below. Insert the node in increasing order. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 8, 9, 10, 11, 15, 20, 17.","// insertioning a key on a B-tree in C

#include <stdio.h>
#include <stdlib.h>

#define MAX 3
#define MIN 2

struct btreeNode {
  int item[MAX + 1], count;
  struct btreeNode *link[MAX + 1];
};

struct btreeNode *root;

// Node creation
struct btreeNode *createNode(int item, struct btreeNode *child) {
  struct btreeNode *newNode;
  newNode = (struct btreeNode *)malloc(sizeof(struct btreeNode));
  newNode->item[1] = item;
  newNode->count = 1;
  newNode->link[0] = root;
  newNode->link[1] = child;
  return newNode;
}

// Insert
void insertValue(int item, int pos, struct btreeNode *node,
          struct btreeNode *child) {
  int j = node->count;
  while (j > pos) {
    node->item[j + 1] = node->item[j];
    node->link[j + 1] = node->link[j];
    j--;
  }
  node->item[j + 1] = item;
  node->link[j + 1] = child;
  node->count++;
}

// Split node
void splitNode(int item, int *pval, int pos, struct btreeNode *node,
         struct btreeNode *child, struct btreeNode **newNode) {
  int median, j;

  if (pos > MIN)
    median = MIN + 1;
  else
    median = MIN;

  *newNode = (struct btreeNode *)malloc(sizeof(struct btreeNode));
  j = median + 1;
  while (j <= MAX) {
    (*newNode)->item[j - median] = node->item[j];
    (*newNode)->link[j - median] = node->link[j];
    j++;
  }
  node->count = median;
  (*newNode)->count = MAX - median;

  if (pos <= MIN) {
    insertValue(item, pos, node, child);
  } else {
    insertValue(item, pos - median, *newNode, child);
  }
  *pval = node->item[node->count];
  (*newNode)->link[0] = node->link[node->count];
  node->count--;
}

// Set the value of node
int setNodeValue(int item, int *pval,
           struct btreeNode *node, struct btreeNode **child) {
  int pos;
  if (!node) {
    *pval = item;
    *child = NULL;
    return 1;
  }

  if (item < node->item[1]) {
    pos = 0;
  } else {
    for (pos = node->count;
       (item < node->item[pos] && pos > 1); pos--)
      ;
    if (item == node->item[pos]) {
      printf(""Duplicates not allowed\n"");
      return 0;
    }
  }
  if (setNodeValue(item, pval, node->link[pos], child)) {
    if (node->count < MAX) {
      insertValue(*pval, pos, node, *child);
    } else {
      splitNode(*pval, pval, pos, node, *child, child);
      return 1;
    }
  }
  return 0;
}

// Insert the value
void insertion(int item) {
  int flag, i;
  struct btreeNode *child;

  flag = setNodeValue(item, &i, root, &child);
  if (flag)
    root = createNode(i, child);
}

// Copy the successor
void copySuccessor(struct btreeNode *myNode, int pos) {
  struct btreeNode *dummy;
  dummy = myNode->link[pos];

  for (; dummy->link[0] != NULL;)
    dummy = dummy->link[0];
  myNode->item[pos] = dummy->item[1];
}

// Do rightshift
void rightShift(struct btreeNode *myNode, int pos) {
  struct btreeNode *x = myNode->link[pos];
  int j = x->count;

  while (j > 0) {
    x->item[j + 1] = x->item[j];
    x->link[j + 1] = x->link[j];
  }
  x->item[1] = myNode->item[pos];
  x->link[1] = x->link[0];
  x->count++;

  x = myNode->link[pos - 1];
  myNode->item[pos] = x->item[x->count];
  myNode->link[pos] = x->link[x->count];
  x->count--;
  return;
}

// Do leftshift
void leftShift(struct btreeNode *myNode, int pos) {
  int j = 1;
  struct btreeNode *x = myNode->link[pos - 1];

  x->count++;
  x->item[x->count] = myNode->item[pos];
  x->link[x->count] = myNode->link[pos]->link[0];

  x = myNode->link[pos];
  myNode->item[pos] = x->item[1];
  x->link[0] = x->link[1];
  x->count--;

  while (j <= x->count) {
    x->item[j] = x->item[j + 1];
    x->link[j] = x->link[j + 1];
    j++;
  }
  return;
}

// Merge the nodes
void mergeNodes(struct btreeNode *myNode, int pos) {
  int j = 1;
  struct btreeNode *x1 = myNode->link[pos], *x2 = myNode->link[pos - 1];

  x2->count++;
  x2->item[x2->count] = myNode->item[pos];
  x2->link[x2->count] = myNode->link[0];

  while (j <= x1->count) {
    x2->count++;
    x2->item[x2->count] = x1->item[j];
    x2->link[x2->count] = x1->link[j];
    j++;
  }

  j = pos;
  while (j < myNode->count) {
    myNode->item[j] = myNode->item[j + 1];
    myNode->link[j] = myNode->link[j + 1];
    j++;
  }
  myNode->count--;
  free(x1);
}

// Adjust the node
void adjustNode(struct btreeNode *myNode, int pos) {
  if (!pos) {
    if (myNode->link[1]->count > MIN) {
      leftShift(myNode, 1);
    } else {
      mergeNodes(myNode, 1);
    }
  } else {
    if (myNode->count != pos) {
      if (myNode->link[pos - 1]->count > MIN) {
        rightShift(myNode, pos);
      } else {
        if (myNode->link[pos + 1]->count > MIN) {
          leftShift(myNode, pos + 1);
        } else {
          mergeNodes(myNode, pos);
        }
      }
    } else {
      if (myNode->link[pos - 1]->count > MIN)
        rightShift(myNode, pos);
      else
        mergeNodes(myNode, pos);
    }
  }
}

// Traverse the tree
void traversal(struct btreeNode *myNode) {
  int i;
  if (myNode) {
    for (i = 0; i < myNode->count; i++) {
      traversal(myNode->link[i]);
      printf(""%d "", myNode->item[i + 1]);
    }
    traversal(myNode->link[i]);
  }
}

int main() {
  int item, ch;

  insertion(8);
  insertion(9);
  insertion(10);
  insertion(11);
  insertion(15);
  insertion(16);
  insertion(17);
  insertion(18);
  insertion(20);
  insertion(23);

  traversal(root);
}"
Insertion into a B-tree,"Inserting an element on a B-tree consists of two events: searching the appropriate node to insert the element and splitting the node if required.Insertion operation always takes place in the bottom-up approach. Let us understand these events below. If the tree is empty, allocate a root node and insert the key. Update the allowed number of keys in the node. Search the appropriate node for insertion. If the node is full, follow the steps below. Insert the elements in increasing order. Now, there are elements greater than its limit. So, split at the median. Push the median key upwards and make the left keys as a left child and the right keys as a right child. If the node is not full, follow the steps below. Insert the node in increasing order. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 8, 9, 10, 11, 15, 20, 17.","// Inserting a key on a B-tree in C++

#include <iostream>
using namespace std;

class Node {
  int *keys;
  int t;
  Node **C;
  int n;
  bool leaf;

   public:
  Node(int _t, bool _leaf);

  void insertNonFull(int k);
  void splitChild(int i, Node *y);
  void traverse();

  friend class BTree;
};

class BTree {
  Node *root;
  int t;

   public:
  BTree(int _t) {
    root = NULL;
    t = _t;
  }

  void traverse() {
    if (root != NULL)
      root->traverse();
  }

  void insert(int k);
};

Node::Node(int t1, bool leaf1) {
  t = t1;
  leaf = leaf1;

  keys = new int[2 * t - 1];
  C = new Node *[2 * t];

  n = 0;
}

// Traverse the nodes
void Node::traverse() {
  int i;
  for (i = 0; i < n; i++) {
    if (leaf == false)
      C[i]->traverse();
    cout << "" "" << keys[i];
  }

  if (leaf == false)
    C[i]->traverse();
}

// Insert the node
void BTree::insert(int k) {
  if (root == NULL) {
    root = new Node(t, true);
    root->keys[0] = k;
    root->n = 1;
  } else {
    if (root->n == 2 * t - 1) {
      Node *s = new Node(t, false);

      s->C[0] = root;

      s->splitChild(0, root);

      int i = 0;
      if (s->keys[0] < k)
        i++;
      s->C[i]->insertNonFull(k);

      root = s;
    } else
      root->insertNonFull(k);
  }
}

// Insert non full condition
void Node::insertNonFull(int k) {
  int i = n - 1;

  if (leaf == true) {
    while (i >= 0 && keys[i] > k) {
      keys[i + 1] = keys[i];
      i--;
    }

    keys[i + 1] = k;
    n = n + 1;
  } else {
    while (i >= 0 && keys[i] > k)
      i--;

    if (C[i + 1]->n == 2 * t - 1) {
      splitChild(i + 1, C[i + 1]);

      if (keys[i + 1] < k)
        i++;
    }
    C[i + 1]->insertNonFull(k);
  }
}

// split the child
void Node::splitChild(int i, Node *y) {
  Node *z = new Node(y->t, y->leaf);
  z->n = t - 1;

  for (int j = 0; j < t - 1; j++)
    z->keys[j] = y->keys[j + t];

  if (y->leaf == false) {
    for (int j = 0; j < t; j++)
      z->C[j] = y->C[j + t];
  }

  y->n = t - 1;
  for (int j = n; j >= i + 1; j--)
    C[j + 1] = C[j];

  C[i + 1] = z;

  for (int j = n - 1; j >= i; j--)
    keys[j + 1] = keys[j];

  keys[i] = y->keys[t - 1];
  n = n + 1;
}

int main() {
  BTree t(3);
  t.insert(8);
  t.insert(9);
  t.insert(10);
  t.insert(11);
  t.insert(15);
  t.insert(16);
  t.insert(17);
  t.insert(18);
  t.insert(20);
  t.insert(23);

  cout << ""The B-tree is: "";
  t.traverse();
}"
Deletion from a B-tree,"Deleting an element on a B-tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required. While deleting a tree, a condition called underflow may occur. Underflow occurs when a node contains less than the minimum number of keys it should hold. The terms to be understood before studying deletion operation are: Inorder Predecessor
		The largest key on the left child of a node is called its inorder predecessor. Inorder Successor
		The smallest key on the right child of a node is called its inorder successor. Before going through the steps below, one must know these facts about a B tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) There are three main cases for deletion operation in a B tree. The key to be deleted lies in the leaf. There are two cases for it. The deletion of the key does not violate the property of the minimum number of keys a node should hold.
		
		In the tree below, deleting 32 does not violate the above properties.
		
			Deleting a leaf key (32) from B-tree The deletion of the key violates the property of the minimum number of keys a node should hold. In this case, we borrow a key from its immediate neighboring sibling node in the order of left to right.
		
		First, visit the immediate left sibling. If the left sibling node has more than a minimum number of keys, then borrow a key from this node.
		
		Else, check to borrow from the immediate right sibling node.
		
		In the tree below, deleting 31 results in the above condition. Let us borrow a key from the left sibling node.
		
			Deleting a leaf key (31)
		
		If both the immediate sibling nodes already have a minimum number of keys, then merge the node with either the left sibling node or the right sibling node. This merging is done through the parent node.
		
		Deleting 30 results in the above case.
		 
		
			Delete a leaf key (30) If the key to be deleted lies in the internal node, the following cases occur. The internal node, which is deleted, is replaced by an inorder predecessor if the left child has more than the minimum number of keys.
		
			Deleting an internal node (33) The internal node, which is deleted, is replaced by an inorder successor if the right child has more than the minimum number of keys. If either child has exactly a minimum number of keys then, merge the left and the right children.
		 
		
			Deleting an internal node (30)
		
		After merging if the parent node has less than the minimum number of keys then, look for the siblings as in Case I.  In this case, the height of the tree shrinks. If the target key lies in an internal node, and the deletion of the key leads to a fewer number of keys in the node (i.e. less than the minimum required), then look for the inorder predecessor and the inorder successor. If both the children contain a minimum number of keys then, borrowing cannot take place. This leads to Case II(3) i.e. merging the children. Again, look for the sibling to borrow a key. But, if the sibling also has only a minimum number of keys then, merge the node with the sibling along with the parent. Arrange the children accordingly (increasing order). Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) ","# Deleting a key on a B-tree in Python


# Btree node
class BTreeNode:
    def __init__(self, leaf=False):
        self.leaf = leaf
        self.keys = []
        self.child = []


class BTree:
    def __init__(self, t):
        self.root = BTreeNode(True)
        self.t = t

    # Insert a key
    def insert(self, k):
        root = self.root
        if len(root.keys) == (2 * self.t) - 1:
            temp = BTreeNode()
            self.root = temp
            temp.child.insert(0, root)
            self.split_child(temp, 0)
            self.insert_non_full(temp, k)
        else:
            self.insert_non_full(root, k)

    # Insert non full
    def insert_non_full(self, x, k):
        i = len(x.keys) - 1
        if x.leaf:
            x.keys.append((None, None))
            while i >= 0 and k[0] < x.keys[i][0]:
                x.keys[i + 1] = x.keys[i]
                i -= 1
            x.keys[i + 1] = k
        else:
            while i >= 0 and k[0] < x.keys[i][0]:
                i -= 1
            i += 1
            if len(x.child[i].keys) == (2 * self.t) - 1:
                self.split_child(x, i)
                if k[0] > x.keys[i][0]:
                    i += 1
            self.insert_non_full(x.child[i], k)

    # Split the child
    def split_child(self, x, i):
        t = self.t
        y = x.child[i]
        z = BTreeNode(y.leaf)
        x.child.insert(i + 1, z)
        x.keys.insert(i, y.keys[t - 1])
        z.keys = y.keys[t: (2 * t) - 1]
        y.keys = y.keys[0: t - 1]
        if not y.leaf:
            z.child = y.child[t: 2 * t]
            y.child = y.child[0: t - 1]

    # Delete a node
    def delete(self, x, k):
        t = self.t
        i = 0
        while i < len(x.keys) and k[0] > x.keys[i][0]:
            i += 1
        if x.leaf:
            if i < len(x.keys) and x.keys[i][0] == k[0]:
                x.keys.pop(i)
                return
            return

        if i < len(x.keys) and x.keys[i][0] == k[0]:
            return self.delete_internal_node(x, k, i)
        elif len(x.child[i].keys) >= t:
            self.delete(x.child[i], k)
        else:
            if i != 0 and i + 2 < len(x.child):
                if len(x.child[i - 1].keys) >= t:
                    self.delete_sibling(x, i, i - 1)
                elif len(x.child[i + 1].keys) >= t:
                    self.delete_sibling(x, i, i + 1)
                else:
                    self.delete_merge(x, i, i + 1)
            elif i == 0:
                if len(x.child[i + 1].keys) >= t:
                    self.delete_sibling(x, i, i + 1)
                else:
                    self.delete_merge(x, i, i + 1)
            elif i + 1 == len(x.child):
                if len(x.child[i - 1].keys) >= t:
                    self.delete_sibling(x, i, i - 1)
                else:
                    self.delete_merge(x, i, i - 1)
            self.delete(x.child[i], k)

    # Delete internal node
    def delete_internal_node(self, x, k, i):
        t = self.t
        if x.leaf:
            if x.keys[i][0] == k[0]:
                x.keys.pop(i)
                return
            return

        if len(x.child[i].keys) >= t:
            x.keys[i] = self.delete_predecessor(x.child[i])
            return
        elif len(x.child[i + 1].keys) >= t:
            x.keys[i] = self.delete_successor(x.child[i + 1])
            return
        else:
            self.delete_merge(x, i, i + 1)
            self.delete_internal_node(x.child[i], k, self.t - 1)

    # Delete the predecessor
    def delete_predecessor(self, x):
        if x.leaf:
            return x.pop()
        n = len(x.keys) - 1
        if len(x.child[n].keys) >= self.t:
            self.delete_sibling(x, n + 1, n)
        else:
            self.delete_merge(x, n, n + 1)
        self.delete_predecessor(x.child[n])

    # Delete the successor
    def delete_successor(self, x):
        if x.leaf:
            return x.keys.pop(0)
        if len(x.child[1].keys) >= self.t:
            self.delete_sibling(x, 0, 1)
        else:
            self.delete_merge(x, 0, 1)
        self.delete_successor(x.child[0])

    # Delete resolution
    def delete_merge(self, x, i, j):
        cnode = x.child[i]

        if j > i:
            rsnode = x.child[j]
            cnode.keys.append(x.keys[i])
            for k in range(len(rsnode.keys)):
                cnode.keys.append(rsnode.keys[k])
                if len(rsnode.child) > 0:
                    cnode.child.append(rsnode.child[k])
            if len(rsnode.child) > 0:
                cnode.child.append(rsnode.child.pop())
            new = cnode
            x.keys.pop(i)
            x.child.pop(j)
        else:
            lsnode = x.child[j]
            lsnode.keys.append(x.keys[j])
            for i in range(len(cnode.keys)):
                lsnode.keys.append(cnode.keys[i])
                if len(lsnode.child) > 0:
                    lsnode.child.append(cnode.child[i])
            if len(lsnode.child) > 0:
                lsnode.child.append(cnode.child.pop())
            new = lsnode
            x.keys.pop(j)
            x.child.pop(i)

        if x == self.root and len(x.keys) == 0:
            self.root = new

    # Delete the sibling
    def delete_sibling(self, x, i, j):
        cnode = x.child[i]
        if i < j:
            rsnode = x.child[j]
            cnode.keys.append(x.keys[i])
            x.keys[i] = rsnode.keys[0]
            if len(rsnode.child) > 0:
                cnode.child.append(rsnode.child[0])
                rsnode.child.pop(0)
            rsnode.keys.pop(0)
        else:
            lsnode = x.child[j]
            cnode.keys.insert(0, x.keys[i - 1])
            x.keys[i - 1] = lsnode.keys.pop()
            if len(lsnode.child) > 0:
                cnode.child.insert(0, lsnode.child.pop())

    # Print the tree
    def print_tree(self, x, l=0):
        print(""Level "", l, "" "", len(x.keys), end="":"")
        for i in x.keys:
            print(i, end="" "")
        print()
        l += 1
        if len(x.child) > 0:
            for i in x.child:
                self.print_tree(i, l)



B = BTree(3)

for i in range(10):
    B.insert((i, 2 * i))

B.print_tree(B.root)

B.delete(B.root, (8,))
print(""\n"")
B.print_tree(B.root)"
Deletion from a B-tree,"Deleting an element on a B-tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required. While deleting a tree, a condition called underflow may occur. Underflow occurs when a node contains less than the minimum number of keys it should hold. The terms to be understood before studying deletion operation are: Inorder Predecessor
		The largest key on the left child of a node is called its inorder predecessor. Inorder Successor
		The smallest key on the right child of a node is called its inorder successor. Before going through the steps below, one must know these facts about a B tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) There are three main cases for deletion operation in a B tree. The key to be deleted lies in the leaf. There are two cases for it. The deletion of the key does not violate the property of the minimum number of keys a node should hold.
		
		In the tree below, deleting 32 does not violate the above properties.
		
			Deleting a leaf key (32) from B-tree The deletion of the key violates the property of the minimum number of keys a node should hold. In this case, we borrow a key from its immediate neighboring sibling node in the order of left to right.
		
		First, visit the immediate left sibling. If the left sibling node has more than a minimum number of keys, then borrow a key from this node.
		
		Else, check to borrow from the immediate right sibling node.
		
		In the tree below, deleting 31 results in the above condition. Let us borrow a key from the left sibling node.
		
			Deleting a leaf key (31)
		
		If both the immediate sibling nodes already have a minimum number of keys, then merge the node with either the left sibling node or the right sibling node. This merging is done through the parent node.
		
		Deleting 30 results in the above case.
		 
		
			Delete a leaf key (30) If the key to be deleted lies in the internal node, the following cases occur. The internal node, which is deleted, is replaced by an inorder predecessor if the left child has more than the minimum number of keys.
		
			Deleting an internal node (33) The internal node, which is deleted, is replaced by an inorder successor if the right child has more than the minimum number of keys. If either child has exactly a minimum number of keys then, merge the left and the right children.
		 
		
			Deleting an internal node (30)
		
		After merging if the parent node has less than the minimum number of keys then, look for the siblings as in Case I.  In this case, the height of the tree shrinks. If the target key lies in an internal node, and the deletion of the key leads to a fewer number of keys in the node (i.e. less than the minimum required), then look for the inorder predecessor and the inorder successor. If both the children contain a minimum number of keys then, borrowing cannot take place. This leads to Case II(3) i.e. merging the children. Again, look for the sibling to borrow a key. But, if the sibling also has only a minimum number of keys then, merge the node with the sibling along with the parent. Arrange the children accordingly (increasing order). Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) ","// Inserting a key on a B-tree in Java

import java.util.Stack;

public class BTree {

  private int T;

  public class Node {
    int n;
    int key[] = new int[2 * T - 1];
    Node child[] = new Node[2 * T];
    boolean leaf = true;

    public int Find(int k) {
      for (int i = 0; i < this.n; i++) {
        if (this.key[i] == k) {
          return i;
        }
      }
      return -1;
    };
  }

  public BTree(int t) {
    T = t;
    root = new Node();
    root.n = 0;
    root.leaf = true;
  }

  private Node root;

  // Search the key
  private Node Search(Node x, int key) {
    int i = 0;
    if (x == null)
      return x;
    for (i = 0; i < x.n; i++) {
      if (key < x.key[i]) {
        break;
      }
      if (key == x.key[i]) {
        return x;
      }
    }
    if (x.leaf) {
      return null;
    } else {
      return Search(x.child[i], key);
    }
  }

  // Split function
  private void Split(Node x, int pos, Node y) {
    Node z = new Node();
    z.leaf = y.leaf;
    z.n = T - 1;
    for (int j = 0; j < T - 1; j++) {
      z.key[j] = y.key[j + T];
    }
    if (!y.leaf) {
      for (int j = 0; j < T; j++) {
        z.child[j] = y.child[j + T];
      }
    }
    y.n = T - 1;
    for (int j = x.n; j >= pos + 1; j--) {
      x.child[j + 1] = x.child[j];
    }
    x.child[pos + 1] = z;

    for (int j = x.n - 1; j >= pos; j--) {
      x.key[j + 1] = x.key[j];
    }
    x.key[pos] = y.key[T - 1];
    x.n = x.n + 1;
  }

  // Insert the key
  public void Insert(final int key) {
    Node r = root;
    if (r.n == 2 * T - 1) {
      Node s = new Node();
      root = s;
      s.leaf = false;
      s.n = 0;
      s.child[0] = r;
      Split(s, 0, r);
      _Insert(s, key);
    } else {
      _Insert(r, key);
    }
  }

  // Insert the node
  final private void _Insert(Node x, int k) {

    if (x.leaf) {
      int i = 0;
      for (i = x.n - 1; i >= 0 && k < x.key[i]; i--) {
        x.key[i + 1] = x.key[i];
      }
      x.key[i + 1] = k;
      x.n = x.n + 1;
    } else {
      int i = 0;
      for (i = x.n - 1; i >= 0 && k < x.key[i]; i--) {
      }
      ;
      i++;
      Node tmp = x.child[i];
      if (tmp.n == 2 * T - 1) {
        Split(x, i, tmp);
        if (k > x.key[i]) {
          i++;
        }
      }
      _Insert(x.child[i], k);
    }

  }

  public void Show() {
    Show(root);
  }

  private void Remove(Node x, int key) {
    int pos = x.Find(key);
    if (pos != -1) {
      if (x.leaf) {
        int i = 0;
        for (i = 0; i < x.n && x.key[i] != key; i++) {
        }
        ;
        for (; i < x.n; i++) {
          if (i != 2 * T - 2) {
            x.key[i] = x.key[i + 1];
          }
        }
        x.n--;
        return;
      }
      if (!x.leaf) {

        Node pred = x.child[pos];
        int predKey = 0;
        if (pred.n >= T) {
          for (;;) {
            if (pred.leaf) {
              System.out.println(pred.n);
              predKey = pred.key[pred.n - 1];
              break;
            } else {
              pred = pred.child[pred.n];
            }
          }
          Remove(pred, predKey);
          x.key[pos] = predKey;
          return;
        }

        Node nextNode = x.child[pos + 1];
        if (nextNode.n >= T) {
          int nextKey = nextNode.key[0];
          if (!nextNode.leaf) {
            nextNode = nextNode.child[0];
            for (;;) {
              if (nextNode.leaf) {
                nextKey = nextNode.key[nextNode.n - 1];
                break;
              } else {
                nextNode = nextNode.child[nextNode.n];
              }
            }
          }
          Remove(nextNode, nextKey);
          x.key[pos] = nextKey;
          return;
        }

        int temp = pred.n + 1;
        pred.key[pred.n++] = x.key[pos];
        for (int i = 0, j = pred.n; i < nextNode.n; i++) {
          pred.key[j++] = nextNode.key[i];
          pred.n++;
        }
        for (int i = 0; i < nextNode.n + 1; i++) {
          pred.child[temp++] = nextNode.child[i];
        }

        x.child[pos] = pred;
        for (int i = pos; i < x.n; i++) {
          if (i != 2 * T - 2) {
            x.key[i] = x.key[i + 1];
          }
        }
        for (int i = pos + 1; i < x.n + 1; i++) {
          if (i != 2 * T - 1) {
            x.child[i] = x.child[i + 1];
          }
        }
        x.n--;
        if (x.n == 0) {
          if (x == root) {
            root = x.child[0];
          }
          x = x.child[0];
        }
        Remove(pred, key);
        return;
      }
    } else {
      for (pos = 0; pos < x.n; pos++) {
        if (x.key[pos] > key) {
          break;
        }
      }
      Node tmp = x.child[pos];
      if (tmp.n >= T) {
        Remove(tmp, key);
        return;
      }
      if (true) {
        Node nb = null;
        int devider = -1;

        if (pos != x.n && x.child[pos + 1].n >= T) {
          devider = x.key[pos];
          nb = x.child[pos + 1];
          x.key[pos] = nb.key[0];
          tmp.key[tmp.n++] = devider;
          tmp.child[tmp.n] = nb.child[0];
          for (int i = 1; i < nb.n; i++) {
            nb.key[i - 1] = nb.key[i];
          }
          for (int i = 1; i <= nb.n; i++) {
            nb.child[i - 1] = nb.child[i];
          }
          nb.n--;
          Remove(tmp, key);
          return;
        } else if (pos != 0 && x.child[pos - 1].n >= T) {

          devider = x.key[pos - 1];
          nb = x.child[pos - 1];
          x.key[pos - 1] = nb.key[nb.n - 1];
          Node child = nb.child[nb.n];
          nb.n--;

          for (int i = tmp.n; i > 0; i--) {
            tmp.key[i] = tmp.key[i - 1];
          }
          tmp.key[0] = devider;
          for (int i = tmp.n + 1; i > 0; i--) {
            tmp.child[i] = tmp.child[i - 1];
          }
          tmp.child[0] = child;
          tmp.n++;
          Remove(tmp, key);
          return;
        } else {
          Node lt = null;
          Node rt = null;
          boolean last = false;
          if (pos != x.n) {
            devider = x.key[pos];
            lt = x.child[pos];
            rt = x.child[pos + 1];
          } else {
            devider = x.key[pos - 1];
            rt = x.child[pos];
            lt = x.child[pos - 1];
            last = true;
            pos--;
          }
          for (int i = pos; i < x.n - 1; i++) {
            x.key[i] = x.key[i + 1];
          }
          for (int i = pos + 1; i < x.n; i++) {
            x.child[i] = x.child[i + 1];
          }
          x.n--;
          lt.key[lt.n++] = devider;

          for (int i = 0, j = lt.n; i < rt.n + 1; i++, j++) {
            if (i < rt.n) {
              lt.key[j] = rt.key[i];
            }
            lt.child[j] = rt.child[i];
          }
          lt.n += rt.n;
          if (x.n == 0) {
            if (x == root) {
              root = x.child[0];
            }
            x = x.child[0];
          }
          Remove(lt, key);
          return;
        }
      }
    }
  }

  public void Remove(int key) {
    Node x = Search(root, key);
    if (x == null) {
      return;
    }
    Remove(root, key);
  }

  public void Task(int a, int b) {
    Stack<Integer> st = new Stack<>();
    FindKeys(a, b, root, st);
    while (st.isEmpty() == false) {
      this.Remove(root, st.pop());
    }
  }

  private void FindKeys(int a, int b, Node x, Stack<Integer> st) {
    int i = 0;
    for (i = 0; i < x.n && x.key[i] < b; i++) {
      if (x.key[i] > a) {
        st.push(x.key[i]);
      }
    }
    if (!x.leaf) {
      for (int j = 0; j < i + 1; j++) {
        FindKeys(a, b, x.child[j], st);
      }
    }
  }

  public boolean Contain(int k) {
    if (this.Search(root, k) != null) {
      return true;
    } else {
      return false;
    }
  }

  // Show the node
  private void Show(Node x) {
    assert (x == null);
    for (int i = 0; i < x.n; i++) {
      System.out.print(x.key[i] + "" "");
    }
    if (!x.leaf) {
      for (int i = 0; i < x.n + 1; i++) {
        Show(x.child[i]);
      }
    }
  }

  public static void main(String[] args) {
    BTree b = new BTree(3);
    b.Insert(8);
    b.Insert(9);
    b.Insert(10);
    b.Insert(11);
    b.Insert(15);
    b.Insert(20);
    b.Insert(17);

    b.Show();

    b.Remove(10);
    System.out.println();
    b.Show();
  }
}"
Deletion from a B-tree,"Deleting an element on a B-tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required. While deleting a tree, a condition called underflow may occur. Underflow occurs when a node contains less than the minimum number of keys it should hold. The terms to be understood before studying deletion operation are: Inorder Predecessor
		The largest key on the left child of a node is called its inorder predecessor. Inorder Successor
		The smallest key on the right child of a node is called its inorder successor. Before going through the steps below, one must know these facts about a B tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) There are three main cases for deletion operation in a B tree. The key to be deleted lies in the leaf. There are two cases for it. The deletion of the key does not violate the property of the minimum number of keys a node should hold.
		
		In the tree below, deleting 32 does not violate the above properties.
		
			Deleting a leaf key (32) from B-tree The deletion of the key violates the property of the minimum number of keys a node should hold. In this case, we borrow a key from its immediate neighboring sibling node in the order of left to right.
		
		First, visit the immediate left sibling. If the left sibling node has more than a minimum number of keys, then borrow a key from this node.
		
		Else, check to borrow from the immediate right sibling node.
		
		In the tree below, deleting 31 results in the above condition. Let us borrow a key from the left sibling node.
		
			Deleting a leaf key (31)
		
		If both the immediate sibling nodes already have a minimum number of keys, then merge the node with either the left sibling node or the right sibling node. This merging is done through the parent node.
		
		Deleting 30 results in the above case.
		 
		
			Delete a leaf key (30) If the key to be deleted lies in the internal node, the following cases occur. The internal node, which is deleted, is replaced by an inorder predecessor if the left child has more than the minimum number of keys.
		
			Deleting an internal node (33) The internal node, which is deleted, is replaced by an inorder successor if the right child has more than the minimum number of keys. If either child has exactly a minimum number of keys then, merge the left and the right children.
		 
		
			Deleting an internal node (30)
		
		After merging if the parent node has less than the minimum number of keys then, look for the siblings as in Case I.  In this case, the height of the tree shrinks. If the target key lies in an internal node, and the deletion of the key leads to a fewer number of keys in the node (i.e. less than the minimum required), then look for the inorder predecessor and the inorder successor. If both the children contain a minimum number of keys then, borrowing cannot take place. This leads to Case II(3) i.e. merging the children. Again, look for the sibling to borrow a key. But, if the sibling also has only a minimum number of keys then, merge the node with the sibling along with the parent. Arrange the children accordingly (increasing order). Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) ","// Deleting a key from a B-tree in C

#include <stdio.h>
#include <stdlib.h>

#define MAX 3
#define MIN 2

struct BTreeNode {
  int item[MAX + 1], count;
  struct BTreeNode *linker[MAX + 1];
};

struct BTreeNode *root;

// Node creation
struct BTreeNode *createNode(int item, struct BTreeNode *child) {
  struct BTreeNode *newNode;
  newNode = (struct BTreeNode *)malloc(sizeof(struct BTreeNode));
  newNode->item[1] = item;
  newNode->count = 1;
  newNode->linker[0] = root;
  newNode->linker[1] = child;
  return newNode;
}

// Add value to the node
void addValToNode(int item, int pos, struct BTreeNode *node,
          struct BTreeNode *child) {
  int j = node->count;
  while (j > pos) {
    node->item[j + 1] = node->item[j];
    node->linker[j + 1] = node->linker[j];
    j--;
  }
  node->item[j + 1] = item;
  node->linker[j + 1] = child;
  node->count++;
}

// Split the node
void splitNode(int item, int *pval, int pos, struct BTreeNode *node,
         struct BTreeNode *child, struct BTreeNode **newNode) {
  int median, j;

  if (pos > MIN)
    median = MIN + 1;
  else
    median = MIN;

  *newNode = (struct BTreeNode *)malloc(sizeof(struct BTreeNode));
  j = median + 1;
  while (j <= MAX) {
    (*newNode)->item[j - median] = node->item[j];
    (*newNode)->linker[j - median] = node->linker[j];
    j++;
  }
  node->count = median;
  (*newNode)->count = MAX - median;

  if (pos <= MIN) {
    addValToNode(item, pos, node, child);
  } else {
    addValToNode(item, pos - median, *newNode, child);
  }
  *pval = node->item[node->count];
  (*newNode)->linker[0] = node->linker[node->count];
  node->count--;
}

// Set the value in the node
int setValueInNode(int item, int *pval,
           struct BTreeNode *node, struct BTreeNode **child) {
  int pos;
  if (!node) {
    *pval = item;
    *child = NULL;
    return 1;
  }

  if (item < node->item[1]) {
    pos = 0;
  } else {
    for (pos = node->count;
       (item < node->item[pos] && pos > 1); pos--)
      ;
    if (item == node->item[pos]) {
      printf(""Duplicates not allowed\n"");
      return 0;
    }
  }
  if (setValueInNode(item, pval, node->linker[pos], child)) {
    if (node->count < MAX) {
      addValToNode(*pval, pos, node, *child);
    } else {
      splitNode(*pval, pval, pos, node, *child, child);
      return 1;
    }
  }
  return 0;
}

// Insertion operation
void insertion(int item) {
  int flag, i;
  struct BTreeNode *child;

  flag = setValueInNode(item, &i, root, &child);
  if (flag)
    root = createNode(i, child);
}

// Copy the successor
void copySuccessor(struct BTreeNode *myNode, int pos) {
  struct BTreeNode *dummy;
  dummy = myNode->linker[pos];

  for (; dummy->linker[0] != NULL;)
    dummy = dummy->linker[0];
  myNode->item[pos] = dummy->item[1];
}

// Remove the value
void removeVal(struct BTreeNode *myNode, int pos) {
  int i = pos + 1;
  while (i <= myNode->count) {
    myNode->item[i - 1] = myNode->item[i];
    myNode->linker[i - 1] = myNode->linker[i];
    i++;
  }
  myNode->count--;
}

// Do right shift
void rightShift(struct BTreeNode *myNode, int pos) {
  struct BTreeNode *x = myNode->linker[pos];
  int j = x->count;

  while (j > 0) {
    x->item[j + 1] = x->item[j];
    x->linker[j + 1] = x->linker[j];
  }
  x->item[1] = myNode->item[pos];
  x->linker[1] = x->linker[0];
  x->count++;

  x = myNode->linker[pos - 1];
  myNode->item[pos] = x->item[x->count];
  myNode->linker[pos] = x->linker[x->count];
  x->count--;
  return;
}

// Do left shift
void leftShift(struct BTreeNode *myNode, int pos) {
  int j = 1;
  struct BTreeNode *x = myNode->linker[pos - 1];

  x->count++;
  x->item[x->count] = myNode->item[pos];
  x->linker[x->count] = myNode->linker[pos]->linker[0];

  x = myNode->linker[pos];
  myNode->item[pos] = x->item[1];
  x->linker[0] = x->linker[1];
  x->count--;

  while (j <= x->count) {
    x->item[j] = x->item[j + 1];
    x->linker[j] = x->linker[j + 1];
    j++;
  }
  return;
}

// Merge the nodes
void mergeNodes(struct BTreeNode *myNode, int pos) {
  int j = 1;
  struct BTreeNode *x1 = myNode->linker[pos], *x2 = myNode->linker[pos - 1];

  x2->count++;
  x2->item[x2->count] = myNode->item[pos];
  x2->linker[x2->count] = myNode->linker[0];

  while (j <= x1->count) {
    x2->count++;
    x2->item[x2->count] = x1->item[j];
    x2->linker[x2->count] = x1->linker[j];
    j++;
  }

  j = pos;
  while (j < myNode->count) {
    myNode->item[j] = myNode->item[j + 1];
    myNode->linker[j] = myNode->linker[j + 1];
    j++;
  }
  myNode->count--;
  free(x1);
}

// Adjust the node
void adjustNode(struct BTreeNode *myNode, int pos) {
  if (!pos) {
    if (myNode->linker[1]->count > MIN) {
      leftShift(myNode, 1);
    } else {
      mergeNodes(myNode, 1);
    }
  } else {
    if (myNode->count != pos) {
      if (myNode->linker[pos - 1]->count > MIN) {
        rightShift(myNode, pos);
      } else {
        if (myNode->linker[pos + 1]->count > MIN) {
          leftShift(myNode, pos + 1);
        } else {
          mergeNodes(myNode, pos);
        }
      }
    } else {
      if (myNode->linker[pos - 1]->count > MIN)
        rightShift(myNode, pos);
      else
        mergeNodes(myNode, pos);
    }
  }
}

// Delete a value from the node
int delValFromNode(int item, struct BTreeNode *myNode) {
  int pos, flag = 0;
  if (myNode) {
    if (item < myNode->item[1]) {
      pos = 0;
      flag = 0;
    } else {
      for (pos = myNode->count; (item < myNode->item[pos] && pos > 1); pos--)
        ;
      if (item == myNode->item[pos]) {
        flag = 1;
      } else {
        flag = 0;
      }
    }
    if (flag) {
      if (myNode->linker[pos - 1]) {
        copySuccessor(myNode, pos);
        flag = delValFromNode(myNode->item[pos], myNode->linker[pos]);
        if (flag == 0) {
          printf(""Given data is not present in B-Tree\n"");
        }
      } else {
        removeVal(myNode, pos);
      }
    } else {
      flag = delValFromNode(item, myNode->linker[pos]);
    }
    if (myNode->linker[pos]) {
      if (myNode->linker[pos]->count < MIN)
        adjustNode(myNode, pos);
    }
  }
  return flag;
}

// Delete operaiton
void delete (int item, struct BTreeNode *myNode) {
  struct BTreeNode *tmp;
  if (!delValFromNode(item, myNode)) {
    printf(""Not present\n"");
    return;
  } else {
    if (myNode->count == 0) {
      tmp = myNode;
      myNode = myNode->linker[0];
      free(tmp);
    }
  }
  root = myNode;
  return;
}

void searching(int item, int *pos, struct BTreeNode *myNode) {
  if (!myNode) {
    return;
  }

  if (item < myNode->item[1]) {
    *pos = 0;
  } else {
    for (*pos = myNode->count;
       (item < myNode->item[*pos] && *pos > 1); (*pos)--)
      ;
    if (item == myNode->item[*pos]) {
      printf(""%d present in B-tree"", item);
      return;
    }
  }
  searching(item, pos, myNode->linker[*pos]);
  return;
}

void traversal(struct BTreeNode *myNode) {
  int i;
  if (myNode) {
    for (i = 0; i < myNode->count; i++) {
      traversal(myNode->linker[i]);
      printf(""%d "", myNode->item[i + 1]);
    }
    traversal(myNode->linker[i]);
  }
}

int main() {
  int item, ch;

  insertion(8);
  insertion(9);
  insertion(10);
  insertion(11);
  insertion(15);
  insertion(16);
  insertion(17);
  insertion(18);
  insertion(20);
  insertion(23);

  traversal(root);

  delete (20, root);
  printf(""\n"");
  traversal(root);
}"
Deletion from a B-tree,"Deleting an element on a B-tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required. While deleting a tree, a condition called underflow may occur. Underflow occurs when a node contains less than the minimum number of keys it should hold. The terms to be understood before studying deletion operation are: Inorder Predecessor
		The largest key on the left child of a node is called its inorder predecessor. Inorder Successor
		The smallest key on the right child of a node is called its inorder successor. Before going through the steps below, one must know these facts about a B tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) There are three main cases for deletion operation in a B tree. The key to be deleted lies in the leaf. There are two cases for it. The deletion of the key does not violate the property of the minimum number of keys a node should hold.
		
		In the tree below, deleting 32 does not violate the above properties.
		
			Deleting a leaf key (32) from B-tree The deletion of the key violates the property of the minimum number of keys a node should hold. In this case, we borrow a key from its immediate neighboring sibling node in the order of left to right.
		
		First, visit the immediate left sibling. If the left sibling node has more than a minimum number of keys, then borrow a key from this node.
		
		Else, check to borrow from the immediate right sibling node.
		
		In the tree below, deleting 31 results in the above condition. Let us borrow a key from the left sibling node.
		
			Deleting a leaf key (31)
		
		If both the immediate sibling nodes already have a minimum number of keys, then merge the node with either the left sibling node or the right sibling node. This merging is done through the parent node.
		
		Deleting 30 results in the above case.
		 
		
			Delete a leaf key (30) If the key to be deleted lies in the internal node, the following cases occur. The internal node, which is deleted, is replaced by an inorder predecessor if the left child has more than the minimum number of keys.
		
			Deleting an internal node (33) The internal node, which is deleted, is replaced by an inorder successor if the right child has more than the minimum number of keys. If either child has exactly a minimum number of keys then, merge the left and the right children.
		 
		
			Deleting an internal node (30)
		
		After merging if the parent node has less than the minimum number of keys then, look for the siblings as in Case I.  In this case, the height of the tree shrinks. If the target key lies in an internal node, and the deletion of the key leads to a fewer number of keys in the node (i.e. less than the minimum required), then look for the inorder predecessor and the inorder successor. If both the children contain a minimum number of keys then, borrowing cannot take place. This leads to Case II(3) i.e. merging the children. Again, look for the sibling to borrow a key. But, if the sibling also has only a minimum number of keys then, merge the node with the sibling along with the parent. Arrange the children accordingly (increasing order). Best case Time complexity: Θ(log n) Average case Space complexity: Θ(n) Worst case Space complexity: Θ(n) ","// Deleting a key from a B-tree in C++

#include <iostream>
using namespace std;

class BTreeNode {
  int *keys;
  int t;
  BTreeNode **C;
  int n;
  bool leaf;

   public:
  BTreeNode(int _t, bool _leaf);

  void traverse();

  int findKey(int k);
  void insertNonFull(int k);
  void splitChild(int i, BTreeNode *y);
  void deletion(int k);
  void removeFromLeaf(int idx);
  void removeFromNonLeaf(int idx);
  int getPredecessor(int idx);
  int getSuccessor(int idx);
  void fill(int idx);
  void borrowFromPrev(int idx);
  void borrowFromNext(int idx);
  void merge(int idx);
  friend class BTree;
};

class BTree {
  BTreeNode *root;
  int t;

   public:
  BTree(int _t) {
    root = NULL;
    t = _t;
  }

  void traverse() {
    if (root != NULL)
      root->traverse();
  }

  void insertion(int k);

  void deletion(int k);
};

// B tree node
BTreeNode::BTreeNode(int t1, bool leaf1) {
  t = t1;
  leaf = leaf1;

  keys = new int[2 * t - 1];
  C = new BTreeNode *[2 * t];

  n = 0;
}

// Find the key
int BTreeNode::findKey(int k) {
  int idx = 0;
  while (idx < n && keys[idx] < k)
    ++idx;
  return idx;
}

// Deletion operation
void BTreeNode::deletion(int k) {
  int idx = findKey(k);

  if (idx < n && keys[idx] == k) {
    if (leaf)
      removeFromLeaf(idx);
    else
      removeFromNonLeaf(idx);
  } else {
    if (leaf) {
      cout << ""The key "" << k << "" is does not exist in the tree\n"";
      return;
    }

    bool flag = ((idx == n) ? true : false);

    if (C[idx]->n < t)
      fill(idx);

    if (flag && idx > n)
      C[idx - 1]->deletion(k);
    else
      C[idx]->deletion(k);
  }
  return;
}

// Remove from the leaf
void BTreeNode::removeFromLeaf(int idx) {
  for (int i = idx + 1; i < n; ++i)
    keys[i - 1] = keys[i];

  n--;

  return;
}

// Delete from non leaf node
void BTreeNode::removeFromNonLeaf(int idx) {
  int k = keys[idx];

  if (C[idx]->n >= t) {
    int pred = getPredecessor(idx);
    keys[idx] = pred;
    C[idx]->deletion(pred);
  }

  else if (C[idx + 1]->n >= t) {
    int succ = getSuccessor(idx);
    keys[idx] = succ;
    C[idx + 1]->deletion(succ);
  }

  else {
    merge(idx);
    C[idx]->deletion(k);
  }
  return;
}

int BTreeNode::getPredecessor(int idx) {
  BTreeNode *cur = C[idx];
  while (!cur->leaf)
    cur = cur->C[cur->n];

  return cur->keys[cur->n - 1];
}

int BTreeNode::getSuccessor(int idx) {
  BTreeNode *cur = C[idx + 1];
  while (!cur->leaf)
    cur = cur->C[0];

  return cur->keys[0];
}

void BTreeNode::fill(int idx) {
  if (idx != 0 && C[idx - 1]->n >= t)
    borrowFromPrev(idx);

  else if (idx != n && C[idx + 1]->n >= t)
    borrowFromNext(idx);

  else {
    if (idx != n)
      merge(idx);
    else
      merge(idx - 1);
  }
  return;
}

// Borrow from previous
void BTreeNode::borrowFromPrev(int idx) {
  BTreeNode *child = C[idx];
  BTreeNode *sibling = C[idx - 1];

  for (int i = child->n - 1; i >= 0; --i)
    child->keys[i + 1] = child->keys[i];

  if (!child->leaf) {
    for (int i = child->n; i >= 0; --i)
      child->C[i + 1] = child->C[i];
  }

  child->keys[0] = keys[idx - 1];

  if (!child->leaf)
    child->C[0] = sibling->C[sibling->n];

  keys[idx - 1] = sibling->keys[sibling->n - 1];

  child->n += 1;
  sibling->n -= 1;

  return;
}

// Borrow from the next
void BTreeNode::borrowFromNext(int idx) {
  BTreeNode *child = C[idx];
  BTreeNode *sibling = C[idx + 1];

  child->keys[(child->n)] = keys[idx];

  if (!(child->leaf))
    child->C[(child->n) + 1] = sibling->C[0];

  keys[idx] = sibling->keys[0];

  for (int i = 1; i < sibling->n; ++i)
    sibling->keys[i - 1] = sibling->keys[i];

  if (!sibling->leaf) {
    for (int i = 1; i <= sibling->n; ++i)
      sibling->C[i - 1] = sibling->C[i];
  }

  child->n += 1;
  sibling->n -= 1;

  return;
}

// Merge
void BTreeNode::merge(int idx) {
  BTreeNode *child = C[idx];
  BTreeNode *sibling = C[idx + 1];

  child->keys[t - 1] = keys[idx];

  for (int i = 0; i < sibling->n; ++i)
    child->keys[i + t] = sibling->keys[i];

  if (!child->leaf) {
    for (int i = 0; i <= sibling->n; ++i)
      child->C[i + t] = sibling->C[i];
  }

  for (int i = idx + 1; i < n; ++i)
    keys[i - 1] = keys[i];

  for (int i = idx + 2; i <= n; ++i)
    C[i - 1] = C[i];

  child->n += sibling->n + 1;
  n--;

  delete (sibling);
  return;
}

// Insertion operation
void BTree::insertion(int k) {
  if (root == NULL) {
    root = new BTreeNode(t, true);
    root->keys[0] = k;
    root->n = 1;
  } else {
    if (root->n == 2 * t - 1) {
      BTreeNode *s = new BTreeNode(t, false);

      s->C[0] = root;

      s->splitChild(0, root);

      int i = 0;
      if (s->keys[0] < k)
        i++;
      s->C[i]->insertNonFull(k);

      root = s;
    } else
      root->insertNonFull(k);
  }
}

// Insertion non full
void BTreeNode::insertNonFull(int k) {
  int i = n - 1;

  if (leaf == true) {
    while (i >= 0 && keys[i] > k) {
      keys[i + 1] = keys[i];
      i--;
    }

    keys[i + 1] = k;
    n = n + 1;
  } else {
    while (i >= 0 && keys[i] > k)
      i--;

    if (C[i + 1]->n == 2 * t - 1) {
      splitChild(i + 1, C[i + 1]);

      if (keys[i + 1] < k)
        i++;
    }
    C[i + 1]->insertNonFull(k);
  }
}

// Split child
void BTreeNode::splitChild(int i, BTreeNode *y) {
  BTreeNode *z = new BTreeNode(y->t, y->leaf);
  z->n = t - 1;

  for (int j = 0; j < t - 1; j++)
    z->keys[j] = y->keys[j + t];

  if (y->leaf == false) {
    for (int j = 0; j < t; j++)
      z->C[j] = y->C[j + t];
  }

  y->n = t - 1;

  for (int j = n; j >= i + 1; j--)
    C[j + 1] = C[j];

  C[i + 1] = z;

  for (int j = n - 1; j >= i; j--)
    keys[j + 1] = keys[j];

  keys[i] = y->keys[t - 1];

  n = n + 1;
}

// Traverse
void BTreeNode::traverse() {
  int i;
  for (i = 0; i < n; i++) {
    if (leaf == false)
      C[i]->traverse();
    cout << "" "" << keys[i];
  }

  if (leaf == false)
    C[i]->traverse();
}

// Delete Operation
void BTree::deletion(int k) {
  if (!root) {
    cout << ""The tree is empty\n"";
    return;
  }

  root->deletion(k);

  if (root->n == 0) {
    BTreeNode *tmp = root;
    if (root->leaf)
      root = NULL;
    else
      root = root->C[0];

    delete tmp;
  }
  return;
}

int main() {
  BTree t(3);
  t.insertion(8);
  t.insertion(9);
  t.insertion(10);
  t.insertion(11);
  t.insertion(15);
  t.insertion(16);
  t.insertion(17);
  t.insertion(18);
  t.insertion(20);
  t.insertion(23);

  cout << ""The B-tree is: "";
  t.traverse();

  t.deletion(20);

  cout << ""\nThe B-tree is: "";
  t.traverse();
}"
B+ Tree,"A B+ tree is an advanced form of a self-balancing tree in which all the values are present in the leaf level. An important concept to be understood before learning B+ tree is multilevel indexing. In multilevel indexing, the index of indices is created as in figure below. It makes accessing the data easier and faster. All leaves are at the same level. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys.  The data pointers are present only at the leaf nodes on a B+ tree whereas the data pointers are present in the internal, leaf or root nodes on a B-tree. The leaves are not connected with each other on a B-tree whereas they are connected on a B+ tree. Operations on a B+ tree are faster than on a B-tree.  The following steps are followed to search for data in a B+ Tree of order m. Let the data to be searched be k. Start from the root node. Compare k with the keys at the root node [k1, k2, k3,......km - 1]. If k < k1, go to the left child of the root node. Else if k == k1, compare k2. If k < k2, k lies between k1 and k2. So, search in the left child of k2. If k > k2, go for k3, k4,...km-1 as in steps 2 and 3. Repeat the above steps until a leaf node is reached. If k exists in the leaf node, return true else return false. Let us search k = 45 on the following B+ tree. Compare k with the root node.
		
			k is not found at the root Since k > 25, go to the right child.
		
			Go to right of the root Compare k with 35. Since k > 30, compare k with 45.
		
			k not found Since k ≥ 45, so go to the right child.
		
			go to the right k is found.
		
			k is found If linear search is implemented inside a node, then total complexity is Θ(logt n). If binary search is used, then total complexity is Θ(log2t.logt n). Multilevel Indexing
	Faster operations on the tree (insertion, deletion, search)
	Database indexing Multilevel Indexing Faster operations on the tree (insertion, deletion, search) Database indexing","# B+ tee in python


import math

# Node creation
class Node:
    def __init__(self, order):
        self.order = order
        self.values = []
        self.keys = []
        self.nextKey = None
        self.parent = None
        self.check_leaf = False

    # Insert at the leaf
    def insert_at_leaf(self, leaf, value, key):
        if (self.values):
            temp1 = self.values
            for i in range(len(temp1)):
                if (value == temp1[i]):
                    self.keys[i].append(key)
                    break
                elif (value < temp1[i]):
                    self.values = self.values[:i] + [value] + self.values[i:]
                    self.keys = self.keys[:i] + [[key]] + self.keys[i:]
                    break
                elif (i + 1 == len(temp1)):
                    self.values.append(value)
                    self.keys.append([key])
                    break
        else:
            self.values = [value]
            self.keys = [[key]]


# B plus tree
class BplusTree:
    def __init__(self, order):
        self.root = Node(order)
        self.root.check_leaf = True

    # Insert operation
    def insert(self, value, key):
        value = str(value)
        old_node = self.search(value)
        old_node.insert_at_leaf(old_node, value, key)

        if (len(old_node.values) == old_node.order):
            node1 = Node(old_node.order)
            node1.check_leaf = True
            node1.parent = old_node.parent
            mid = int(math.ceil(old_node.order / 2)) - 1
            node1.values = old_node.values[mid + 1:]
            node1.keys = old_node.keys[mid + 1:]
            node1.nextKey = old_node.nextKey
            old_node.values = old_node.values[:mid + 1]
            old_node.keys = old_node.keys[:mid + 1]
            old_node.nextKey = node1
            self.insert_in_parent(old_node, node1.values[0], node1)

    # Search operation for different operations
    def search(self, value):
        current_node = self.root
        while(current_node.check_leaf == False):
            temp2 = current_node.values
            for i in range(len(temp2)):
                if (value == temp2[i]):
                    current_node = current_node.keys[i + 1]
                    break
                elif (value < temp2[i]):
                    current_node = current_node.keys[i]
                    break
                elif (i + 1 == len(current_node.values)):
                    current_node = current_node.keys[i + 1]
                    break
        return current_node

    # Find the node
    def find(self, value, key):
        l = self.search(value)
        for i, item in enumerate(l.values):
            if item == value:
                if key in l.keys[i]:
                    return True
                else:
                    return False
        return False

    # Inserting at the parent
    def insert_in_parent(self, n, value, ndash):
        if (self.root == n):
            rootNode = Node(n.order)
            rootNode.values = [value]
            rootNode.keys = [n, ndash]
            self.root = rootNode
            n.parent = rootNode
            ndash.parent = rootNode
            return

        parentNode = n.parent
        temp3 = parentNode.keys
        for i in range(len(temp3)):
            if (temp3[i] == n):
                parentNode.values = parentNode.values[:i] + \
                    [value] + parentNode.values[i:]
                parentNode.keys = parentNode.keys[:i +
                                                  1] + [ndash] + parentNode.keys[i + 1:]
                if (len(parentNode.keys) > parentNode.order):
                    parentdash = Node(parentNode.order)
                    parentdash.parent = parentNode.parent
                    mid = int(math.ceil(parentNode.order / 2)) - 1
                    parentdash.values = parentNode.values[mid + 1:]
                    parentdash.keys = parentNode.keys[mid + 1:]
                    value_ = parentNode.values[mid]
                    if (mid == 0):
                        parentNode.values = parentNode.values[:mid + 1]
                    else:
                        parentNode.values = parentNode.values[:mid]
                    parentNode.keys = parentNode.keys[:mid + 1]
                    for j in parentNode.keys:
                        j.parent = parentNode
                    for j in parentdash.keys:
                        j.parent = parentdash
                    self.insert_in_parent(parentNode, value_, parentdash)

    # Delete a node
    def delete(self, value, key):
        node_ = self.search(value)

        temp = 0
        for i, item in enumerate(node_.values):
            if item == value:
                temp = 1

                if key in node_.keys[i]:
                    if len(node_.keys[i]) > 1:
                        node_.keys[i].pop(node_.keys[i].index(key))
                    elif node_ == self.root:
                        node_.values.pop(i)
                        node_.keys.pop(i)
                    else:
                        node_.keys[i].pop(node_.keys[i].index(key))
                        del node_.keys[i]
                        node_.values.pop(node_.values.index(value))
                        self.deleteEntry(node_, value, key)
                else:
                    print(""Value not in Key"")
                    return
        if temp == 0:
            print(""Value not in Tree"")
            return

    # Delete an entry
    def deleteEntry(self, node_, value, key):

        if not node_.check_leaf:
            for i, item in enumerate(node_.keys):
                if item == key:
                    node_.keys.pop(i)
                    break
            for i, item in enumerate(node_.values):
                if item == value:
                    node_.values.pop(i)
                    break

        if self.root == node_ and len(node_.keys) == 1:
            self.root = node_.keys[0]
            node_.keys[0].parent = None
            del node_
            return
        elif (len(node_.keys) < int(math.ceil(node_.order / 2)) and node_.check_leaf == False) or (len(node_.values) < int(math.ceil((node_.order - 1) / 2)) and node_.check_leaf == True):

            is_predecessor = 0
            parentNode = node_.parent
            PrevNode = -1
            NextNode = -1
            PrevK = -1
            PostK = -1
            for i, item in enumerate(parentNode.keys):

                if item == node_:
                    if i > 0:
                        PrevNode = parentNode.keys[i - 1]
                        PrevK = parentNode.values[i - 1]

                    if i < len(parentNode.keys) - 1:
                        NextNode = parentNode.keys[i + 1]
                        PostK = parentNode.values[i]

            if PrevNode == -1:
                ndash = NextNode
                value_ = PostK
            elif NextNode == -1:
                is_predecessor = 1
                ndash = PrevNode
                value_ = PrevK
            else:
                if len(node_.values) + len(NextNode.values) < node_.order:
                    ndash = NextNode
                    value_ = PostK
                else:
                    is_predecessor = 1
                    ndash = PrevNode
                    value_ = PrevK

            if len(node_.values) + len(ndash.values) < node_.order:
                if is_predecessor == 0:
                    node_, ndash = ndash, node_
                ndash.keys += node_.keys
                if not node_.check_leaf:
                    ndash.values.append(value_)
                else:
                    ndash.nextKey = node_.nextKey
                ndash.values += node_.values

                if not ndash.check_leaf:
                    for j in ndash.keys:
                        j.parent = ndash

                self.deleteEntry(node_.parent, value_, node_)
                del node_
            else:
                if is_predecessor == 1:
                    if not node_.check_leaf:
                        ndashpm = ndash.keys.pop(-1)
                        ndashkm_1 = ndash.values.pop(-1)
                        node_.keys = [ndashpm] + node_.keys
                        node_.values = [value_] + node_.values
                        parentNode = node_.parent
                        for i, item in enumerate(parentNode.values):
                            if item == value_:
                                p.values[i] = ndashkm_1
                                break
                    else:
                        ndashpm = ndash.keys.pop(-1)
                        ndashkm = ndash.values.pop(-1)
                        node_.keys = [ndashpm] + node_.keys
                        node_.values = [ndashkm] + node_.values
                        parentNode = node_.parent
                        for i, item in enumerate(p.values):
                            if item == value_:
                                parentNode.values[i] = ndashkm
                                break
                else:
                    if not node_.check_leaf:
                        ndashp0 = ndash.keys.pop(0)
                        ndashk0 = ndash.values.pop(0)
                        node_.keys = node_.keys + [ndashp0]
                        node_.values = node_.values + [value_]
                        parentNode = node_.parent
                        for i, item in enumerate(parentNode.values):
                            if item == value_:
                                parentNode.values[i] = ndashk0
                                break
                    else:
                        ndashp0 = ndash.keys.pop(0)
                        ndashk0 = ndash.values.pop(0)
                        node_.keys = node_.keys + [ndashp0]
                        node_.values = node_.values + [ndashk0]
                        parentNode = node_.parent
                        for i, item in enumerate(parentNode.values):
                            if item == value_:
                                parentNode.values[i] = ndash.values[0]
                                break

                if not ndash.check_leaf:
                    for j in ndash.keys:
                        j.parent = ndash
                if not node_.check_leaf:
                    for j in node_.keys:
                        j.parent = node_
                if not parentNode.check_leaf:
                    for j in parentNode.keys:
                        j.parent = parentNode


# Print the tree
def printTree(tree):
    lst = [tree.root]
    level = [0]
    leaf = None
    flag = 0
    lev_leaf = 0

    node1 = Node(str(level[0]) + str(tree.root.values))

    while (len(lst) != 0):
        x = lst.pop(0)
        lev = level.pop(0)
        if (x.check_leaf == False):
            for i, item in enumerate(x.keys):
                print(item.values)
        else:
            for i, item in enumerate(x.keys):
                print(item.values)
            if (flag == 0):
                lev_leaf = lev
                leaf = x
                flag = 1


record_len = 3
bplustree = BplusTree(record_len)
bplustree.insert('5', '33')
bplustree.insert('15', '21')
bplustree.insert('25', '31')
bplustree.insert('35', '41')
bplustree.insert('45', '10')

printTree(bplustree)

if(bplustree.find('5', '34')):
    print(""Found"")
else:
    print(""Not found"")"
B+ Tree,"A B+ tree is an advanced form of a self-balancing tree in which all the values are present in the leaf level. An important concept to be understood before learning B+ tree is multilevel indexing. In multilevel indexing, the index of indices is created as in figure below. It makes accessing the data easier and faster. All leaves are at the same level. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys.  The data pointers are present only at the leaf nodes on a B+ tree whereas the data pointers are present in the internal, leaf or root nodes on a B-tree. The leaves are not connected with each other on a B-tree whereas they are connected on a B+ tree. Operations on a B+ tree are faster than on a B-tree.  The following steps are followed to search for data in a B+ Tree of order m. Let the data to be searched be k. Start from the root node. Compare k with the keys at the root node [k1, k2, k3,......km - 1]. If k < k1, go to the left child of the root node. Else if k == k1, compare k2. If k < k2, k lies between k1 and k2. So, search in the left child of k2. If k > k2, go for k3, k4,...km-1 as in steps 2 and 3. Repeat the above steps until a leaf node is reached. If k exists in the leaf node, return true else return false. Let us search k = 45 on the following B+ tree. Compare k with the root node.
		
			k is not found at the root Since k > 25, go to the right child.
		
			Go to right of the root Compare k with 35. Since k > 30, compare k with 45.
		
			k not found Since k ≥ 45, so go to the right child.
		
			go to the right k is found.
		
			k is found If linear search is implemented inside a node, then total complexity is Θ(logt n). If binary search is used, then total complexity is Θ(log2t.logt n). Multilevel Indexing
	Faster operations on the tree (insertion, deletion, search)
	Database indexing Multilevel Indexing Faster operations on the tree (insertion, deletion, search) Database indexing","// Searching on a B+ tree in Java

import java.util.*;

public class BPlusTree {
  int m;
  InternalNode root;
  LeafNode firstLeaf;

  // Binary search program
  private int binarySearch(DictionaryPair[] dps, int numPairs, int t) {
    Comparator<DictionaryPair> c = new Comparator<DictionaryPair>() {
      @Override
      public int compare(DictionaryPair o1, DictionaryPair o2) {
        Integer a = Integer.valueOf(o1.key);
        Integer b = Integer.valueOf(o2.key);
        return a.compareTo(b);
      }
    };
    return Arrays.binarySearch(dps, 0, numPairs, new DictionaryPair(t, 0), c);
  }

  // Find the leaf node
  private LeafNode findLeafNode(int key) {

    Integer[] keys = this.root.keys;
    int i;

    for (i = 0; i < this.root.degree - 1; i++) {
      if (key < keys[i]) {
        break;
      }
    }

    Node child = this.root.childPointers[i];
    if (child instanceof LeafNode) {
      return (LeafNode) child;
    } else {
      return findLeafNode((InternalNode) child, key);
    }
  }

  // Find the leaf node
  private LeafNode findLeafNode(InternalNode node, int key) {

    Integer[] keys = node.keys;
    int i;

    for (i = 0; i < node.degree - 1; i++) {
      if (key < keys[i]) {
        break;
      }
    }
    Node childNode = node.childPointers[i];
    if (childNode instanceof LeafNode) {
      return (LeafNode) childNode;
    } else {
      return findLeafNode((InternalNode) node.childPointers[i], key);
    }
  }

  // Finding the index of the pointer
  private int findIndexOfPointer(Node[] pointers, LeafNode node) {
    int i;
    for (i = 0; i < pointers.length; i++) {
      if (pointers[i] == node) {
        break;
      }
    }
    return i;
  }

  // Get the mid point
  private int getMidpoint() {
    return (int) Math.ceil((this.m + 1) / 2.0) - 1;
  }

  // Balance the tree
  private void handleDeficiency(InternalNode in) {

    InternalNode sibling;
    InternalNode parent = in.parent;

    if (this.root == in) {
      for (int i = 0; i < in.childPointers.length; i++) {
        if (in.childPointers[i] != null) {
          if (in.childPointers[i] instanceof InternalNode) {
            this.root = (InternalNode) in.childPointers[i];
            this.root.parent = null;
          } else if (in.childPointers[i] instanceof LeafNode) {
            this.root = null;
          }
        }
      }
    }

    else if (in.leftSibling != null && in.leftSibling.isLendable()) {
      sibling = in.leftSibling;
    } else if (in.rightSibling != null && in.rightSibling.isLendable()) {
      sibling = in.rightSibling;

      int borrowedKey = sibling.keys[0];
      Node pointer = sibling.childPointers[0];

      in.keys[in.degree - 1] = parent.keys[0];
      in.childPointers[in.degree] = pointer;

      parent.keys[0] = borrowedKey;

      sibling.removePointer(0);
      Arrays.sort(sibling.keys);
      sibling.removePointer(0);
      shiftDown(in.childPointers, 1);
    } else if (in.leftSibling != null && in.leftSibling.isMergeable()) {

    } else if (in.rightSibling != null && in.rightSibling.isMergeable()) {
      sibling = in.rightSibling;
      sibling.keys[sibling.degree - 1] = parent.keys[parent.degree - 2];
      Arrays.sort(sibling.keys, 0, sibling.degree);
      parent.keys[parent.degree - 2] = null;

      for (int i = 0; i < in.childPointers.length; i++) {
        if (in.childPointers[i] != null) {
          sibling.prependChildPointer(in.childPointers[i]);
          in.childPointers[i].parent = sibling;
          in.removePointer(i);
        }
      }

      parent.removePointer(in);

      sibling.leftSibling = in.leftSibling;
    }

    if (parent != null && parent.isDeficient()) {
      handleDeficiency(parent);
    }
  }

  private boolean isEmpty() {
    return firstLeaf == null;
  }

  private int linearNullSearch(DictionaryPair[] dps) {
    for (int i = 0; i < dps.length; i++) {
      if (dps[i] == null) {
        return i;
      }
    }
    return -1;
  }

  private int linearNullSearch(Node[] pointers) {
    for (int i = 0; i < pointers.length; i++) {
      if (pointers[i] == null) {
        return i;
      }
    }
    return -1;
  }

  private void shiftDown(Node[] pointers, int amount) {
    Node[] newPointers = new Node[this.m + 1];
    for (int i = amount; i < pointers.length; i++) {
      newPointers[i - amount] = pointers[i];
    }
    pointers = newPointers;
  }

  private void sortDictionary(DictionaryPair[] dictionary) {
    Arrays.sort(dictionary, new Comparator<DictionaryPair>() {
      @Override
      public int compare(DictionaryPair o1, DictionaryPair o2) {
        if (o1 == null && o2 == null) {
          return 0;
        }
        if (o1 == null) {
          return 1;
        }
        if (o2 == null) {
          return -1;
        }
        return o1.compareTo(o2);
      }
    });
  }

  private Node[] splitChildPointers(InternalNode in, int split) {

    Node[] pointers = in.childPointers;
    Node[] halfPointers = new Node[this.m + 1];

    for (int i = split + 1; i < pointers.length; i++) {
      halfPointers[i - split - 1] = pointers[i];
      in.removePointer(i);
    }

    return halfPointers;
  }

  private DictionaryPair[] splitDictionary(LeafNode ln, int split) {

    DictionaryPair[] dictionary = ln.dictionary;

    DictionaryPair[] halfDict = new DictionaryPair[this.m];

    for (int i = split; i < dictionary.length; i++) {
      halfDict[i - split] = dictionary[i];
      ln.delete(i);
    }

    return halfDict;
  }

  private void splitInternalNode(InternalNode in) {

    InternalNode parent = in.parent;

    int midpoint = getMidpoint();
    int newParentKey = in.keys[midpoint];
    Integer[] halfKeys = splitKeys(in.keys, midpoint);
    Node[] halfPointers = splitChildPointers(in, midpoint);

    in.degree = linearNullSearch(in.childPointers);

    InternalNode sibling = new InternalNode(this.m, halfKeys, halfPointers);
    for (Node pointer : halfPointers) {
      if (pointer != null) {
        pointer.parent = sibling;
      }
    }

    sibling.rightSibling = in.rightSibling;
    if (sibling.rightSibling != null) {
      sibling.rightSibling.leftSibling = sibling;
    }
    in.rightSibling = sibling;
    sibling.leftSibling = in;

    if (parent == null) {

      Integer[] keys = new Integer[this.m];
      keys[0] = newParentKey;
      InternalNode newRoot = new InternalNode(this.m, keys);
      newRoot.appendChildPointer(in);
      newRoot.appendChildPointer(sibling);
      this.root = newRoot;

      in.parent = newRoot;
      sibling.parent = newRoot;

    } else {

      parent.keys[parent.degree - 1] = newParentKey;
      Arrays.sort(parent.keys, 0, parent.degree);

      int pointerIndex = parent.findIndexOfPointer(in) + 1;
      parent.insertChildPointer(sibling, pointerIndex);
      sibling.parent = parent;
    }
  }

  private Integer[] splitKeys(Integer[] keys, int split) {

    Integer[] halfKeys = new Integer[this.m];

    keys[split] = null;

    for (int i = split + 1; i < keys.length; i++) {
      halfKeys[i - split - 1] = keys[i];
      keys[i] = null;
    }

    return halfKeys;
  }

  public void insert(int key, double value) {
    if (isEmpty()) {

      LeafNode ln = new LeafNode(this.m, new DictionaryPair(key, value));

      this.firstLeaf = ln;

    } else {
      LeafNode ln = (this.root == null) ? this.firstLeaf : findLeafNode(key);

      if (!ln.insert(new DictionaryPair(key, value))) {

        ln.dictionary[ln.numPairs] = new DictionaryPair(key, value);
        ln.numPairs++;
        sortDictionary(ln.dictionary);

        int midpoint = getMidpoint();
        DictionaryPair[] halfDict = splitDictionary(ln, midpoint);

        if (ln.parent == null) {

          Integer[] parent_keys = new Integer[this.m];
          parent_keys[0] = halfDict[0].key;
          InternalNode parent = new InternalNode(this.m, parent_keys);
          ln.parent = parent;
          parent.appendChildPointer(ln);

        } else {
          int newParentKey = halfDict[0].key;
          ln.parent.keys[ln.parent.degree - 1] = newParentKey;
          Arrays.sort(ln.parent.keys, 0, ln.parent.degree);
        }

        LeafNode newLeafNode = new LeafNode(this.m, halfDict, ln.parent);

        int pointerIndex = ln.parent.findIndexOfPointer(ln) + 1;
        ln.parent.insertChildPointer(newLeafNode, pointerIndex);

        newLeafNode.rightSibling = ln.rightSibling;
        if (newLeafNode.rightSibling != null) {
          newLeafNode.rightSibling.leftSibling = newLeafNode;
        }
        ln.rightSibling = newLeafNode;
        newLeafNode.leftSibling = ln;

        if (this.root == null) {

          this.root = ln.parent;

        } else {
          InternalNode in = ln.parent;
          while (in != null) {
            if (in.isOverfull()) {
              splitInternalNode(in);
            } else {
              break;
            }
            in = in.parent;
          }
        }
      }
    }
  }

  public Double search(int key) {

    if (isEmpty()) {
      return null;
    }

    LeafNode ln = (this.root == null) ? this.firstLeaf : findLeafNode(key);

    DictionaryPair[] dps = ln.dictionary;
    int index = binarySearch(dps, ln.numPairs, key);

    if (index < 0) {
      return null;
    } else {
      return dps[index].value;
    }
  }

  public ArrayList<Double> search(int lowerBound, int upperBound) {

    ArrayList<Double> values = new ArrayList<Double>();

    LeafNode currNode = this.firstLeaf;
    while (currNode != null) {

      DictionaryPair dps[] = currNode.dictionary;
      for (DictionaryPair dp : dps) {

        if (dp == null) {
          break;
        }

        if (lowerBound <= dp.key && dp.key <= upperBound) {
          values.add(dp.value);
        }
      }
      currNode = currNode.rightSibling;

    }

    return values;
  }

  public BPlusTree(int m) {
    this.m = m;
    this.root = null;
  }

  public class Node {
    InternalNode parent;
  }

  private class InternalNode extends Node {
    int maxDegree;
    int minDegree;
    int degree;
    InternalNode leftSibling;
    InternalNode rightSibling;
    Integer[] keys;
    Node[] childPointers;

    private void appendChildPointer(Node pointer) {
      this.childPointers[degree] = pointer;
      this.degree++;
    }

    private int findIndexOfPointer(Node pointer) {
      for (int i = 0; i < childPointers.length; i++) {
        if (childPointers[i] == pointer) {
          return i;
        }
      }
      return -1;
    }

    private void insertChildPointer(Node pointer, int index) {
      for (int i = degree - 1; i >= index; i--) {
        childPointers[i + 1] = childPointers[i];
      }
      this.childPointers[index] = pointer;
      this.degree++;
    }

    private boolean isDeficient() {
      return this.degree < this.minDegree;
    }

    private boolean isLendable() {
      return this.degree > this.minDegree;
    }

    private boolean isMergeable() {
      return this.degree == this.minDegree;
    }

    private boolean isOverfull() {
      return this.degree == maxDegree + 1;
    }

    private void prependChildPointer(Node pointer) {
      for (int i = degree - 1; i >= 0; i--) {
        childPointers[i + 1] = childPointers[i];
      }
      this.childPointers[0] = pointer;
      this.degree++;
    }

    private void removeKey(int index) {
      this.keys[index] = null;
    }

    private void removePointer(int index) {
      this.childPointers[index] = null;
      this.degree--;
    }

    private void removePointer(Node pointer) {
      for (int i = 0; i < childPointers.length; i++) {
        if (childPointers[i] == pointer) {
          this.childPointers[i] = null;
        }
      }
      this.degree--;
    }

    private InternalNode(int m, Integer[] keys) {
      this.maxDegree = m;
      this.minDegree = (int) Math.ceil(m / 2.0);
      this.degree = 0;
      this.keys = keys;
      this.childPointers = new Node[this.maxDegree + 1];
    }

    private InternalNode(int m, Integer[] keys, Node[] pointers) {
      this.maxDegree = m;
      this.minDegree = (int) Math.ceil(m / 2.0);
      this.degree = linearNullSearch(pointers);
      this.keys = keys;
      this.childPointers = pointers;
    }
  }

  public class LeafNode extends Node {
    int maxNumPairs;
    int minNumPairs;
    int numPairs;
    LeafNode leftSibling;
    LeafNode rightSibling;
    DictionaryPair[] dictionary;

    public void delete(int index) {
      this.dictionary[index] = null;
      numPairs--;
    }

    public boolean insert(DictionaryPair dp) {
      if (this.isFull()) {
        return false;
      } else {
        this.dictionary[numPairs] = dp;
        numPairs++;
        Arrays.sort(this.dictionary, 0, numPairs);

        return true;
      }
    }

    public boolean isDeficient() {
      return numPairs < minNumPairs;
    }

    public boolean isFull() {
      return numPairs == maxNumPairs;
    }

    public boolean isLendable() {
      return numPairs > minNumPairs;
    }

    public boolean isMergeable() {
      return numPairs == minNumPairs;
    }

    public LeafNode(int m, DictionaryPair dp) {
      this.maxNumPairs = m - 1;
      this.minNumPairs = (int) (Math.ceil(m / 2) - 1);
      this.dictionary = new DictionaryPair[m];
      this.numPairs = 0;
      this.insert(dp);
    }

    public LeafNode(int m, DictionaryPair[] dps, InternalNode parent) {
      this.maxNumPairs = m - 1;
      this.minNumPairs = (int) (Math.ceil(m / 2) - 1);
      this.dictionary = dps;
      this.numPairs = linearNullSearch(dps);
      this.parent = parent;
    }
  }

  public class DictionaryPair implements Comparable<DictionaryPair> {
    int key;
    double value;

    public DictionaryPair(int key, double value) {
      this.key = key;
      this.value = value;
    }

    public int compareTo(DictionaryPair o) {
      if (key == o.key) {
        return 0;
      } else if (key > o.key) {
        return 1;
      } else {
        return -1;
      }
    }
  }

  public static void main(String[] args) {
    BPlusTree bpt = null;
    bpt = new BPlusTree(3);
    bpt.insert(5, 33);
    bpt.insert(15, 21);
    bpt.insert(25, 31);
    bpt.insert(35, 41);
    bpt.insert(45, 10);

    if (bpt.search(15) != null) {
      System.out.println(""Found"");
    } else {
      System.out.println(""Not Found"");
    }
    ;
  }
}"
B+ Tree,"A B+ tree is an advanced form of a self-balancing tree in which all the values are present in the leaf level. An important concept to be understood before learning B+ tree is multilevel indexing. In multilevel indexing, the index of indices is created as in figure below. It makes accessing the data easier and faster. All leaves are at the same level. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys.  The data pointers are present only at the leaf nodes on a B+ tree whereas the data pointers are present in the internal, leaf or root nodes on a B-tree. The leaves are not connected with each other on a B-tree whereas they are connected on a B+ tree. Operations on a B+ tree are faster than on a B-tree.  The following steps are followed to search for data in a B+ Tree of order m. Let the data to be searched be k. Start from the root node. Compare k with the keys at the root node [k1, k2, k3,......km - 1]. If k < k1, go to the left child of the root node. Else if k == k1, compare k2. If k < k2, k lies between k1 and k2. So, search in the left child of k2. If k > k2, go for k3, k4,...km-1 as in steps 2 and 3. Repeat the above steps until a leaf node is reached. If k exists in the leaf node, return true else return false. Let us search k = 45 on the following B+ tree. Compare k with the root node.
		
			k is not found at the root Since k > 25, go to the right child.
		
			Go to right of the root Compare k with 35. Since k > 30, compare k with 45.
		
			k not found Since k ≥ 45, so go to the right child.
		
			go to the right k is found.
		
			k is found If linear search is implemented inside a node, then total complexity is Θ(logt n). If binary search is used, then total complexity is Θ(log2t.logt n). Multilevel Indexing
	Faster operations on the tree (insertion, deletion, search)
	Database indexing Multilevel Indexing Faster operations on the tree (insertion, deletion, search) Database indexing","// Searching on a B+ Tree in C

#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

// Default order
#define ORDER 3

typedef struct record {
  int value;
} record;

// Node
typedef struct node {
  void **pointers;
  int *keys;
  struct node *parent;
  bool is_leaf;
  int num_keys;
  struct node *next;
} node;

int order = ORDER;
node *queue = NULL;
bool verbose_output = false;

// Enqueue
void enqueue(node *new_node);

// Dequeue
node *dequeue(void);
int height(node *const root);
int pathToLeaves(node *const root, node *child);
void printLeaves(node *const root);
void printTree(node *const root);
void findAndPrint(node *const root, int key, bool verbose);
void findAndPrintRange(node *const root, int range1, int range2, bool verbose);
int findRange(node *const root, int key_start, int key_end, bool verbose,
        int returned_keys[], void *returned_pointers[]);
node *findLeaf(node *const root, int key, bool verbose);
record *find(node *root, int key, bool verbose, node **leaf_out);
int cut(int length);

record *makeRecord(int value);
node *makeNode(void);
node *makeLeaf(void);
int getLeftIndex(node *parent, node *left);
node *insertIntoLeaf(node *leaf, int key, record *pointer);
node *insertIntoLeafAfterSplitting(node *root, node *leaf, int key,
                   record *pointer);
node *insertIntoNode(node *root, node *parent,
           int left_index, int key, node *right);
node *insertIntoNodeAfterSplitting(node *root, node *parent,
                   int left_index,
                   int key, node *right);
node *insertIntoParent(node *root, node *left, int key, node *right);
node *insertIntoNewRoot(node *left, int key, node *right);
node *startNewTree(int key, record *pointer);
node *insert(node *root, int key, int value);

// Enqueue
void enqueue(node *new_node) {
  node *c;
  if (queue == NULL) {
    queue = new_node;
    queue->next = NULL;
  } else {
    c = queue;
    while (c->next != NULL) {
      c = c->next;
    }
    c->next = new_node;
    new_node->next = NULL;
  }
}

// Dequeue
node *dequeue(void) {
  node *n = queue;
  queue = queue->next;
  n->next = NULL;
  return n;
}

// Print the leaves
void printLeaves(node *const root) {
  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  int i;
  node *c = root;
  while (!c->is_leaf)
    c = c->pointers[0];
  while (true) {
    for (i = 0; i < c->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", c->pointers[i]);
      printf(""%d "", c->keys[i]);
    }
    if (verbose_output)
      printf(""%p "", c->pointers[order - 1]);
    if (c->pointers[order - 1] != NULL) {
      printf("" | "");
      c = c->pointers[order - 1];
    } else
      break;
  }
  printf(""\n"");
}

// Calculate height
int height(node *const root) {
  int h = 0;
  node *c = root;
  while (!c->is_leaf) {
    c = c->pointers[0];
    h++;
  }
  return h;
}

// Get path to root
int pathToLeaves(node *const root, node *child) {
  int length = 0;
  node *c = child;
  while (c != root) {
    c = c->parent;
    length++;
  }
  return length;
}

// Print the tree
void printTree(node *const root) {
  node *n = NULL;
  int i = 0;
  int rank = 0;
  int new_rank = 0;

  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  queue = NULL;
  enqueue(root);
  while (queue != NULL) {
    n = dequeue();
    if (n->parent != NULL && n == n->parent->pointers[0]) {
      new_rank = pathToLeaves(root, n);
      if (new_rank != rank) {
        rank = new_rank;
        printf(""\n"");
      }
    }
    if (verbose_output)
      printf(""(%p)"", n);
    for (i = 0; i < n->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", n->pointers[i]);
      printf(""%d "", n->keys[i]);
    }
    if (!n->is_leaf)
      for (i = 0; i <= n->num_keys; i++)
        enqueue(n->pointers[i]);
    if (verbose_output) {
      if (n->is_leaf)
        printf(""%p "", n->pointers[order - 1]);
      else
        printf(""%p "", n->pointers[n->num_keys]);
    }
    printf(""| "");
  }
  printf(""\n"");
}

// Find the node and print it
void findAndPrint(node *const root, int key, bool verbose) {
  node *leaf = NULL;
  record *r = find(root, key, verbose, NULL);
  if (r == NULL)
    printf(""Record not found under key %d.\n"", key);
  else
    printf(""Record at %p -- key %d, value %d.\n"",
         r, key, r->value);
}

// Find and print the range
void findAndPrintRange(node *const root, int key_start, int key_end,
             bool verbose) {
  int i;
  int array_size = key_end - key_start + 1;
  int returned_keys[array_size];
  void *returned_pointers[array_size];
  int num_found = findRange(root, key_start, key_end, verbose,
                returned_keys, returned_pointers);
  if (!num_found)
    printf(""None found.\n"");
  else {
    for (i = 0; i < num_found; i++)
      printf(""Key: %d   Location: %p  Value: %d\n"",
           returned_keys[i],
           returned_pointers[i],
           ((record *)
            returned_pointers[i])
             ->value);
  }
}

// Find the range
int findRange(node *const root, int key_start, int key_end, bool verbose,
        int returned_keys[], void *returned_pointers[]) {
  int i, num_found;
  num_found = 0;
  node *n = findLeaf(root, key_start, verbose);
  if (n == NULL)
    return 0;
  for (i = 0; i < n->num_keys && n->keys[i] < key_start; i++)
    ;
  if (i == n->num_keys)
    return 0;
  while (n != NULL) {
    for (; i < n->num_keys && n->keys[i] <= key_end; i++) {
      returned_keys[num_found] = n->keys[i];
      returned_pointers[num_found] = n->pointers[i];
      num_found++;
    }
    n = n->pointers[order - 1];
    i = 0;
  }
  return num_found;
}

// Find the leaf
node *findLeaf(node *const root, int key, bool verbose) {
  if (root == NULL) {
    if (verbose)
      printf(""Empty tree.\n"");
    return root;
  }
  int i = 0;
  node *c = root;
  while (!c->is_leaf) {
    if (verbose) {
      printf(""["");
      for (i = 0; i < c->num_keys - 1; i++)
        printf(""%d "", c->keys[i]);
      printf(""%d] "", c->keys[i]);
    }
    i = 0;
    while (i < c->num_keys) {
      if (key >= c->keys[i])
        i++;
      else
        break;
    }
    if (verbose)
      printf(""%d ->\n"", i);
    c = (node *)c->pointers[i];
  }
  if (verbose) {
    printf(""Leaf ["");
    for (i = 0; i < c->num_keys - 1; i++)
      printf(""%d "", c->keys[i]);
    printf(""%d] ->\n"", c->keys[i]);
  }
  return c;
}

record *find(node *root, int key, bool verbose, node **leaf_out) {
  if (root == NULL) {
    if (leaf_out != NULL) {
      *leaf_out = NULL;
    }
    return NULL;
  }

  int i = 0;
  node *leaf = NULL;

  leaf = findLeaf(root, key, verbose);

  for (i = 0; i < leaf->num_keys; i++)
    if (leaf->keys[i] == key)
      break;
  if (leaf_out != NULL) {
    *leaf_out = leaf;
  }
  if (i == leaf->num_keys)
    return NULL;
  else
    return (record *)leaf->pointers[i];
}

int cut(int length) {
  if (length % 2 == 0)
    return length / 2;
  else
    return length / 2 + 1;
}

record *makeRecord(int value) {
  record *new_record = (record *)malloc(sizeof(record));
  if (new_record == NULL) {
    perror(""Record creation."");
    exit(EXIT_FAILURE);
  } else {
    new_record->value = value;
  }
  return new_record;
}

node *makeNode(void) {
  node *new_node;
  new_node = malloc(sizeof(node));
  if (new_node == NULL) {
    perror(""Node creation."");
    exit(EXIT_FAILURE);
  }
  new_node->keys = malloc((order - 1) * sizeof(int));
  if (new_node->keys == NULL) {
    perror(""New node keys array."");
    exit(EXIT_FAILURE);
  }
  new_node->pointers = malloc(order * sizeof(void *));
  if (new_node->pointers == NULL) {
    perror(""New node pointers array."");
    exit(EXIT_FAILURE);
  }
  new_node->is_leaf = false;
  new_node->num_keys = 0;
  new_node->parent = NULL;
  new_node->next = NULL;
  return new_node;
}

node *makeLeaf(void) {
  node *leaf = makeNode();
  leaf->is_leaf = true;
  return leaf;
}

int getLeftIndex(node *parent, node *left) {
  int left_index = 0;
  while (left_index <= parent->num_keys &&
       parent->pointers[left_index] != left)
    left_index++;
  return left_index;
}

node *insertIntoLeaf(node *leaf, int key, record *pointer) {
  int i, insertion_point;

  insertion_point = 0;
  while (insertion_point < leaf->num_keys && leaf->keys[insertion_point] < key)
    insertion_point++;

  for (i = leaf->num_keys; i > insertion_point; i--) {
    leaf->keys[i] = leaf->keys[i - 1];
    leaf->pointers[i] = leaf->pointers[i - 1];
  }
  leaf->keys[insertion_point] = key;
  leaf->pointers[insertion_point] = pointer;
  leaf->num_keys++;
  return leaf;
}

node *insertIntoLeafAfterSplitting(node *root, node *leaf, int key, record *pointer) {
  node *new_leaf;
  int *temp_keys;
  void **temp_pointers;
  int insertion_index, split, new_key, i, j;

  new_leaf = makeLeaf();

  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    perror(""Temporary keys array."");
    exit(EXIT_FAILURE);
  }

  temp_pointers = malloc(order * sizeof(void *));
  if (temp_pointers == NULL) {
    perror(""Temporary pointers array."");
    exit(EXIT_FAILURE);
  }

  insertion_index = 0;
  while (insertion_index < order - 1 && leaf->keys[insertion_index] < key)
    insertion_index++;

  for (i = 0, j = 0; i < leaf->num_keys; i++, j++) {
    if (j == insertion_index)
      j++;
    temp_keys[j] = leaf->keys[i];
    temp_pointers[j] = leaf->pointers[i];
  }

  temp_keys[insertion_index] = key;
  temp_pointers[insertion_index] = pointer;

  leaf->num_keys = 0;

  split = cut(order - 1);

  for (i = 0; i < split; i++) {
    leaf->pointers[i] = temp_pointers[i];
    leaf->keys[i] = temp_keys[i];
    leaf->num_keys++;
  }

  for (i = split, j = 0; i < order; i++, j++) {
    new_leaf->pointers[j] = temp_pointers[i];
    new_leaf->keys[j] = temp_keys[i];
    new_leaf->num_keys++;
  }

  free(temp_pointers);
  free(temp_keys);

  new_leaf->pointers[order - 1] = leaf->pointers[order - 1];
  leaf->pointers[order - 1] = new_leaf;

  for (i = leaf->num_keys; i < order - 1; i++)
    leaf->pointers[i] = NULL;
  for (i = new_leaf->num_keys; i < order - 1; i++)
    new_leaf->pointers[i] = NULL;

  new_leaf->parent = leaf->parent;
  new_key = new_leaf->keys[0];

  return insertIntoParent(root, leaf, new_key, new_leaf);
}

node *insertIntoNode(node *root, node *n,
           int left_index, int key, node *right) {
  int i;

  for (i = n->num_keys; i > left_index; i--) {
    n->pointers[i + 1] = n->pointers[i];
    n->keys[i] = n->keys[i - 1];
  }
  n->pointers[left_index + 1] = right;
  n->keys[left_index] = key;
  n->num_keys++;
  return root;
}

node *insertIntoNodeAfterSplitting(node *root, node *old_node, int left_index,
                   int key, node *right) {
  int i, j, split, k_prime;
  node *new_node, *child;
  int *temp_keys;
  node **temp_pointers;

  temp_pointers = malloc((order + 1) * sizeof(node *));
  if (temp_pointers == NULL) {
    exit(EXIT_FAILURE);
  }
  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    exit(EXIT_FAILURE);
  }

  for (i = 0, j = 0; i < old_node->num_keys + 1; i++, j++) {
    if (j == left_index + 1)
      j++;
    temp_pointers[j] = old_node->pointers[i];
  }

  for (i = 0, j = 0; i < old_node->num_keys; i++, j++) {
    if (j == left_index)
      j++;
    temp_keys[j] = old_node->keys[i];
  }

  temp_pointers[left_index + 1] = right;
  temp_keys[left_index] = key;

  split = cut(order);
  new_node = makeNode();
  old_node->num_keys = 0;
  for (i = 0; i < split - 1; i++) {
    old_node->pointers[i] = temp_pointers[i];
    old_node->keys[i] = temp_keys[i];
    old_node->num_keys++;
  }
  old_node->pointers[i] = temp_pointers[i];
  k_prime = temp_keys[split - 1];
  for (++i, j = 0; i < order; i++, j++) {
    new_node->pointers[j] = temp_pointers[i];
    new_node->keys[j] = temp_keys[i];
    new_node->num_keys++;
  }
  new_node->pointers[j] = temp_pointers[i];
  free(temp_pointers);
  free(temp_keys);
  new_node->parent = old_node->parent;
  for (i = 0; i <= new_node->num_keys; i++) {
    child = new_node->pointers[i];
    child->parent = new_node;
  }

  return insertIntoParent(root, old_node, k_prime, new_node);
}

node *insertIntoParent(node *root, node *left, int key, node *right) {
  int left_index;
  node *parent;

  parent = left->parent;

  if (parent == NULL)
    return insertIntoNewRoot(left, key, right);

  left_index = getLeftIndex(parent, left);

  if (parent->num_keys < order - 1)
    return insertIntoNode(root, parent, left_index, key, right);

  return insertIntoNodeAfterSplitting(root, parent, left_index, key, right);
}

node *insertIntoNewRoot(node *left, int key, node *right) {
  node *root = makeNode();
  root->keys[0] = key;
  root->pointers[0] = left;
  root->pointers[1] = right;
  root->num_keys++;
  root->parent = NULL;
  left->parent = root;
  right->parent = root;
  return root;
}

node *startNewTree(int key, record *pointer) {
  node *root = makeLeaf();
  root->keys[0] = key;
  root->pointers[0] = pointer;
  root->pointers[order - 1] = NULL;
  root->parent = NULL;
  root->num_keys++;
  return root;
}

node *insert(node *root, int key, int value) {
  record *record_pointer = NULL;
  node *leaf = NULL;

  record_pointer = find(root, key, false, NULL);
  if (record_pointer != NULL) {
    record_pointer->value = value;
    return root;
  }

  record_pointer = makeRecord(value);

  if (root == NULL)
    return startNewTree(key, record_pointer);

  leaf = findLeaf(root, key, false);

  if (leaf->num_keys < order - 1) {
    leaf = insertIntoLeaf(leaf, key, record_pointer);
    return root;
  }

  return insertIntoLeafAfterSplitting(root, leaf, key, record_pointer);
}

int main() {
  node *root;
  char instruction;

  root = NULL;

  root = insert(root, 5, 33);
  root = insert(root, 15, 21);
  root = insert(root, 25, 31);
  root = insert(root, 35, 41);
  root = insert(root, 45, 10);

  printTree(root);

  findAndPrint(root, 15, instruction = 'a');
}"
B+ Tree,"A B+ tree is an advanced form of a self-balancing tree in which all the values are present in the leaf level. An important concept to be understood before learning B+ tree is multilevel indexing. In multilevel indexing, the index of indices is created as in figure below. It makes accessing the data easier and faster. All leaves are at the same level. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys.  The data pointers are present only at the leaf nodes on a B+ tree whereas the data pointers are present in the internal, leaf or root nodes on a B-tree. The leaves are not connected with each other on a B-tree whereas they are connected on a B+ tree. Operations on a B+ tree are faster than on a B-tree.  The following steps are followed to search for data in a B+ Tree of order m. Let the data to be searched be k. Start from the root node. Compare k with the keys at the root node [k1, k2, k3,......km - 1]. If k < k1, go to the left child of the root node. Else if k == k1, compare k2. If k < k2, k lies between k1 and k2. So, search in the left child of k2. If k > k2, go for k3, k4,...km-1 as in steps 2 and 3. Repeat the above steps until a leaf node is reached. If k exists in the leaf node, return true else return false. Let us search k = 45 on the following B+ tree. Compare k with the root node.
		
			k is not found at the root Since k > 25, go to the right child.
		
			Go to right of the root Compare k with 35. Since k > 30, compare k with 45.
		
			k not found Since k ≥ 45, so go to the right child.
		
			go to the right k is found.
		
			k is found If linear search is implemented inside a node, then total complexity is Θ(logt n). If binary search is used, then total complexity is Θ(log2t.logt n). Multilevel Indexing
	Faster operations on the tree (insertion, deletion, search)
	Database indexing Multilevel Indexing Faster operations on the tree (insertion, deletion, search) Database indexing","// Searching on a B+ tree in C++

#include <climits>
#include <fstream>
#include <iostream>
#include <sstream>
using namespace std;
int MAX = 3;

// BP node
class Node {
  bool IS_LEAF;
  int *key, size;
  Node **ptr;
  friend class BPTree;

   public:
  Node();
};

// BP tree
class BPTree {
  Node *root;
  void insertInternal(int, Node *, Node *);
  Node *findParent(Node *, Node *);

   public:
  BPTree();
  void search(int);
  void insert(int);
  void display(Node *);
  Node *getRoot();
};

Node::Node() {
  key = new int[MAX];
  ptr = new Node *[MAX + 1];
}

BPTree::BPTree() {
  root = NULL;
}

// Search operation
void BPTree::search(int x) {
  if (root == NULL) {
    cout << ""Tree is empty\n"";
  } else {
    Node *cursor = root;
    while (cursor->IS_LEAF == false) {
      for (int i = 0; i < cursor->size; i++) {
        if (x < cursor->key[i]) {
          cursor = cursor->ptr[i];
          break;
        }
        if (i == cursor->size - 1) {
          cursor = cursor->ptr[i + 1];
          break;
        }
      }
    }
    for (int i = 0; i < cursor->size; i++) {
      if (cursor->key[i] == x) {
        cout << ""Found\n"";
        return;
      }
    }
    cout << ""Not found\n"";
  }
}

// Insert Operation
void BPTree::insert(int x) {
  if (root == NULL) {
    root = new Node;
    root->key[0] = x;
    root->IS_LEAF = true;
    root->size = 1;
  } else {
    Node *cursor = root;
    Node *parent;
    while (cursor->IS_LEAF == false) {
      parent = cursor;
      for (int i = 0; i < cursor->size; i++) {
        if (x < cursor->key[i]) {
          cursor = cursor->ptr[i];
          break;
        }
        if (i == cursor->size - 1) {
          cursor = cursor->ptr[i + 1];
          break;
        }
      }
    }
    if (cursor->size < MAX) {
      int i = 0;
      while (x > cursor->key[i] && i < cursor->size)
        i++;
      for (int j = cursor->size; j > i; j--) {
        cursor->key[j] = cursor->key[j - 1];
      }
      cursor->key[i] = x;
      cursor->size++;
      cursor->ptr[cursor->size] = cursor->ptr[cursor->size - 1];
      cursor->ptr[cursor->size - 1] = NULL;
    } else {
      Node *newLeaf = new Node;
      int virtualNode[MAX + 1];
      for (int i = 0; i < MAX; i++) {
        virtualNode[i] = cursor->key[i];
      }
      int i = 0, j;
      while (x > virtualNode[i] && i < MAX)
        i++;
      for (int j = MAX + 1; j > i; j--) {
        virtualNode[j] = virtualNode[j - 1];
      }
      virtualNode[i] = x;
      newLeaf->IS_LEAF = true;
      cursor->size = (MAX + 1) / 2;
      newLeaf->size = MAX + 1 - (MAX + 1) / 2;
      cursor->ptr[cursor->size] = newLeaf;
      newLeaf->ptr[newLeaf->size] = cursor->ptr[MAX];
      cursor->ptr[MAX] = NULL;
      for (i = 0; i < cursor->size; i++) {
        cursor->key[i] = virtualNode[i];
      }
      for (i = 0, j = cursor->size; i < newLeaf->size; i++, j++) {
        newLeaf->key[i] = virtualNode[j];
      }
      if (cursor == root) {
        Node *newRoot = new Node;
        newRoot->key[0] = newLeaf->key[0];
        newRoot->ptr[0] = cursor;
        newRoot->ptr[1] = newLeaf;
        newRoot->IS_LEAF = false;
        newRoot->size = 1;
        root = newRoot;
      } else {
        insertInternal(newLeaf->key[0], parent, newLeaf);
      }
    }
  }
}

// Insert Operation
void BPTree::insertInternal(int x, Node *cursor, Node *child) {
  if (cursor->size < MAX) {
    int i = 0;
    while (x > cursor->key[i] && i < cursor->size)
      i++;
    for (int j = cursor->size; j > i; j--) {
      cursor->key[j] = cursor->key[j - 1];
    }
    for (int j = cursor->size + 1; j > i + 1; j--) {
      cursor->ptr[j] = cursor->ptr[j - 1];
    }
    cursor->key[i] = x;
    cursor->size++;
    cursor->ptr[i + 1] = child;
  } else {
    Node *newInternal = new Node;
    int virtualKey[MAX + 1];
    Node *virtualPtr[MAX + 2];
    for (int i = 0; i < MAX; i++) {
      virtualKey[i] = cursor->key[i];
    }
    for (int i = 0; i < MAX + 1; i++) {
      virtualPtr[i] = cursor->ptr[i];
    }
    int i = 0, j;
    while (x > virtualKey[i] && i < MAX)
      i++;
    for (int j = MAX + 1; j > i; j--) {
      virtualKey[j] = virtualKey[j - 1];
    }
    virtualKey[i] = x;
    for (int j = MAX + 2; j > i + 1; j--) {
      virtualPtr[j] = virtualPtr[j - 1];
    }
    virtualPtr[i + 1] = child;
    newInternal->IS_LEAF = false;
    cursor->size = (MAX + 1) / 2;
    newInternal->size = MAX - (MAX + 1) / 2;
    for (i = 0, j = cursor->size + 1; i < newInternal->size; i++, j++) {
      newInternal->key[i] = virtualKey[j];
    }
    for (i = 0, j = cursor->size + 1; i < newInternal->size + 1; i++, j++) {
      newInternal->ptr[i] = virtualPtr[j];
    }
    if (cursor == root) {
      Node *newRoot = new Node;
      newRoot->key[0] = cursor->key[cursor->size];
      newRoot->ptr[0] = cursor;
      newRoot->ptr[1] = newInternal;
      newRoot->IS_LEAF = false;
      newRoot->size = 1;
      root = newRoot;
    } else {
      insertInternal(cursor->key[cursor->size], findParent(root, cursor), newInternal);
    }
  }
}

// Find the parent
Node *BPTree::findParent(Node *cursor, Node *child) {
  Node *parent;
  if (cursor->IS_LEAF || (cursor->ptr[0])->IS_LEAF) {
    return NULL;
  }
  for (int i = 0; i < cursor->size + 1; i++) {
    if (cursor->ptr[i] == child) {
      parent = cursor;
      return parent;
    } else {
      parent = findParent(cursor->ptr[i], child);
      if (parent != NULL)
        return parent;
    }
  }
  return parent;
}

// Print the tree
void BPTree::display(Node *cursor) {
  if (cursor != NULL) {
    for (int i = 0; i < cursor->size; i++) {
      cout << cursor->key[i] << "" "";
    }
    cout << ""\n"";
    if (cursor->IS_LEAF != true) {
      for (int i = 0; i < cursor->size + 1; i++) {
        display(cursor->ptr[i]);
      }
    }
  }
}

// Get the root
Node *BPTree::getRoot() {
  return root;
}

int main() {
  BPTree node;
  node.insert(5);
  node.insert(15);
  node.insert(25);
  node.insert(35);
  node.insert(45);
  node.insert(55);
  node.insert(40);
  node.insert(30);
  node.insert(20);
  node.display(node.getRoot());

  node.search(15);
}"
Insertion on a B+ Tree,"Inserting an element into a B+ tree consists of three main events: searching the appropriate leaf, inserting the element and balancing/splitting the tree. Let us understand these events below. Before inserting an element into a B+ tree, these properties must be kept in mind. The root has at least two children.
	Each node except root can have a maximum of m children and at least m/2 children.
	Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The following steps are followed for inserting an element. Since every element is inserted into the leaf node, go to the appropriate leaf node. Insert the key into the leaf node. If the leaf is not full, insert the key into the leaf node in increasing order. If the leaf is full, insert the key into the leaf node in increasing order and balance the tree in the following way. Break the node at m/2th position. Add m/2th key to the parent node as well. If the parent node is already full, follow steps 2 to 3. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 5,15, 25, 35, 45. Insert 5.
		
			Insert 5 Insert 15.
		
			Insert 15 Insert 25.
		
			Insert 25 Insert 35.
		
			Insert 35 Insert 45.
		
			Insert 45 Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n).","# B+ tee in python


import math

# Node creation
class Node:
    def __init__(self, order):
        self.order = order
        self.values = []
        self.keys = []
        self.nextKey = None
        self.parent = None
        self.check_leaf = False

    # Insert at the leaf
    def insert_at_leaf(self, leaf, value, key):
        if (self.values):
            temp1 = self.values
            for i in range(len(temp1)):
                if (value == temp1[i]):
                    self.keys[i].append(key)
                    break
                elif (value < temp1[i]):
                    self.values = self.values[:i] + [value] + self.values[i:]
                    self.keys = self.keys[:i] + [[key]] + self.keys[i:]
                    break
                elif (i + 1 == len(temp1)):
                    self.values.append(value)
                    self.keys.append([key])
                    break
        else:
            self.values = [value]
            self.keys = [[key]]


# B plus tree
class BplusTree:
    def __init__(self, order):
        self.root = Node(order)
        self.root.check_leaf = True

    # Insert operation
    def insert(self, value, key):
        value = str(value)
        old_node = self.search(value)
        old_node.insert_at_leaf(old_node, value, key)

        if (len(old_node.values) == old_node.order):
            node1 = Node(old_node.order)
            node1.check_leaf = True
            node1.parent = old_node.parent
            mid = int(math.ceil(old_node.order / 2)) - 1
            node1.values = old_node.values[mid + 1:]
            node1.keys = old_node.keys[mid + 1:]
            node1.nextKey = old_node.nextKey
            old_node.values = old_node.values[:mid + 1]
            old_node.keys = old_node.keys[:mid + 1]
            old_node.nextKey = node1
            self.insert_in_parent(old_node, node1.values[0], node1)

    # Search operation for different operations
    def search(self, value):
        current_node = self.root
        while(current_node.check_leaf == False):
            temp2 = current_node.values
            for i in range(len(temp2)):
                if (value == temp2[i]):
                    current_node = current_node.keys[i + 1]
                    break
                elif (value < temp2[i]):
                    current_node = current_node.keys[i]
                    break
                elif (i + 1 == len(current_node.values)):
                    current_node = current_node.keys[i + 1]
                    break
        return current_node

    # Find the node
    def find(self, value, key):
        l = self.search(value)
        for i, item in enumerate(l.values):
            if item == value:
                if key in l.keys[i]:
                    return True
                else:
                    return False
        return False

    # Inserting at the parent
    def insert_in_parent(self, n, value, ndash):
        if (self.root == n):
            rootNode = Node(n.order)
            rootNode.values = [value]
            rootNode.keys = [n, ndash]
            self.root = rootNode
            n.parent = rootNode
            ndash.parent = rootNode
            return

        parentNode = n.parent
        temp3 = parentNode.keys
        for i in range(len(temp3)):
            if (temp3[i] == n):
                parentNode.values = parentNode.values[:i] + \
                    [value] + parentNode.values[i:]
                parentNode.keys = parentNode.keys[:i +
                                                  1] + [ndash] + parentNode.keys[i + 1:]
                if (len(parentNode.keys) > parentNode.order):
                    parentdash = Node(parentNode.order)
                    parentdash.parent = parentNode.parent
                    mid = int(math.ceil(parentNode.order / 2)) - 1
                    parentdash.values = parentNode.values[mid + 1:]
                    parentdash.keys = parentNode.keys[mid + 1:]
                    value_ = parentNode.values[mid]
                    if (mid == 0):
                        parentNode.values = parentNode.values[:mid + 1]
                    else:
                        parentNode.values = parentNode.values[:mid]
                    parentNode.keys = parentNode.keys[:mid + 1]
                    for j in parentNode.keys:
                        j.parent = parentNode
                    for j in parentdash.keys:
                        j.parent = parentdash
                    self.insert_in_parent(parentNode, value_, parentdash)

# Print the tree
def printTree(tree):
    lst = [tree.root]
    level = [0]
    leaf = None
    flag = 0
    lev_leaf = 0

    node1 = Node(str(level[0]) + str(tree.root.values))

    while (len(lst) != 0):
        x = lst.pop(0)
        lev = level.pop(0)
        if (x.check_leaf == False):
            for i, item in enumerate(x.keys):
                print(item.values)
        else:
            for i, item in enumerate(x.keys):
                print(item.values)
            if (flag == 0):
                lev_leaf = lev
                leaf = x
                flag = 1


record_len = 3
bplustree = BplusTree(record_len)
bplustree.insert('5', '33')
bplustree.insert('15', '21')
bplustree.insert('25', '31')
bplustree.insert('35', '41')
bplustree.insert('45', '10')

printTree(bplustree)

if(bplustree.find('5', '34')):
    print(""Found"")
else:
    print(""Not found"")
"
Insertion on a B+ Tree,"Inserting an element into a B+ tree consists of three main events: searching the appropriate leaf, inserting the element and balancing/splitting the tree. Let us understand these events below. Before inserting an element into a B+ tree, these properties must be kept in mind. The root has at least two children.
	Each node except root can have a maximum of m children and at least m/2 children.
	Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The following steps are followed for inserting an element. Since every element is inserted into the leaf node, go to the appropriate leaf node. Insert the key into the leaf node. If the leaf is not full, insert the key into the leaf node in increasing order. If the leaf is full, insert the key into the leaf node in increasing order and balance the tree in the following way. Break the node at m/2th position. Add m/2th key to the parent node as well. If the parent node is already full, follow steps 2 to 3. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 5,15, 25, 35, 45. Insert 5.
		
			Insert 5 Insert 15.
		
			Insert 15 Insert 25.
		
			Insert 25 Insert 35.
		
			Insert 35 Insert 45.
		
			Insert 45 Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n).","// Searching on a B+ tree in Java

import java.util.*;

public class BPlusTree {
  int m;
  InternalNode root;
  LeafNode firstLeaf;

  // Binary search program
  private int binarySearch(DictionaryPair[] dps, int numPairs, int t) {
    Comparator<DictionaryPair> c = new Comparator<DictionaryPair>() {
      @Override
      public int compare(DictionaryPair o1, DictionaryPair o2) {
        Integer a = Integer.valueOf(o1.key);
        Integer b = Integer.valueOf(o2.key);
        return a.compareTo(b);
      }
    };
    return Arrays.binarySearch(dps, 0, numPairs, new DictionaryPair(t, 0), c);
  }

  // Find the leaf node
  private LeafNode findLeafNode(int key) {

    Integer[] keys = this.root.keys;
    int i;

    for (i = 0; i < this.root.degree - 1; i++) {
      if (key < keys[i]) {
        break;
      }
    }

    Node child = this.root.childPointers[i];
    if (child instanceof LeafNode) {
      return (LeafNode) child;
    } else {
      return findLeafNode((InternalNode) child, key);
    }
  }

  // Find the leaf node
  private LeafNode findLeafNode(InternalNode node, int key) {

    Integer[] keys = node.keys;
    int i;

    for (i = 0; i < node.degree - 1; i++) {
      if (key < keys[i]) {
        break;
      }
    }
    Node childNode = node.childPointers[i];
    if (childNode instanceof LeafNode) {
      return (LeafNode) childNode;
    } else {
      return findLeafNode((InternalNode) node.childPointers[i], key);
    }
  }

  // Finding the index of the pointer
  private int findIndexOfPointer(Node[] pointers, LeafNode node) {
    int i;
    for (i = 0; i < pointers.length; i++) {
      if (pointers[i] == node) {
        break;
      }
    }
    return i;
  }

  // Get the mid point
  private int getMidpoint() {
    return (int) Math.ceil((this.m + 1) / 2.0) - 1;
  }

  // Balance the tree
  private void handleDeficiency(InternalNode in) {

    InternalNode sibling;
    InternalNode parent = in.parent;

    if (this.root == in) {
      for (int i = 0; i < in.childPointers.length; i++) {
        if (in.childPointers[i] != null) {
          if (in.childPointers[i] instanceof InternalNode) {
            this.root = (InternalNode) in.childPointers[i];
            this.root.parent = null;
          } else if (in.childPointers[i] instanceof LeafNode) {
            this.root = null;
          }
        }
      }
    }

    else if (in.leftSibling != null && in.leftSibling.isLendable()) {
      sibling = in.leftSibling;
    } else if (in.rightSibling != null && in.rightSibling.isLendable()) {
      sibling = in.rightSibling;

      int borrowedKey = sibling.keys[0];
      Node pointer = sibling.childPointers[0];

      in.keys[in.degree - 1] = parent.keys[0];
      in.childPointers[in.degree] = pointer;

      parent.keys[0] = borrowedKey;

      sibling.removePointer(0);
      Arrays.sort(sibling.keys);
      sibling.removePointer(0);
      shiftDown(in.childPointers, 1);
    } else if (in.leftSibling != null && in.leftSibling.isMergeable()) {

    } else if (in.rightSibling != null && in.rightSibling.isMergeable()) {
      sibling = in.rightSibling;
      sibling.keys[sibling.degree - 1] = parent.keys[parent.degree - 2];
      Arrays.sort(sibling.keys, 0, sibling.degree);
      parent.keys[parent.degree - 2] = null;

      for (int i = 0; i < in.childPointers.length; i++) {
        if (in.childPointers[i] != null) {
          sibling.prependChildPointer(in.childPointers[i]);
          in.childPointers[i].parent = sibling;
          in.removePointer(i);
        }
      }

      parent.removePointer(in);

      sibling.leftSibling = in.leftSibling;
    }

    if (parent != null && parent.isDeficient()) {
      handleDeficiency(parent);
    }
  }

  private boolean isEmpty() {
    return firstLeaf == null;
  }

  private int linearNullSearch(DictionaryPair[] dps) {
    for (int i = 0; i < dps.length; i++) {
      if (dps[i] == null) {
        return i;
      }
    }
    return -1;
  }

  private int linearNullSearch(Node[] pointers) {
    for (int i = 0; i < pointers.length; i++) {
      if (pointers[i] == null) {
        return i;
      }
    }
    return -1;
  }

  private void shiftDown(Node[] pointers, int amount) {
    Node[] newPointers = new Node[this.m + 1];
    for (int i = amount; i < pointers.length; i++) {
      newPointers[i - amount] = pointers[i];
    }
    pointers = newPointers;
  }

  private void sortDictionary(DictionaryPair[] dictionary) {
    Arrays.sort(dictionary, new Comparator<DictionaryPair>() {
      @Override
      public int compare(DictionaryPair o1, DictionaryPair o2) {
        if (o1 == null && o2 == null) {
          return 0;
        }
        if (o1 == null) {
          return 1;
        }
        if (o2 == null) {
          return -1;
        }
        return o1.compareTo(o2);
      }
    });
  }

  private Node[] splitChildPointers(InternalNode in, int split) {

    Node[] pointers = in.childPointers;
    Node[] halfPointers = new Node[this.m + 1];

    for (int i = split + 1; i < pointers.length; i++) {
      halfPointers[i - split - 1] = pointers[i];
      in.removePointer(i);
    }

    return halfPointers;
  }

  private DictionaryPair[] splitDictionary(LeafNode ln, int split) {

    DictionaryPair[] dictionary = ln.dictionary;

    DictionaryPair[] halfDict = new DictionaryPair[this.m];

    for (int i = split; i < dictionary.length; i++) {
      halfDict[i - split] = dictionary[i];
      ln.delete(i);
    }

    return halfDict;
  }

  private void splitInternalNode(InternalNode in) {

    InternalNode parent = in.parent;

    int midpoint = getMidpoint();
    int newParentKey = in.keys[midpoint];
    Integer[] halfKeys = splitKeys(in.keys, midpoint);
    Node[] halfPointers = splitChildPointers(in, midpoint);

    in.degree = linearNullSearch(in.childPointers);

    InternalNode sibling = new InternalNode(this.m, halfKeys, halfPointers);
    for (Node pointer : halfPointers) {
      if (pointer != null) {
        pointer.parent = sibling;
      }
    }

    sibling.rightSibling = in.rightSibling;
    if (sibling.rightSibling != null) {
      sibling.rightSibling.leftSibling = sibling;
    }
    in.rightSibling = sibling;
    sibling.leftSibling = in;

    if (parent == null) {

      Integer[] keys = new Integer[this.m];
      keys[0] = newParentKey;
      InternalNode newRoot = new InternalNode(this.m, keys);
      newRoot.appendChildPointer(in);
      newRoot.appendChildPointer(sibling);
      this.root = newRoot;

      in.parent = newRoot;
      sibling.parent = newRoot;

    } else {

      parent.keys[parent.degree - 1] = newParentKey;
      Arrays.sort(parent.keys, 0, parent.degree);

      int pointerIndex = parent.findIndexOfPointer(in) + 1;
      parent.insertChildPointer(sibling, pointerIndex);
      sibling.parent = parent;
    }
  }

  private Integer[] splitKeys(Integer[] keys, int split) {

    Integer[] halfKeys = new Integer[this.m];

    keys[split] = null;

    for (int i = split + 1; i < keys.length; i++) {
      halfKeys[i - split - 1] = keys[i];
      keys[i] = null;
    }

    return halfKeys;
  }

  public void insert(int key, double value) {
    if (isEmpty()) {

      LeafNode ln = new LeafNode(this.m, new DictionaryPair(key, value));

      this.firstLeaf = ln;

    } else {
      LeafNode ln = (this.root == null) ? this.firstLeaf : findLeafNode(key);

      if (!ln.insert(new DictionaryPair(key, value))) {

        ln.dictionary[ln.numPairs] = new DictionaryPair(key, value);
        ln.numPairs++;
        sortDictionary(ln.dictionary);

        int midpoint = getMidpoint();
        DictionaryPair[] halfDict = splitDictionary(ln, midpoint);

        if (ln.parent == null) {

          Integer[] parent_keys = new Integer[this.m];
          parent_keys[0] = halfDict[0].key;
          InternalNode parent = new InternalNode(this.m, parent_keys);
          ln.parent = parent;
          parent.appendChildPointer(ln);

        } else {
          int newParentKey = halfDict[0].key;
          ln.parent.keys[ln.parent.degree - 1] = newParentKey;
          Arrays.sort(ln.parent.keys, 0, ln.parent.degree);
        }

        LeafNode newLeafNode = new LeafNode(this.m, halfDict, ln.parent);

        int pointerIndex = ln.parent.findIndexOfPointer(ln) + 1;
        ln.parent.insertChildPointer(newLeafNode, pointerIndex);

        newLeafNode.rightSibling = ln.rightSibling;
        if (newLeafNode.rightSibling != null) {
          newLeafNode.rightSibling.leftSibling = newLeafNode;
        }
        ln.rightSibling = newLeafNode;
        newLeafNode.leftSibling = ln;

        if (this.root == null) {

          this.root = ln.parent;

        } else {
          InternalNode in = ln.parent;
          while (in != null) {
            if (in.isOverfull()) {
              splitInternalNode(in);
            } else {
              break;
            }
            in = in.parent;
          }
        }
      }
    }
  }

  public Double search(int key) {

    if (isEmpty()) {
      return null;
    }

    LeafNode ln = (this.root == null) ? this.firstLeaf : findLeafNode(key);

    DictionaryPair[] dps = ln.dictionary;
    int index = binarySearch(dps, ln.numPairs, key);

    if (index < 0) {
      return null;
    } else {
      return dps[index].value;
    }
  }

  public ArrayList<Double> search(int lowerBound, int upperBound) {

    ArrayList<Double> values = new ArrayList<Double>();

    LeafNode currNode = this.firstLeaf;
    while (currNode != null) {

      DictionaryPair dps[] = currNode.dictionary;
      for (DictionaryPair dp : dps) {

        if (dp == null) {
          break;
        }

        if (lowerBound <= dp.key && dp.key <= upperBound) {
          values.add(dp.value);
        }
      }
      currNode = currNode.rightSibling;

    }

    return values;
  }

  public BPlusTree(int m) {
    this.m = m;
    this.root = null;
  }

  public class Node {
    InternalNode parent;
  }

  private class InternalNode extends Node {
    int maxDegree;
    int minDegree;
    int degree;
    InternalNode leftSibling;
    InternalNode rightSibling;
    Integer[] keys;
    Node[] childPointers;

    private void appendChildPointer(Node pointer) {
      this.childPointers[degree] = pointer;
      this.degree++;
    }

    private int findIndexOfPointer(Node pointer) {
      for (int i = 0; i < childPointers.length; i++) {
        if (childPointers[i] == pointer) {
          return i;
        }
      }
      return -1;
    }

    private void insertChildPointer(Node pointer, int index) {
      for (int i = degree - 1; i >= index; i--) {
        childPointers[i + 1] = childPointers[i];
      }
      this.childPointers[index] = pointer;
      this.degree++;
    }

    private boolean isDeficient() {
      return this.degree < this.minDegree;
    }

    private boolean isLendable() {
      return this.degree > this.minDegree;
    }

    private boolean isMergeable() {
      return this.degree == this.minDegree;
    }

    private boolean isOverfull() {
      return this.degree == maxDegree + 1;
    }

    private void prependChildPointer(Node pointer) {
      for (int i = degree - 1; i >= 0; i--) {
        childPointers[i + 1] = childPointers[i];
      }
      this.childPointers[0] = pointer;
      this.degree++;
    }

    private void removeKey(int index) {
      this.keys[index] = null;
    }

    private void removePointer(int index) {
      this.childPointers[index] = null;
      this.degree--;
    }

    private void removePointer(Node pointer) {
      for (int i = 0; i < childPointers.length; i++) {
        if (childPointers[i] == pointer) {
          this.childPointers[i] = null;
        }
      }
      this.degree--;
    }

    private InternalNode(int m, Integer[] keys) {
      this.maxDegree = m;
      this.minDegree = (int) Math.ceil(m / 2.0);
      this.degree = 0;
      this.keys = keys;
      this.childPointers = new Node[this.maxDegree + 1];
    }

    private InternalNode(int m, Integer[] keys, Node[] pointers) {
      this.maxDegree = m;
      this.minDegree = (int) Math.ceil(m / 2.0);
      this.degree = linearNullSearch(pointers);
      this.keys = keys;
      this.childPointers = pointers;
    }
  }

  public class LeafNode extends Node {
    int maxNumPairs;
    int minNumPairs;
    int numPairs;
    LeafNode leftSibling;
    LeafNode rightSibling;
    DictionaryPair[] dictionary;

    public void delete(int index) {
      this.dictionary[index] = null;
      numPairs--;
    }

    public boolean insert(DictionaryPair dp) {
      if (this.isFull()) {
        return false;
      } else {
        this.dictionary[numPairs] = dp;
        numPairs++;
        Arrays.sort(this.dictionary, 0, numPairs);

        return true;
      }
    }

    public boolean isDeficient() {
      return numPairs < minNumPairs;
    }

    public boolean isFull() {
      return numPairs == maxNumPairs;
    }

    public boolean isLendable() {
      return numPairs > minNumPairs;
    }

    public boolean isMergeable() {
      return numPairs == minNumPairs;
    }

    public LeafNode(int m, DictionaryPair dp) {
      this.maxNumPairs = m - 1;
      this.minNumPairs = (int) (Math.ceil(m / 2) - 1);
      this.dictionary = new DictionaryPair[m];
      this.numPairs = 0;
      this.insert(dp);
    }

    public LeafNode(int m, DictionaryPair[] dps, InternalNode parent) {
      this.maxNumPairs = m - 1;
      this.minNumPairs = (int) (Math.ceil(m / 2) - 1);
      this.dictionary = dps;
      this.numPairs = linearNullSearch(dps);
      this.parent = parent;
    }
  }

  public class DictionaryPair implements Comparable<DictionaryPair> {
    int key;
    double value;

    public DictionaryPair(int key, double value) {
      this.key = key;
      this.value = value;
    }

    public int compareTo(DictionaryPair o) {
      if (key == o.key) {
        return 0;
      } else if (key > o.key) {
        return 1;
      } else {
        return -1;
      }
    }
  }

  public static void main(String[] args) {
    BPlusTree bpt = null;
    bpt = new BPlusTree(3);
    bpt.insert(5, 33);
    bpt.insert(15, 21);
    bpt.insert(25, 31);
    bpt.insert(35, 41);
    bpt.insert(45, 10);

    if (bpt.search(15) != null) {
      System.out.println(""Found"");
    } else {
      System.out.println(""Not Found"");
    }
    ;
  }
}"
Insertion on a B+ Tree,"Inserting an element into a B+ tree consists of three main events: searching the appropriate leaf, inserting the element and balancing/splitting the tree. Let us understand these events below. Before inserting an element into a B+ tree, these properties must be kept in mind. The root has at least two children.
	Each node except root can have a maximum of m children and at least m/2 children.
	Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The following steps are followed for inserting an element. Since every element is inserted into the leaf node, go to the appropriate leaf node. Insert the key into the leaf node. If the leaf is not full, insert the key into the leaf node in increasing order. If the leaf is full, insert the key into the leaf node in increasing order and balance the tree in the following way. Break the node at m/2th position. Add m/2th key to the parent node as well. If the parent node is already full, follow steps 2 to 3. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 5,15, 25, 35, 45. Insert 5.
		
			Insert 5 Insert 15.
		
			Insert 15 Insert 25.
		
			Insert 25 Insert 35.
		
			Insert 35 Insert 45.
		
			Insert 45 Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n).","// Searching on a B+ Tree in C

#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

// Default order
#define ORDER 3

typedef struct record {
  int value;
} record;

// Node
typedef struct node {
  void **pointers;
  int *keys;
  struct node *parent;
  bool is_leaf;
  int num_keys;
  struct node *next;
} node;

int order = ORDER;
node *queue = NULL;
bool verbose_output = false;

// Enqueue
void enqueue(node *new_node);

// Dequeue
node *dequeue(void);
int height(node *const root);
int pathToLeaves(node *const root, node *child);
void printLeaves(node *const root);
void printTree(node *const root);
void findAndPrint(node *const root, int key, bool verbose);
void findAndPrintRange(node *const root, int range1, int range2, bool verbose);
int findRange(node *const root, int key_start, int key_end, bool verbose,
        int returned_keys[], void *returned_pointers[]);
node *findLeaf(node *const root, int key, bool verbose);
record *find(node *root, int key, bool verbose, node **leaf_out);
int cut(int length);

record *makeRecord(int value);
node *makeNode(void);
node *makeLeaf(void);
int getLeftIndex(node *parent, node *left);
node *insertIntoLeaf(node *leaf, int key, record *pointer);
node *insertIntoLeafAfterSplitting(node *root, node *leaf, int key,
                   record *pointer);
node *insertIntoNode(node *root, node *parent,
           int left_index, int key, node *right);
node *insertIntoNodeAfterSplitting(node *root, node *parent,
                   int left_index,
                   int key, node *right);
node *insertIntoParent(node *root, node *left, int key, node *right);
node *insertIntoNewRoot(node *left, int key, node *right);
node *startNewTree(int key, record *pointer);
node *insert(node *root, int key, int value);

// Enqueue
void enqueue(node *new_node) {
  node *c;
  if (queue == NULL) {
    queue = new_node;
    queue->next = NULL;
  } else {
    c = queue;
    while (c->next != NULL) {
      c = c->next;
    }
    c->next = new_node;
    new_node->next = NULL;
  }
}

// Dequeue
node *dequeue(void) {
  node *n = queue;
  queue = queue->next;
  n->next = NULL;
  return n;
}

// Print the leaves
void printLeaves(node *const root) {
  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  int i;
  node *c = root;
  while (!c->is_leaf)
    c = c->pointers[0];
  while (true) {
    for (i = 0; i < c->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", c->pointers[i]);
      printf(""%d "", c->keys[i]);
    }
    if (verbose_output)
      printf(""%p "", c->pointers[order - 1]);
    if (c->pointers[order - 1] != NULL) {
      printf("" | "");
      c = c->pointers[order - 1];
    } else
      break;
  }
  printf(""\n"");
}

// Calculate height
int height(node *const root) {
  int h = 0;
  node *c = root;
  while (!c->is_leaf) {
    c = c->pointers[0];
    h++;
  }
  return h;
}

// Get path to root
int pathToLeaves(node *const root, node *child) {
  int length = 0;
  node *c = child;
  while (c != root) {
    c = c->parent;
    length++;
  }
  return length;
}

// Print the tree
void printTree(node *const root) {
  node *n = NULL;
  int i = 0;
  int rank = 0;
  int new_rank = 0;

  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  queue = NULL;
  enqueue(root);
  while (queue != NULL) {
    n = dequeue();
    if (n->parent != NULL && n == n->parent->pointers[0]) {
      new_rank = pathToLeaves(root, n);
      if (new_rank != rank) {
        rank = new_rank;
        printf(""\n"");
      }
    }
    if (verbose_output)
      printf(""(%p)"", n);
    for (i = 0; i < n->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", n->pointers[i]);
      printf(""%d "", n->keys[i]);
    }
    if (!n->is_leaf)
      for (i = 0; i <= n->num_keys; i++)
        enqueue(n->pointers[i]);
    if (verbose_output) {
      if (n->is_leaf)
        printf(""%p "", n->pointers[order - 1]);
      else
        printf(""%p "", n->pointers[n->num_keys]);
    }
    printf(""| "");
  }
  printf(""\n"");
}

// Find the node and print it
void findAndPrint(node *const root, int key, bool verbose) {
  node *leaf = NULL;
  record *r = find(root, key, verbose, NULL);
  if (r == NULL)
    printf(""Record not found under key %d.\n"", key);
  else
    printf(""Record at %p -- key %d, value %d.\n"",
         r, key, r->value);
}

// Find and print the range
void findAndPrintRange(node *const root, int key_start, int key_end,
             bool verbose) {
  int i;
  int array_size = key_end - key_start + 1;
  int returned_keys[array_size];
  void *returned_pointers[array_size];
  int num_found = findRange(root, key_start, key_end, verbose,
                returned_keys, returned_pointers);
  if (!num_found)
    printf(""None found.\n"");
  else {
    for (i = 0; i < num_found; i++)
      printf(""Key: %d   Location: %p  Value: %d\n"",
           returned_keys[i],
           returned_pointers[i],
           ((record *)
            returned_pointers[i])
             ->value);
  }
}

// Find the range
int findRange(node *const root, int key_start, int key_end, bool verbose,
        int returned_keys[], void *returned_pointers[]) {
  int i, num_found;
  num_found = 0;
  node *n = findLeaf(root, key_start, verbose);
  if (n == NULL)
    return 0;
  for (i = 0; i < n->num_keys && n->keys[i] < key_start; i++)
    ;
  if (i == n->num_keys)
    return 0;
  while (n != NULL) {
    for (; i < n->num_keys && n->keys[i] <= key_end; i++) {
      returned_keys[num_found] = n->keys[i];
      returned_pointers[num_found] = n->pointers[i];
      num_found++;
    }
    n = n->pointers[order - 1];
    i = 0;
  }
  return num_found;
}

// Find the leaf
node *findLeaf(node *const root, int key, bool verbose) {
  if (root == NULL) {
    if (verbose)
      printf(""Empty tree.\n"");
    return root;
  }
  int i = 0;
  node *c = root;
  while (!c->is_leaf) {
    if (verbose) {
      printf(""["");
      for (i = 0; i < c->num_keys - 1; i++)
        printf(""%d "", c->keys[i]);
      printf(""%d] "", c->keys[i]);
    }
    i = 0;
    while (i < c->num_keys) {
      if (key >= c->keys[i])
        i++;
      else
        break;
    }
    if (verbose)
      printf(""%d ->\n"", i);
    c = (node *)c->pointers[i];
  }
  if (verbose) {
    printf(""Leaf ["");
    for (i = 0; i < c->num_keys - 1; i++)
      printf(""%d "", c->keys[i]);
    printf(""%d] ->\n"", c->keys[i]);
  }
  return c;
}

record *find(node *root, int key, bool verbose, node **leaf_out) {
  if (root == NULL) {
    if (leaf_out != NULL) {
      *leaf_out = NULL;
    }
    return NULL;
  }

  int i = 0;
  node *leaf = NULL;

  leaf = findLeaf(root, key, verbose);

  for (i = 0; i < leaf->num_keys; i++)
    if (leaf->keys[i] == key)
      break;
  if (leaf_out != NULL) {
    *leaf_out = leaf;
  }
  if (i == leaf->num_keys)
    return NULL;
  else
    return (record *)leaf->pointers[i];
}

int cut(int length) {
  if (length % 2 == 0)
    return length / 2;
  else
    return length / 2 + 1;
}

record *makeRecord(int value) {
  record *new_record = (record *)malloc(sizeof(record));
  if (new_record == NULL) {
    perror(""Record creation."");
    exit(EXIT_FAILURE);
  } else {
    new_record->value = value;
  }
  return new_record;
}

node *makeNode(void) {
  node *new_node;
  new_node = malloc(sizeof(node));
  if (new_node == NULL) {
    perror(""Node creation."");
    exit(EXIT_FAILURE);
  }
  new_node->keys = malloc((order - 1) * sizeof(int));
  if (new_node->keys == NULL) {
    perror(""New node keys array."");
    exit(EXIT_FAILURE);
  }
  new_node->pointers = malloc(order * sizeof(void *));
  if (new_node->pointers == NULL) {
    perror(""New node pointers array."");
    exit(EXIT_FAILURE);
  }
  new_node->is_leaf = false;
  new_node->num_keys = 0;
  new_node->parent = NULL;
  new_node->next = NULL;
  return new_node;
}

node *makeLeaf(void) {
  node *leaf = makeNode();
  leaf->is_leaf = true;
  return leaf;
}

int getLeftIndex(node *parent, node *left) {
  int left_index = 0;
  while (left_index <= parent->num_keys &&
       parent->pointers[left_index] != left)
    left_index++;
  return left_index;
}

node *insertIntoLeaf(node *leaf, int key, record *pointer) {
  int i, insertion_point;

  insertion_point = 0;
  while (insertion_point < leaf->num_keys && leaf->keys[insertion_point] < key)
    insertion_point++;

  for (i = leaf->num_keys; i > insertion_point; i--) {
    leaf->keys[i] = leaf->keys[i - 1];
    leaf->pointers[i] = leaf->pointers[i - 1];
  }
  leaf->keys[insertion_point] = key;
  leaf->pointers[insertion_point] = pointer;
  leaf->num_keys++;
  return leaf;
}

node *insertIntoLeafAfterSplitting(node *root, node *leaf, int key, record *pointer) {
  node *new_leaf;
  int *temp_keys;
  void **temp_pointers;
  int insertion_index, split, new_key, i, j;

  new_leaf = makeLeaf();

  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    perror(""Temporary keys array."");
    exit(EXIT_FAILURE);
  }

  temp_pointers = malloc(order * sizeof(void *));
  if (temp_pointers == NULL) {
    perror(""Temporary pointers array."");
    exit(EXIT_FAILURE);
  }

  insertion_index = 0;
  while (insertion_index < order - 1 && leaf->keys[insertion_index] < key)
    insertion_index++;

  for (i = 0, j = 0; i < leaf->num_keys; i++, j++) {
    if (j == insertion_index)
      j++;
    temp_keys[j] = leaf->keys[i];
    temp_pointers[j] = leaf->pointers[i];
  }

  temp_keys[insertion_index] = key;
  temp_pointers[insertion_index] = pointer;

  leaf->num_keys = 0;

  split = cut(order - 1);

  for (i = 0; i < split; i++) {
    leaf->pointers[i] = temp_pointers[i];
    leaf->keys[i] = temp_keys[i];
    leaf->num_keys++;
  }

  for (i = split, j = 0; i < order; i++, j++) {
    new_leaf->pointers[j] = temp_pointers[i];
    new_leaf->keys[j] = temp_keys[i];
    new_leaf->num_keys++;
  }

  free(temp_pointers);
  free(temp_keys);

  new_leaf->pointers[order - 1] = leaf->pointers[order - 1];
  leaf->pointers[order - 1] = new_leaf;

  for (i = leaf->num_keys; i < order - 1; i++)
    leaf->pointers[i] = NULL;
  for (i = new_leaf->num_keys; i < order - 1; i++)
    new_leaf->pointers[i] = NULL;

  new_leaf->parent = leaf->parent;
  new_key = new_leaf->keys[0];

  return insertIntoParent(root, leaf, new_key, new_leaf);
}

node *insertIntoNode(node *root, node *n,
           int left_index, int key, node *right) {
  int i;

  for (i = n->num_keys; i > left_index; i--) {
    n->pointers[i + 1] = n->pointers[i];
    n->keys[i] = n->keys[i - 1];
  }
  n->pointers[left_index + 1] = right;
  n->keys[left_index] = key;
  n->num_keys++;
  return root;
}

node *insertIntoNodeAfterSplitting(node *root, node *old_node, int left_index,
                   int key, node *right) {
  int i, j, split, k_prime;
  node *new_node, *child;
  int *temp_keys;
  node **temp_pointers;

  temp_pointers = malloc((order + 1) * sizeof(node *));
  if (temp_pointers == NULL) {
    exit(EXIT_FAILURE);
  }
  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    exit(EXIT_FAILURE);
  }

  for (i = 0, j = 0; i < old_node->num_keys + 1; i++, j++) {
    if (j == left_index + 1)
      j++;
    temp_pointers[j] = old_node->pointers[i];
  }

  for (i = 0, j = 0; i < old_node->num_keys; i++, j++) {
    if (j == left_index)
      j++;
    temp_keys[j] = old_node->keys[i];
  }

  temp_pointers[left_index + 1] = right;
  temp_keys[left_index] = key;

  split = cut(order);
  new_node = makeNode();
  old_node->num_keys = 0;
  for (i = 0; i < split - 1; i++) {
    old_node->pointers[i] = temp_pointers[i];
    old_node->keys[i] = temp_keys[i];
    old_node->num_keys++;
  }
  old_node->pointers[i] = temp_pointers[i];
  k_prime = temp_keys[split - 1];
  for (++i, j = 0; i < order; i++, j++) {
    new_node->pointers[j] = temp_pointers[i];
    new_node->keys[j] = temp_keys[i];
    new_node->num_keys++;
  }
  new_node->pointers[j] = temp_pointers[i];
  free(temp_pointers);
  free(temp_keys);
  new_node->parent = old_node->parent;
  for (i = 0; i <= new_node->num_keys; i++) {
    child = new_node->pointers[i];
    child->parent = new_node;
  }

  return insertIntoParent(root, old_node, k_prime, new_node);
}

node *insertIntoParent(node *root, node *left, int key, node *right) {
  int left_index;
  node *parent;

  parent = left->parent;

  if (parent == NULL)
    return insertIntoNewRoot(left, key, right);

  left_index = getLeftIndex(parent, left);

  if (parent->num_keys < order - 1)
    return insertIntoNode(root, parent, left_index, key, right);

  return insertIntoNodeAfterSplitting(root, parent, left_index, key, right);
}

node *insertIntoNewRoot(node *left, int key, node *right) {
  node *root = makeNode();
  root->keys[0] = key;
  root->pointers[0] = left;
  root->pointers[1] = right;
  root->num_keys++;
  root->parent = NULL;
  left->parent = root;
  right->parent = root;
  return root;
}

node *startNewTree(int key, record *pointer) {
  node *root = makeLeaf();
  root->keys[0] = key;
  root->pointers[0] = pointer;
  root->pointers[order - 1] = NULL;
  root->parent = NULL;
  root->num_keys++;
  return root;
}

node *insert(node *root, int key, int value) {
  record *record_pointer = NULL;
  node *leaf = NULL;

  record_pointer = find(root, key, false, NULL);
  if (record_pointer != NULL) {
    record_pointer->value = value;
    return root;
  }

  record_pointer = makeRecord(value);

  if (root == NULL)
    return startNewTree(key, record_pointer);

  leaf = findLeaf(root, key, false);

  if (leaf->num_keys < order - 1) {
    leaf = insertIntoLeaf(leaf, key, record_pointer);
    return root;
  }

  return insertIntoLeafAfterSplitting(root, leaf, key, record_pointer);
}

int main() {
  node *root;
  char instruction;

  root = NULL;

  root = insert(root, 5, 33);
  root = insert(root, 15, 21);
  root = insert(root, 25, 31);
  root = insert(root, 35, 41);
  root = insert(root, 45, 10);

  printTree(root);

  findAndPrint(root, 15, instruction = 'a');
}"
Insertion on a B+ Tree,"Inserting an element into a B+ tree consists of three main events: searching the appropriate leaf, inserting the element and balancing/splitting the tree. Let us understand these events below. Before inserting an element into a B+ tree, these properties must be kept in mind. The root has at least two children.
	Each node except root can have a maximum of m children and at least m/2 children.
	Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The root has at least two children. Each node except root can have a maximum of m children and at least m/2 children. Each node can contain a maximum of m - 1 keys and a minimum of ⌈m/2⌉ - 1 keys. The following steps are followed for inserting an element. Since every element is inserted into the leaf node, go to the appropriate leaf node. Insert the key into the leaf node. If the leaf is not full, insert the key into the leaf node in increasing order. If the leaf is full, insert the key into the leaf node in increasing order and balance the tree in the following way. Break the node at m/2th position. Add m/2th key to the parent node as well. If the parent node is already full, follow steps 2 to 3. Let us understand the insertion operation with the illustrations below.  The elements to be inserted are 5,15, 25, 35, 45. Insert 5.
		
			Insert 5 Insert 15.
		
			Insert 15 Insert 25.
		
			Insert 25 Insert 35.
		
			Insert 35 Insert 45.
		
			Insert 45 Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n).","// Searching on a B+ tree in C++

#include <climits>
#include <fstream>
#include <iostream>
#include <sstream>
using namespace std;
int MAX = 3;

// BP node
class Node {
  bool IS_LEAF;
  int *key, size;
  Node **ptr;
  friend class BPTree;

   public:
  Node();
};

// BP tree
class BPTree {
  Node *root;
  void insertInternal(int, Node *, Node *);
  Node *findParent(Node *, Node *);

   public:
  BPTree();
  void search(int);
  void insert(int);
  void display(Node *);
  Node *getRoot();
};

Node::Node() {
  key = new int[MAX];
  ptr = new Node *[MAX + 1];
}

BPTree::BPTree() {
  root = NULL;
}

// Search operation
void BPTree::search(int x) {
  if (root == NULL) {
    cout << ""Tree is empty\n"";
  } else {
    Node *cursor = root;
    while (cursor->IS_LEAF == false) {
      for (int i = 0; i < cursor->size; i++) {
        if (x < cursor->key[i]) {
          cursor = cursor->ptr[i];
          break;
        }
        if (i == cursor->size - 1) {
          cursor = cursor->ptr[i + 1];
          break;
        }
      }
    }
    for (int i = 0; i < cursor->size; i++) {
      if (cursor->key[i] == x) {
        cout << ""Found\n"";
        return;
      }
    }
    cout << ""Not found\n"";
  }
}

// Insert Operation
void BPTree::insert(int x) {
  if (root == NULL) {
    root = new Node;
    root->key[0] = x;
    root->IS_LEAF = true;
    root->size = 1;
  } else {
    Node *cursor = root;
    Node *parent;
    while (cursor->IS_LEAF == false) {
      parent = cursor;
      for (int i = 0; i < cursor->size; i++) {
        if (x < cursor->key[i]) {
          cursor = cursor->ptr[i];
          break;
        }
        if (i == cursor->size - 1) {
          cursor = cursor->ptr[i + 1];
          break;
        }
      }
    }
    if (cursor->size < MAX) {
      int i = 0;
      while (x > cursor->key[i] && i < cursor->size)
        i++;
      for (int j = cursor->size; j > i; j--) {
        cursor->key[j] = cursor->key[j - 1];
      }
      cursor->key[i] = x;
      cursor->size++;
      cursor->ptr[cursor->size] = cursor->ptr[cursor->size - 1];
      cursor->ptr[cursor->size - 1] = NULL;
    } else {
      Node *newLeaf = new Node;
      int virtualNode[MAX + 1];
      for (int i = 0; i < MAX; i++) {
        virtualNode[i] = cursor->key[i];
      }
      int i = 0, j;
      while (x > virtualNode[i] && i < MAX)
        i++;
      for (int j = MAX + 1; j > i; j--) {
        virtualNode[j] = virtualNode[j - 1];
      }
      virtualNode[i] = x;
      newLeaf->IS_LEAF = true;
      cursor->size = (MAX + 1) / 2;
      newLeaf->size = MAX + 1 - (MAX + 1) / 2;
      cursor->ptr[cursor->size] = newLeaf;
      newLeaf->ptr[newLeaf->size] = cursor->ptr[MAX];
      cursor->ptr[MAX] = NULL;
      for (i = 0; i < cursor->size; i++) {
        cursor->key[i] = virtualNode[i];
      }
      for (i = 0, j = cursor->size; i < newLeaf->size; i++, j++) {
        newLeaf->key[i] = virtualNode[j];
      }
      if (cursor == root) {
        Node *newRoot = new Node;
        newRoot->key[0] = newLeaf->key[0];
        newRoot->ptr[0] = cursor;
        newRoot->ptr[1] = newLeaf;
        newRoot->IS_LEAF = false;
        newRoot->size = 1;
        root = newRoot;
      } else {
        insertInternal(newLeaf->key[0], parent, newLeaf);
      }
    }
  }
}

// Insert Operation
void BPTree::insertInternal(int x, Node *cursor, Node *child) {
  if (cursor->size < MAX) {
    int i = 0;
    while (x > cursor->key[i] && i < cursor->size)
      i++;
    for (int j = cursor->size; j > i; j--) {
      cursor->key[j] = cursor->key[j - 1];
    }
    for (int j = cursor->size + 1; j > i + 1; j--) {
      cursor->ptr[j] = cursor->ptr[j - 1];
    }
    cursor->key[i] = x;
    cursor->size++;
    cursor->ptr[i + 1] = child;
  } else {
    Node *newInternal = new Node;
    int virtualKey[MAX + 1];
    Node *virtualPtr[MAX + 2];
    for (int i = 0; i < MAX; i++) {
      virtualKey[i] = cursor->key[i];
    }
    for (int i = 0; i < MAX + 1; i++) {
      virtualPtr[i] = cursor->ptr[i];
    }
    int i = 0, j;
    while (x > virtualKey[i] && i < MAX)
      i++;
    for (int j = MAX + 1; j > i; j--) {
      virtualKey[j] = virtualKey[j - 1];
    }
    virtualKey[i] = x;
    for (int j = MAX + 2; j > i + 1; j--) {
      virtualPtr[j] = virtualPtr[j - 1];
    }
    virtualPtr[i + 1] = child;
    newInternal->IS_LEAF = false;
    cursor->size = (MAX + 1) / 2;
    newInternal->size = MAX - (MAX + 1) / 2;
    for (i = 0, j = cursor->size + 1; i < newInternal->size; i++, j++) {
      newInternal->key[i] = virtualKey[j];
    }
    for (i = 0, j = cursor->size + 1; i < newInternal->size + 1; i++, j++) {
      newInternal->ptr[i] = virtualPtr[j];
    }
    if (cursor == root) {
      Node *newRoot = new Node;
      newRoot->key[0] = cursor->key[cursor->size];
      newRoot->ptr[0] = cursor;
      newRoot->ptr[1] = newInternal;
      newRoot->IS_LEAF = false;
      newRoot->size = 1;
      root = newRoot;
    } else {
      insertInternal(cursor->key[cursor->size], findParent(root, cursor), newInternal);
    }
  }
}

// Find the parent
Node *BPTree::findParent(Node *cursor, Node *child) {
  Node *parent;
  if (cursor->IS_LEAF || (cursor->ptr[0])->IS_LEAF) {
    return NULL;
  }
  for (int i = 0; i < cursor->size + 1; i++) {
    if (cursor->ptr[i] == child) {
      parent = cursor;
      return parent;
    } else {
      parent = findParent(cursor->ptr[i], child);
      if (parent != NULL)
        return parent;
    }
  }
  return parent;
}

// Print the tree
void BPTree::display(Node *cursor) {
  if (cursor != NULL) {
    for (int i = 0; i < cursor->size; i++) {
      cout << cursor->key[i] << "" "";
    }
    cout << ""\n"";
    if (cursor->IS_LEAF != true) {
      for (int i = 0; i < cursor->size + 1; i++) {
        display(cursor->ptr[i]);
      }
    }
  }
}

// Get the root
Node *BPTree::getRoot() {
  return root;
}

int main() {
  BPTree node;
  node.insert(5);
  node.insert(15);
  node.insert(25);
  node.insert(35);
  node.insert(45);
  node.insert(55);
  node.insert(40);
  node.insert(30);
  node.insert(20);
  node.display(node.getRoot());

  node.search(15);
}"
Deletion from a B+ Tree,"Deleting an element on a B+ tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required.Underflow is a situation when there is less number of keys in a node than the minimum number of keys it should hold. Before going through the steps below, one must know these facts about a B+ tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) While deleting a key, we have to take care of the keys present in the internal nodes (i.e. indexes) as well because the values are redundant in a B+ tree. Search the key to be deleted then follow the following steps. The key to be deleted is present only at the leaf node not in the indexes (or internal nodes). There are two cases for it: There is more than the minimum number of keys in the node. Simply delete the key.
		
			Deleting 40 from B-tree There is an exact minimum number of keys in the node. Delete the key and borrow a key from the immediate sibling. Add the median key of the sibling node to the parent.
		
			Deleting 5 from B-tree The key to be deleted is present in the internal nodes as well. Then we have to remove them from the internal nodes as well. There are the following cases for this situation. If there is more than the minimum number of keys in the node, simply delete the key from the leaf node and delete the key from the internal node as well.
		Fill the empty space in the internal node with the inorder successor.
		
			Deleting 45 from B-tree If there is an exact minimum number of keys in the node, then delete the key and borrow a key from its immediate sibling (through the parent).
		Fill the empty space created in the index (internal node) with the borrowed key.
		
			Deleting 35 from B-tree This case is similar to Case II(1) but here, empty space is generated above the immediate parent node.
		After deleting the key, merge the empty space with its sibling.
		Fill the empty space in the grandparent node with the inorder successor.
		
			Deleting 25 from B-tree  In this case, the height of the tree gets shrinked. It is a little complicated.Deleting 55 from the tree below leads to this condition. It can be understood in the illustrations below. Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n). ","# B+ tee in python


import math

# Node creation
class Node:
    def __init__(self, order):
        self.order = order
        self.values = []
        self.keys = []
        self.nextKey = None
        self.parent = None
        self.check_leaf = False

    # Insert at the leaf
    def insert_at_leaf(self, leaf, value, key):
        if (self.values):
            temp1 = self.values
            for i in range(len(temp1)):
                if (value == temp1[i]):
                    self.keys[i].append(key)
                    break
                elif (value < temp1[i]):
                    self.values = self.values[:i] + [value] + self.values[i:]
                    self.keys = self.keys[:i] + [[key]] + self.keys[i:]
                    break
                elif (i + 1 == len(temp1)):
                    self.values.append(value)
                    self.keys.append([key])
                    break
        else:
            self.values = [value]
            self.keys = [[key]]


# B plus tree
class BplusTree:
    def __init__(self, order):
        self.root = Node(order)
        self.root.check_leaf = True

    # Insert operation
    def insert(self, value, key):
        value = str(value)
        old_node = self.search(value)
        old_node.insert_at_leaf(old_node, value, key)

        if (len(old_node.values) == old_node.order):
            node1 = Node(old_node.order)
            node1.check_leaf = True
            node1.parent = old_node.parent
            mid = int(math.ceil(old_node.order / 2)) - 1
            node1.values = old_node.values[mid + 1:]
            node1.keys = old_node.keys[mid + 1:]
            node1.nextKey = old_node.nextKey
            old_node.values = old_node.values[:mid + 1]
            old_node.keys = old_node.keys[:mid + 1]
            old_node.nextKey = node1
            self.insert_in_parent(old_node, node1.values[0], node1)

    # Search operation for different operations
    def search(self, value):
        current_node = self.root
        while(current_node.check_leaf == False):
            temp2 = current_node.values
            for i in range(len(temp2)):
                if (value == temp2[i]):
                    current_node = current_node.keys[i + 1]
                    break
                elif (value < temp2[i]):
                    current_node = current_node.keys[i]
                    break
                elif (i + 1 == len(current_node.values)):
                    current_node = current_node.keys[i + 1]
                    break
        return current_node

    # Find the node
    def find(self, value, key):
        l = self.search(value)
        for i, item in enumerate(l.values):
            if item == value:
                if key in l.keys[i]:
                    return True
                else:
                    return False
        return False

    # Inserting at the parent
    def insert_in_parent(self, n, value, ndash):
        if (self.root == n):
            rootNode = Node(n.order)
            rootNode.values = [value]
            rootNode.keys = [n, ndash]
            self.root = rootNode
            n.parent = rootNode
            ndash.parent = rootNode
            return

        parentNode = n.parent
        temp3 = parentNode.keys
        for i in range(len(temp3)):
            if (temp3[i] == n):
                parentNode.values = parentNode.values[:i] + \
                    [value] + parentNode.values[i:]
                parentNode.keys = parentNode.keys[:i +
                                                  1] + [ndash] + parentNode.keys[i + 1:]
                if (len(parentNode.keys) > parentNode.order):
                    parentdash = Node(parentNode.order)
                    parentdash.parent = parentNode.parent
                    mid = int(math.ceil(parentNode.order / 2)) - 1
                    parentdash.values = parentNode.values[mid + 1:]
                    parentdash.keys = parentNode.keys[mid + 1:]
                    value_ = parentNode.values[mid]
                    if (mid == 0):
                        parentNode.values = parentNode.values[:mid + 1]
                    else:
                        parentNode.values = parentNode.values[:mid]
                    parentNode.keys = parentNode.keys[:mid + 1]
                    for j in parentNode.keys:
                        j.parent = parentNode
                    for j in parentdash.keys:
                        j.parent = parentdash
                    self.insert_in_parent(parentNode, value_, parentdash)

    # Delete a node
    def delete(self, value, key):
        node_ = self.search(value)

        temp = 0
        for i, item in enumerate(node_.values):
            if item == value:
                temp = 1

                if key in node_.keys[i]:
                    if len(node_.keys[i]) > 1:
                        node_.keys[i].pop(node_.keys[i].index(key))
                    elif node_ == self.root:
                        node_.values.pop(i)
                        node_.keys.pop(i)
                    else:
                        node_.keys[i].pop(node_.keys[i].index(key))
                        del node_.keys[i]
                        node_.values.pop(node_.values.index(value))
                        self.deleteEntry(node_, value, key)
                else:
                    print(""Value not in Key"")
                    return
        if temp == 0:
            print(""Value not in Tree"")
            return

    # Delete an entry
    def deleteEntry(self, node_, value, key):

        if not node_.check_leaf:
            for i, item in enumerate(node_.keys):
                if item == key:
                    node_.keys.pop(i)
                    break
            for i, item in enumerate(node_.values):
                if item == value:
                    node_.values.pop(i)
                    break

        if self.root == node_ and len(node_.keys) == 1:
            self.root = node_.keys[0]
            node_.keys[0].parent = None
            del node_
            return
        elif (len(node_.keys) < int(math.ceil(node_.order / 2)) and node_.check_leaf == False) or (len(node_.values) < int(math.ceil((node_.order - 1) / 2)) and node_.check_leaf == True):

            is_predecessor = 0
            parentNode = node_.parent
            PrevNode = -1
            NextNode = -1
            PrevK = -1
            PostK = -1
            for i, item in enumerate(parentNode.keys):

                if item == node_:
                    if i > 0:
                        PrevNode = parentNode.keys[i - 1]
                        PrevK = parentNode.values[i - 1]

                    if i < len(parentNode.keys) - 1:
                        NextNode = parentNode.keys[i + 1]
                        PostK = parentNode.values[i]

            if PrevNode == -1:
                ndash = NextNode
                value_ = PostK
            elif NextNode == -1:
                is_predecessor = 1
                ndash = PrevNode
                value_ = PrevK
            else:
                if len(node_.values) + len(NextNode.values) < node_.order:
                    ndash = NextNode
                    value_ = PostK
                else:
                    is_predecessor = 1
                    ndash = PrevNode
                    value_ = PrevK

            if len(node_.values) + len(ndash.values) < node_.order:
                if is_predecessor == 0:
                    node_, ndash = ndash, node_
                ndash.keys += node_.keys
                if not node_.check_leaf:
                    ndash.values.append(value_)
                else:
                    ndash.nextKey = node_.nextKey
                ndash.values += node_.values

                if not ndash.check_leaf:
                    for j in ndash.keys:
                        j.parent = ndash

                self.deleteEntry(node_.parent, value_, node_)
                del node_
            else:
                if is_predecessor == 1:
                    if not node_.check_leaf:
                        ndashpm = ndash.keys.pop(-1)
                        ndashkm_1 = ndash.values.pop(-1)
                        node_.keys = [ndashpm] + node_.keys
                        node_.values = [value_] + node_.values
                        parentNode = node_.parent
                        for i, item in enumerate(parentNode.values):
                            if item == value_:
                                p.values[i] = ndashkm_1
                                break
                    else:
                        ndashpm = ndash.keys.pop(-1)
                        ndashkm = ndash.values.pop(-1)
                        node_.keys = [ndashpm] + node_.keys
                        node_.values = [ndashkm] + node_.values
                        parentNode = node_.parent
                        for i, item in enumerate(p.values):
                            if item == value_:
                                parentNode.values[i] = ndashkm
                                break
                else:
                    if not node_.check_leaf:
                        ndashp0 = ndash.keys.pop(0)
                        ndashk0 = ndash.values.pop(0)
                        node_.keys = node_.keys + [ndashp0]
                        node_.values = node_.values + [value_]
                        parentNode = node_.parent
                        for i, item in enumerate(parentNode.values):
                            if item == value_:
                                parentNode.values[i] = ndashk0
                                break
                    else:
                        ndashp0 = ndash.keys.pop(0)
                        ndashk0 = ndash.values.pop(0)
                        node_.keys = node_.keys + [ndashp0]
                        node_.values = node_.values + [ndashk0]
                        parentNode = node_.parent
                        for i, item in enumerate(parentNode.values):
                            if item == value_:
                                parentNode.values[i] = ndash.values[0]
                                break

                if not ndash.check_leaf:
                    for j in ndash.keys:
                        j.parent = ndash
                if not node_.check_leaf:
                    for j in node_.keys:
                        j.parent = node_
                if not parentNode.check_leaf:
                    for j in parentNode.keys:
                        j.parent = parentNode


# Print the tree
def printTree(tree):
    lst = [tree.root]
    level = [0]
    leaf = None
    flag = 0
    lev_leaf = 0

    node1 = Node(str(level[0]) + str(tree.root.values))

    while (len(lst) != 0):
        x = lst.pop(0)
        lev = level.pop(0)
        if (x.check_leaf == False):
            for i, item in enumerate(x.keys):
                print(item.values)
        else:
            for i, item in enumerate(x.keys):
                print(item.values)
            if (flag == 0):
                lev_leaf = lev
                leaf = x
                flag = 1


record_len = 3
bplustree = BplusTree(record_len)
bplustree.insert('5', '33')
bplustree.insert('15', '21')
bplustree.insert('25', '31')
bplustree.insert('35', '41')
bplustree.insert('45', '10')

printTree(bplustree)

if(bplustree.find('5', '34')):
    print(""Found"")
else:
    print(""Not found"")"
Deletion from a B+ Tree,"Deleting an element on a B+ tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required.Underflow is a situation when there is less number of keys in a node than the minimum number of keys it should hold. Before going through the steps below, one must know these facts about a B+ tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) While deleting a key, we have to take care of the keys present in the internal nodes (i.e. indexes) as well because the values are redundant in a B+ tree. Search the key to be deleted then follow the following steps. The key to be deleted is present only at the leaf node not in the indexes (or internal nodes). There are two cases for it: There is more than the minimum number of keys in the node. Simply delete the key.
		
			Deleting 40 from B-tree There is an exact minimum number of keys in the node. Delete the key and borrow a key from the immediate sibling. Add the median key of the sibling node to the parent.
		
			Deleting 5 from B-tree The key to be deleted is present in the internal nodes as well. Then we have to remove them from the internal nodes as well. There are the following cases for this situation. If there is more than the minimum number of keys in the node, simply delete the key from the leaf node and delete the key from the internal node as well.
		Fill the empty space in the internal node with the inorder successor.
		
			Deleting 45 from B-tree If there is an exact minimum number of keys in the node, then delete the key and borrow a key from its immediate sibling (through the parent).
		Fill the empty space created in the index (internal node) with the borrowed key.
		
			Deleting 35 from B-tree This case is similar to Case II(1) but here, empty space is generated above the immediate parent node.
		After deleting the key, merge the empty space with its sibling.
		Fill the empty space in the grandparent node with the inorder successor.
		
			Deleting 25 from B-tree  In this case, the height of the tree gets shrinked. It is a little complicated.Deleting 55 from the tree below leads to this condition. It can be understood in the illustrations below. Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n). ","
// Searching on a B+ tree in Java
import java.util.*;

public class BPlusTree {
  int m;
  InternalNode root;
  LeafNode firstLeaf;

  // Binary search program
  private int binarySearch(DictionaryPair[] dps, int numPairs, int t) {
    Comparator<DictionaryPair> c = new Comparator<DictionaryPair>() {
      @Override
      public int compare(DictionaryPair o1, DictionaryPair o2) {
        Integer a = Integer.valueOf(o1.key);
        Integer b = Integer.valueOf(o2.key);
        return a.compareTo(b);
      }
    };
    return Arrays.binarySearch(dps, 0, numPairs, new DictionaryPair(t, 0), c);
  }

  // Find the leaf node
  private LeafNode findLeafNode(int key) {

    Integer[] keys = this.root.keys;
    int i;

    for (i = 0; i < this.root.degree - 1; i++) {
      if (key < keys[i]) {
        break;
      }
    }

    Node child = this.root.childPointers[i];
    if (child instanceof LeafNode) {
      return (LeafNode) child;
    } else {
      return findLeafNode((InternalNode) child, key);
    }
  }

  // Find the leaf node
  private LeafNode findLeafNode(InternalNode node, int key) {

    Integer[] keys = node.keys;
    int i;

    for (i = 0; i < node.degree - 1; i++) {
      if (key < keys[i]) {
        break;
      }
    }
    Node childNode = node.childPointers[i];
    if (childNode instanceof LeafNode) {
      return (LeafNode) childNode;
    } else {
      return findLeafNode((InternalNode) node.childPointers[i], key);
    }
  }

  // Finding the index of the pointer
  private int findIndexOfPointer(Node[] pointers, LeafNode node) {
    int i;
    for (i = 0; i < pointers.length; i++) {
      if (pointers[i] == node) {
        break;
      }
    }
    return i;
  }

  // Get the mid point
  private int getMidpoint() {
    return (int) Math.ceil((this.m + 1) / 2.0) - 1;
  }

  // Balance the tree
  private void handleDeficiency(InternalNode in) {

    InternalNode sibling;
    InternalNode parent = in.parent;

    if (this.root == in) {
      for (int i = 0; i < in.childPointers.length; i++) {
        if (in.childPointers[i] != null) {
          if (in.childPointers[i] instanceof InternalNode) {
            this.root = (InternalNode) in.childPointers[i];
            this.root.parent = null;
          } else if (in.childPointers[i] instanceof LeafNode) {
            this.root = null;
          }
        }
      }
    }

    else if (in.leftSibling != null && in.leftSibling.isLendable()) {
      sibling = in.leftSibling;
    } else if (in.rightSibling != null && in.rightSibling.isLendable()) {
      sibling = in.rightSibling;

      int borrowedKey = sibling.keys[0];
      Node pointer = sibling.childPointers[0];

      in.keys[in.degree - 1] = parent.keys[0];
      in.childPointers[in.degree] = pointer;

      parent.keys[0] = borrowedKey;

      sibling.removePointer(0);
      Arrays.sort(sibling.keys);
      sibling.removePointer(0);
      shiftDown(in.childPointers, 1);
    } else if (in.leftSibling != null && in.leftSibling.isMergeable()) {

    } else if (in.rightSibling != null && in.rightSibling.isMergeable()) {
      sibling = in.rightSibling;
      sibling.keys[sibling.degree - 1] = parent.keys[parent.degree - 2];
      Arrays.sort(sibling.keys, 0, sibling.degree);
      parent.keys[parent.degree - 2] = null;

      for (int i = 0; i < in.childPointers.length; i++) {
        if (in.childPointers[i] != null) {
          sibling.prependChildPointer(in.childPointers[i]);
          in.childPointers[i].parent = sibling;
          in.removePointer(i);
        }
      }

      parent.removePointer(in);

      sibling.leftSibling = in.leftSibling;
    }

    if (parent != null && parent.isDeficient()) {
      handleDeficiency(parent);
    }
  }

  private boolean isEmpty() {
    return firstLeaf == null;
  }

  private int linearNullSearch(DictionaryPair[] dps) {
    for (int i = 0; i < dps.length; i++) {
      if (dps[i] == null) {
        return i;
      }
    }
    return -1;
  }

  private int linearNullSearch(Node[] pointers) {
    for (int i = 0; i < pointers.length; i++) {
      if (pointers[i] == null) {
        return i;
      }
    }
    return -1;
  }

  private void shiftDown(Node[] pointers, int amount) {
    Node[] newPointers = new Node[this.m + 1];
    for (int i = amount; i < pointers.length; i++) {
      newPointers[i - amount] = pointers[i];
    }
    pointers = newPointers;
  }

  private void sortDictionary(DictionaryPair[] dictionary) {
    Arrays.sort(dictionary, new Comparator<DictionaryPair>() {
      @Override
      public int compare(DictionaryPair o1, DictionaryPair o2) {
        if (o1 == null && o2 == null) {
          return 0;
        }
        if (o1 == null) {
          return 1;
        }
        if (o2 == null) {
          return -1;
        }
        return o1.compareTo(o2);
      }
    });
  }

  private Node[] splitChildPointers(InternalNode in, int split) {

    Node[] pointers = in.childPointers;
    Node[] halfPointers = new Node[this.m + 1];

    for (int i = split + 1; i < pointers.length; i++) {
      halfPointers[i - split - 1] = pointers[i];
      in.removePointer(i);
    }

    return halfPointers;
  }

  private DictionaryPair[] splitDictionary(LeafNode ln, int split) {

    DictionaryPair[] dictionary = ln.dictionary;

    DictionaryPair[] halfDict = new DictionaryPair[this.m];

    for (int i = split; i < dictionary.length; i++) {
      halfDict[i - split] = dictionary[i];
      ln.delete(i);
    }

    return halfDict;
  }

  private void splitInternalNode(InternalNode in) {

    InternalNode parent = in.parent;

    int midpoint = getMidpoint();
    int newParentKey = in.keys[midpoint];
    Integer[] halfKeys = splitKeys(in.keys, midpoint);
    Node[] halfPointers = splitChildPointers(in, midpoint);

    in.degree = linearNullSearch(in.childPointers);

    InternalNode sibling = new InternalNode(this.m, halfKeys, halfPointers);
    for (Node pointer : halfPointers) {
      if (pointer != null) {
        pointer.parent = sibling;
      }
    }

    sibling.rightSibling = in.rightSibling;
    if (sibling.rightSibling != null) {
      sibling.rightSibling.leftSibling = sibling;
    }
    in.rightSibling = sibling;
    sibling.leftSibling = in;

    if (parent == null) {

      Integer[] keys = new Integer[this.m];
      keys[0] = newParentKey;
      InternalNode newRoot = new InternalNode(this.m, keys);
      newRoot.appendChildPointer(in);
      newRoot.appendChildPointer(sibling);
      this.root = newRoot;

      in.parent = newRoot;
      sibling.parent = newRoot;

    } else {

      parent.keys[parent.degree - 1] = newParentKey;
      Arrays.sort(parent.keys, 0, parent.degree);

      int pointerIndex = parent.findIndexOfPointer(in) + 1;
      parent.insertChildPointer(sibling, pointerIndex);
      sibling.parent = parent;
    }
  }

  private Integer[] splitKeys(Integer[] keys, int split) {

    Integer[] halfKeys = new Integer[this.m];

    keys[split] = null;

    for (int i = split + 1; i < keys.length; i++) {
      halfKeys[i - split - 1] = keys[i];
      keys[i] = null;
    }

    return halfKeys;
  }

  public void insert(int key, double value) {
    if (isEmpty()) {

      LeafNode ln = new LeafNode(this.m, new DictionaryPair(key, value));

      this.firstLeaf = ln;

    } else {
      LeafNode ln = (this.root == null) ? this.firstLeaf : findLeafNode(key);

      if (!ln.insert(new DictionaryPair(key, value))) {

        ln.dictionary[ln.numPairs] = new DictionaryPair(key, value);
        ln.numPairs++;
        sortDictionary(ln.dictionary);

        int midpoint = getMidpoint();
        DictionaryPair[] halfDict = splitDictionary(ln, midpoint);

        if (ln.parent == null) {

          Integer[] parent_keys = new Integer[this.m];
          parent_keys[0] = halfDict[0].key;
          InternalNode parent = new InternalNode(this.m, parent_keys);
          ln.parent = parent;
          parent.appendChildPointer(ln);

        } else {
          int newParentKey = halfDict[0].key;
          ln.parent.keys[ln.parent.degree - 1] = newParentKey;
          Arrays.sort(ln.parent.keys, 0, ln.parent.degree);
        }

        LeafNode newLeafNode = new LeafNode(this.m, halfDict, ln.parent);

        int pointerIndex = ln.parent.findIndexOfPointer(ln) + 1;
        ln.parent.insertChildPointer(newLeafNode, pointerIndex);

        newLeafNode.rightSibling = ln.rightSibling;
        if (newLeafNode.rightSibling != null) {
          newLeafNode.rightSibling.leftSibling = newLeafNode;
        }
        ln.rightSibling = newLeafNode;
        newLeafNode.leftSibling = ln;

        if (this.root == null) {

          this.root = ln.parent;

        } else {
          InternalNode in = ln.parent;
          while (in != null) {
            if (in.isOverfull()) {
              splitInternalNode(in);
            } else {
              break;
            }
            in = in.parent;
          }
        }
      }
    }
  }

  public Double search(int key) {

    if (isEmpty()) {
      return null;
    }

    LeafNode ln = (this.root == null) ? this.firstLeaf : findLeafNode(key);

    DictionaryPair[] dps = ln.dictionary;
    int index = binarySearch(dps, ln.numPairs, key);

    if (index < 0) {
      return null;
    } else {
      return dps[index].value;
    }
  }

  public ArrayList<Double> search(int lowerBound, int upperBound) {

    ArrayList<Double> values = new ArrayList<Double>();

    LeafNode currNode = this.firstLeaf;
    while (currNode != null) {

      DictionaryPair dps[] = currNode.dictionary;
      for (DictionaryPair dp : dps) {

        if (dp == null) {
          break;
        }

        if (lowerBound <= dp.key && dp.key <= upperBound) {
          values.add(dp.value);
        }
      }
      currNode = currNode.rightSibling;

    }

    return values;
  }

  public BPlusTree(int m) {
    this.m = m;
    this.root = null;
  }

  public class Node {
    InternalNode parent;
  }

  private class InternalNode extends Node {
    int maxDegree;
    int minDegree;
    int degree;
    InternalNode leftSibling;
    InternalNode rightSibling;
    Integer[] keys;
    Node[] childPointers;

    private void appendChildPointer(Node pointer) {
      this.childPointers[degree] = pointer;
      this.degree++;
    }

    private int findIndexOfPointer(Node pointer) {
      for (int i = 0; i < childPointers.length; i++) {
        if (childPointers[i] == pointer) {
          return i;
        }
      }
      return -1;
    }

    private void insertChildPointer(Node pointer, int index) {
      for (int i = degree - 1; i >= index; i--) {
        childPointers[i + 1] = childPointers[i];
      }
      this.childPointers[index] = pointer;
      this.degree++;
    }

    private boolean isDeficient() {
      return this.degree < this.minDegree;
    }

    private boolean isLendable() {
      return this.degree > this.minDegree;
    }

    private boolean isMergeable() {
      return this.degree == this.minDegree;
    }

    private boolean isOverfull() {
      return this.degree == maxDegree + 1;
    }

    private void prependChildPointer(Node pointer) {
      for (int i = degree - 1; i >= 0; i--) {
        childPointers[i + 1] = childPointers[i];
      }
      this.childPointers[0] = pointer;
      this.degree++;
    }

    private void removeKey(int index) {
      this.keys[index] = null;
    }

    private void removePointer(int index) {
      this.childPointers[index] = null;
      this.degree--;
    }

    private void removePointer(Node pointer) {
      for (int i = 0; i < childPointers.length; i++) {
        if (childPointers[i] == pointer) {
          this.childPointers[i] = null;
        }
      }
      this.degree--;
    }

    private InternalNode(int m, Integer[] keys) {
      this.maxDegree = m;
      this.minDegree = (int) Math.ceil(m / 2.0);
      this.degree = 0;
      this.keys = keys;
      this.childPointers = new Node[this.maxDegree + 1];
    }

    private InternalNode(int m, Integer[] keys, Node[] pointers) {
      this.maxDegree = m;
      this.minDegree = (int) Math.ceil(m / 2.0);
      this.degree = linearNullSearch(pointers);
      this.keys = keys;
      this.childPointers = pointers;
    }
  }

  public class LeafNode extends Node {
    int maxNumPairs;
    int minNumPairs;
    int numPairs;
    LeafNode leftSibling;
    LeafNode rightSibling;
    DictionaryPair[] dictionary;

    public void delete(int index) {
      this.dictionary[index] = null;
      numPairs--;
    }

    public boolean insert(DictionaryPair dp) {
      if (this.isFull()) {
        return false;
      } else {
        this.dictionary[numPairs] = dp;
        numPairs++;
        Arrays.sort(this.dictionary, 0, numPairs);

        return true;
      }
    }

    public boolean isDeficient() {
      return numPairs < minNumPairs;
    }

    public boolean isFull() {
      return numPairs == maxNumPairs;
    }

    public boolean isLendable() {
      return numPairs > minNumPairs;
    }

    public boolean isMergeable() {
      return numPairs == minNumPairs;
    }

    public LeafNode(int m, DictionaryPair dp) {
      this.maxNumPairs = m - 1;
      this.minNumPairs = (int) (Math.ceil(m / 2) - 1);
      this.dictionary = new DictionaryPair[m];
      this.numPairs = 0;
      this.insert(dp);
    }

    public LeafNode(int m, DictionaryPair[] dps, InternalNode parent) {
      this.maxNumPairs = m - 1;
      this.minNumPairs = (int) (Math.ceil(m / 2) - 1);
      this.dictionary = dps;
      this.numPairs = linearNullSearch(dps);
      this.parent = parent;
    }
  }

  public class DictionaryPair implements Comparable<DictionaryPair> {
    int key;
    double value;

    public DictionaryPair(int key, double value) {
      this.key = key;
      this.value = value;
    }

    public int compareTo(DictionaryPair o) {
      if (key == o.key) {
        return 0;
      } else if (key > o.key) {
        return 1;
      } else {
        return -1;
      }
    }
  }

  public static void main(String[] args) {
    BPlusTree bpt = null;
    bpt = new BPlusTree(3);
    bpt.insert(5, 33);
    bpt.insert(15, 21);
    bpt.insert(25, 31);
    bpt.insert(35, 41);
    bpt.insert(45, 10);

    if (bpt.search(15) != null) {
      System.out.println(""Found"");
    } else {
      System.out.println(""Not Found"");
    }
    ;
  }
}"
Deletion from a B+ Tree,"Deleting an element on a B+ tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required.Underflow is a situation when there is less number of keys in a node than the minimum number of keys it should hold. Before going through the steps below, one must know these facts about a B+ tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) While deleting a key, we have to take care of the keys present in the internal nodes (i.e. indexes) as well because the values are redundant in a B+ tree. Search the key to be deleted then follow the following steps. The key to be deleted is present only at the leaf node not in the indexes (or internal nodes). There are two cases for it: There is more than the minimum number of keys in the node. Simply delete the key.
		
			Deleting 40 from B-tree There is an exact minimum number of keys in the node. Delete the key and borrow a key from the immediate sibling. Add the median key of the sibling node to the parent.
		
			Deleting 5 from B-tree The key to be deleted is present in the internal nodes as well. Then we have to remove them from the internal nodes as well. There are the following cases for this situation. If there is more than the minimum number of keys in the node, simply delete the key from the leaf node and delete the key from the internal node as well.
		Fill the empty space in the internal node with the inorder successor.
		
			Deleting 45 from B-tree If there is an exact minimum number of keys in the node, then delete the key and borrow a key from its immediate sibling (through the parent).
		Fill the empty space created in the index (internal node) with the borrowed key.
		
			Deleting 35 from B-tree This case is similar to Case II(1) but here, empty space is generated above the immediate parent node.
		After deleting the key, merge the empty space with its sibling.
		Fill the empty space in the grandparent node with the inorder successor.
		
			Deleting 25 from B-tree  In this case, the height of the tree gets shrinked. It is a little complicated.Deleting 55 from the tree below leads to this condition. It can be understood in the illustrations below. Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n). ","// Deletion on a B+ Tree in C

#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

// Default order
#define ORDER 3

typedef struct record {
  int value;
} record;

// Node
typedef struct node {
  void **pointers;
  int *keys;
  struct node *parent;
  bool is_leaf;
  int num_keys;
  struct node *next;
} node;

int order = ORDER;
node *queue = NULL;
bool verbose_output = false;

// Enqueue
void enqueue(node *new_node);

// Dequeue
node *dequeue(void);
int height(node *const root);
int pathToLeaves(node *const root, node *child);
void printLeaves(node *const root);
void printTree(node *const root);
void findAndPrint(node *const root, int key, bool verbose);
void findAndPrintRange(node *const root, int range1, int range2, bool verbose);
int findRange(node *const root, int key_start, int key_end, bool verbose,
        int returned_keys[], void *returned_pointers[]);
node *findLeaf(node *const root, int key, bool verbose);
record *find(node *root, int key, bool verbose, node **leaf_out);
int cut(int length);

record *makeRecord(int value);
node *makeNode(void);
node *makeLeaf(void);
int getLeftIndex(node *parent, node *left);
node *insertIntoLeaf(node *leaf, int key, record *pointer);
node *insertIntoLeafAfterSplitting(node *root, node *leaf, int key,
                   record *pointer);
node *insertIntoNode(node *root, node *parent,
           int left_index, int key, node *right);
node *insertIntoNodeAfterSplitting(node *root, node *parent,
                   int left_index,
                   int key, node *right);
node *insertIntoParent(node *root, node *left, int key, node *right);
node *insertIntoNewRoot(node *left, int key, node *right);
node *startNewTree(int key, record *pointer);
node *insert(node *root, int key, int value);

// Enqueue
void enqueue(node *new_node) {
  node *c;
  if (queue == NULL) {
    queue = new_node;
    queue->next = NULL;
  } else {
    c = queue;
    while (c->next != NULL) {
      c = c->next;
    }
    c->next = new_node;
    new_node->next = NULL;
  }
}

// Dequeue
node *dequeue(void) {
  node *n = queue;
  queue = queue->next;
  n->next = NULL;
  return n;
}

// Print the leaves
void printLeaves(node *const root) {
  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  int i;
  node *c = root;
  while (!c->is_leaf)
    c = c->pointers[0];
  while (true) {
    for (i = 0; i < c->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", c->pointers[i]);
      printf(""%d "", c->keys[i]);
    }
    if (verbose_output)
      printf(""%p "", c->pointers[order - 1]);
    if (c->pointers[order - 1] != NULL) {
      printf("" | "");
      c = c->pointers[order - 1];
    } else
      break;
  }
  printf(""\n"");
}

// Calculate height
int height(node *const root) {
  int h = 0;
  node *c = root;
  while (!c->is_leaf) {
    c = c->pointers[0];
    h++;
  }
  return h;
}

// Get path to root
int pathToLeaves(node *const root, node *child) {
  int length = 0;
  node *c = child;
  while (c != root) {
    c = c->parent;
    length++;
  }
  return length;
}

// Print the tree
void printTree(node *const root) {
  node *n = NULL;
  int i = 0;
  int rank = 0;
  int new_rank = 0;

  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  queue = NULL;
  enqueue(root);
  while (queue != NULL) {
    n = dequeue();
    if (n->parent != NULL && n == n->parent->pointers[0]) {
      new_rank = pathToLeaves(root, n);
      if (new_rank != rank) {
        rank = new_rank;
        printf(""\n"");
      }
    }
    if (verbose_output)
      printf(""(%p)"", n);
    for (i = 0; i < n->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", n->pointers[i]);
      printf(""%d "", n->keys[i]);
    }
    if (!n->is_leaf)
      for (i = 0; i <= n->num_keys; i++)
        enqueue(n->pointers[i]);
    if (verbose_output) {
      if (n->is_leaf)
        printf(""%p "", n->pointers[order - 1]);
      else
        printf(""%p "", n->pointers[n->num_keys]);
    }
    printf(""| "");
  }
  printf(""\n"");
}

// Find the node and print it
void findAndPrint(node *const root, int key, bool verbose) {
  node *leaf = NULL;
  record *r = find(root, key, verbose, NULL);
  if (r == NULL)
    printf(""Record not found under key %d.\n"", key);
  else
    printf(""Record at %p -- key %d, value %d.\n"",
         r, key, r->value);
}

// Find and print the range
void findAndPrintRange(node *const root, int key_start, int key_end,
             bool verbose) {
  int i;
  int array_size = key_end - key_start + 1;
  int returned_keys[array_size];
  void *returned_pointers[array_size];
  int num_found = findRange(root, key_start, key_end, verbose,
                returned_keys, returned_pointers);
  if (!num_found)
    printf(""None found.\n"");
  else {
    for (i = 0; i < num_found; i++)
      printf(""Key: %d   Location: %p  Value: %d\n"",
           returned_keys[i],
           returned_pointers[i],
           ((record *)
            returned_pointers[i])
             ->value);
  }
}

// Find the range
int findRange(node *const root, int key_start, int key_end, bool verbose,
        int returned_keys[], void *returned_pointers[]) {
  int i, num_found;
  num_found = 0;
  node *n = findLeaf(root, key_start, verbose);
  if (n == NULL)
    return 0;
  for (i = 0; i < n->num_keys && n->keys[i] < key_start; i++)
    ;
  if (i == n->num_keys)
    return 0;
  while (n != NULL) {
    for (; i < n->num_keys && n->keys[i] <= key_end; i++) {
      returned_keys[num_found] = n->keys[i];
      returned_pointers[num_found] = n->pointers[i];
      num_found++;
    }
    n = n->pointers[order - 1];
    i = 0;
  }
  return num_found;
}

// Find the leaf
node *findLeaf(node *const root, int key, bool verbose) {
  if (root == NULL) {
    if (verbose)
      printf(""Empty tree.\n"");
    return root;
  }
  int i = 0;
  node *c = root;
  while (!c->is_leaf) {
    if (verbose) {
      printf(""["");
      for (i = 0; i < c->num_keys - 1; i++)
        printf(""%d "", c->keys[i]);
      printf(""%d] "", c->keys[i]);
    }
    i = 0;
    while (i < c->num_keys) {
      if (key >= c->keys[i])
        i++;
      else
        break;
    }
    if (verbose)
      printf(""%d ->\n"", i);
    c = (node *)c->pointers[i];
  }
  if (verbose) {
    printf(""Leaf ["");
    for (i = 0; i < c->num_keys - 1; i++)
      printf(""%d "", c->keys[i]);
    printf(""%d] ->\n"", c->keys[i]);
  }
  return c;
}

record *find(node *root, int key, bool verbose, node **leaf_out) {
  if (root == NULL) {
    if (leaf_out != NULL) {
      *leaf_out = NULL;
    }
    return NULL;
  }

  int i = 0;
  node *leaf = NULL;

  leaf = findLeaf(root, key, verbose);

  for (i = 0; i < leaf->num_keys; i++)
    if (leaf->keys[i] == key)
      break;
  if (leaf_out != NULL) {
    *leaf_out = leaf;
  }
  if (i == leaf->num_keys)
    return NULL;
  else
    return (record *)leaf->pointers[i];
}

int cut(int length) {
  if (length % 2 == 0)
    return length / 2;
  else
    return length / 2 + 1;
}

record *makeRecord(int value) {
  record *new_record = (record *)malloc(sizeof(record));
  if (new_record == NULL) {
    perror(""Record creation."");
    exit(EXIT_FAILURE);
  } else {
    new_record->value = value;
  }
  return new_record;
}

node *makeNode(void) {
  node *new_node;
  new_node = malloc(sizeof(node));
  if (new_node == NULL) {
    perror(""Node creation."");
    exit(EXIT_FAILURE);
  }
  new_node->keys = malloc((order - 1) * sizeof(int));
  if (new_node->keys == NULL) {
    perror(""New node keys array."");
    exit(EXIT_FAILURE);
  }
  new_node->pointers = malloc(order * sizeof(void *));
  if (new_node->pointers == NULL) {
    perror(""New node pointers array."");
    exit(EXIT_FAILURE);
  }
  new_node->is_leaf = false;
  new_node->num_keys = 0;
  new_node->parent = NULL;
  new_node->next = NULL;
  return new_node;
}

node *makeLeaf(void) {
  node *leaf = makeNode();
  leaf->is_leaf = true;
  return leaf;
}

int getLeftIndex(node *parent, node *left) {
  int left_index = 0;
  while (left_index <= parent->num_keys &&
       parent->pointers[left_index] != left)
    left_index++;
  return left_index;
}

node *insertIntoLeaf(node *leaf, int key, record *pointer) {
  int i, insertion_point;

  insertion_point = 0;
  while (insertion_point < leaf->num_keys && leaf->keys[insertion_point] < key)
    insertion_point++;

  for (i = leaf->num_keys; i > insertion_point; i--) {
    leaf->keys[i] = leaf->keys[i - 1];
    leaf->pointers[i] = leaf->pointers[i - 1];
  }
  leaf->keys[insertion_point] = key;
  leaf->pointers[insertion_point] = pointer;
  leaf->num_keys++;
  return leaf;
}

node *insertIntoLeafAfterSplitting(node *root, node *leaf, int key, record *pointer) {
  node *new_leaf;
  int *temp_keys;
  void **temp_pointers;
  int insertion_index, split, new_key, i, j;

  new_leaf = makeLeaf();

  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    perror(""Temporary keys array."");
    exit(EXIT_FAILURE);
  }

  temp_pointers = malloc(order * sizeof(void *));
  if (temp_pointers == NULL) {
    perror(""Temporary pointers array."");
    exit(EXIT_FAILURE);
  }

  insertion_index = 0;
  while (insertion_index < order - 1 && leaf->keys[insertion_index] < key)
    insertion_index++;

  for (i = 0, j = 0; i < leaf->num_keys; i++, j++) {
    if (j == insertion_index)
      j++;
    temp_keys[j] = leaf->keys[i];
    temp_pointers[j] = leaf->pointers[i];
  }

  temp_keys[insertion_index] = key;
  temp_pointers[insertion_index] = pointer;

  leaf->num_keys = 0;

  split = cut(order - 1);

  for (i = 0; i < split; i++) {
    leaf->pointers[i] = temp_pointers[i];
    leaf->keys[i] = temp_keys[i];
    leaf->num_keys++;
  }

  for (i = split, j = 0; i < order; i++, j++) {
    new_leaf->pointers[j] = temp_pointers[i];
    new_leaf->keys[j] = temp_keys[i];
    new_leaf->num_keys++;
  }

  free(temp_pointers);
  free(temp_keys);

  new_leaf->pointers[order - 1] = leaf->pointers[order - 1];
  leaf->pointers[order - 1] = new_leaf;

  for (i = leaf->num_keys; i < order - 1; i++)
    leaf->pointers[i] = NULL;
  for (i = new_leaf->num_keys; i < order - 1; i++)
    new_leaf->pointers[i] = NULL;

  new_leaf->parent = leaf->parent;
  new_key = new_leaf->keys[0];

  return insertIntoParent(root, leaf, new_key, new_leaf);
}

node *insertIntoNode(node *root, node *n,
           int left_index, int key, node *right) {
  int i;

  for (i = n->num_keys; i > left_index; i--) {
    n->pointers[i + 1] = n->pointers[i];
    n->keys[i] = n->keys[i - 1];
  }
  n->pointers[left_index + 1] = right;
  n->keys[left_index] = key;
  n->num_keys++;
  return root;
}

node *insertIntoNodeAfterSplitting(node *root, node *old_node, int left_index,
                   int key, node *right) {
  int i, j, split, k_prime;
  node *new_node, *child;
  int *temp_keys;
  node **temp_pointers;

  temp_pointers = malloc((order + 1) * sizeof(node *));
  if (temp_pointers == NULL) {
    exit(EXIT_FAILURE);
  }
  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    exit(EXIT_FAILURE);
  }

  for (i = 0, j = 0; i < old_node->num_keys + 1; i++, j++) {
    if (j == left_index + 1)
      j++;
    temp_pointers[j] = old_node->pointers[i];
  }

  for (i = 0, j = 0; i < old_node->num_keys; i++, j++) {
    if (j == left_index)
      j++;
    temp_keys[j] = old_node->keys[i];
  }

  temp_pointers[left_index + 1] = right;
  temp_keys[left_index] = key;

  split = cut(order);
  new_node = makeNode();
  old_node->num_keys = 0;
  for (i = 0; i < split - 1; i++) {
    old_node->pointers[i] = temp_pointers[i];
    old_node->keys[i] = temp_keys[i];
    old_node->num_keys++;
  }
  old_node->pointers[i] = temp_pointers[i];
  k_prime = temp_keys[split - 1];
  for (++i, j = 0; i < order; i++, j++) {
    new_node->pointers[j] = temp_pointers[i];
    new_node->keys[j] = temp_keys[i];
    new_node->num_keys++;
  }
  new_node->pointers[j] = temp_pointers[i];
  free(temp_pointers);
  free(temp_keys);
  new_node->parent = old_node->parent;
  for (i = 0; i <= new_node->num_keys; i++) {
    child = new_node->pointers[i];
    child->parent = new_node;
  }

  return insertIntoParent(root, old_node, k_prime, new_node);
}

node *insertIntoParent(node *root, node *left, int key, node *right) {
  int left_index;
  node *parent;

  parent = left->parent;

  if (parent == NULL)
    return insertIntoNewRoot(left, key, right);

  left_index = getLeftIndex(parent, left);

  if (parent->num_keys < order - 1)
    return insertIntoNode(root, parent, left_index, key, right);

  return insertIntoNodeAfterSplitting(root, parent, left_index, key, right);
}

node *insertIntoNewRoot(node *left, int key, node *right) {
  node *root = makeNode();
  root->keys[0] = key;
  root->pointers[0] = left;
  root->pointers[1] = right;
  root->num_keys++;
  root->parent = NULL;
  left->parent = root;
  right->parent = root;
  return root;
}

node *startNewTree(int key, record *pointer) {
  node *root = makeLeaf();
  root->keys[0] = key;
  root->pointers[0] = pointer;
  root->pointers[order - 1] = NULL;
  root->parent = NULL;
  root->num_keys++;
  return root;
}

node *insert(node *root, int key, int value) {
  record *record_pointer = NULL;
  node *leaf = NULL;

  record_pointer = find(root, key, false, NULL);
  if (record_pointer != NULL) {
    record_pointer->value = value;
    return root;
  }

  record_pointer = makeRecord(value);

  if (root == NULL)
    return startNewTree(key, record_pointer);

  leaf = findLeaf(root, key, false);

  if (leaf->num_keys < order - 1) {
    leaf = insertIntoLeaf(leaf, key, record_pointer);
    return root;
  }

  return insertIntoLeafAfterSplitting(root, leaf, key, record_pointer);
}

int main() {
  node *root;
  char instruction;

  root = NULL;

  root = insert(root, 5, 33);
  root = insert(root, 15, 21);
  root = insert(root, 25, 31);
  root = insert(root, 35, 41);
  root = insert(root, 45, 10);

  printTree(root);

  findAndPrint(root, 15, instruction = 'a');
}"
Deletion from a B+ Tree,"Deleting an element on a B+ tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required.Underflow is a situation when there is less number of keys in a node than the minimum number of keys it should hold. Before going through the steps below, one must know these facts about a B+ tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) While deleting a key, we have to take care of the keys present in the internal nodes (i.e. indexes) as well because the values are redundant in a B+ tree. Search the key to be deleted then follow the following steps. The key to be deleted is present only at the leaf node not in the indexes (or internal nodes). There are two cases for it: There is more than the minimum number of keys in the node. Simply delete the key.
		
			Deleting 40 from B-tree There is an exact minimum number of keys in the node. Delete the key and borrow a key from the immediate sibling. Add the median key of the sibling node to the parent.
		
			Deleting 5 from B-tree The key to be deleted is present in the internal nodes as well. Then we have to remove them from the internal nodes as well. There are the following cases for this situation. If there is more than the minimum number of keys in the node, simply delete the key from the leaf node and delete the key from the internal node as well.
		Fill the empty space in the internal node with the inorder successor.
		
			Deleting 45 from B-tree If there is an exact minimum number of keys in the node, then delete the key and borrow a key from its immediate sibling (through the parent).
		Fill the empty space created in the index (internal node) with the borrowed key.
		
			Deleting 35 from B-tree This case is similar to Case II(1) but here, empty space is generated above the immediate parent node.
		After deleting the key, merge the empty space with its sibling.
		Fill the empty space in the grandparent node with the inorder successor.
		
			Deleting 25 from B-tree  In this case, the height of the tree gets shrinked. It is a little complicated.Deleting 55 from the tree below leads to this condition. It can be understood in the illustrations below. Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n). ","// Deletion operation on a B+ Tree in C++

#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define DEFAULT_ORDER 3

typedef struct record {
  int value;
} record;

typedef struct node {
  void **pointers;
  int *keys;
  struct node *parent;
  bool is_leaf;
  int num_keys;
  struct node *next;
} node;

int order = DEFAULT_ORDER;
node *queue = NULL;
bool verbose_output = false;

void enqueue(node *new_node);
node *dequeue(void);
int height(node *const root);
int path_to_root(node *const root, node *child);
void print_leaves(node *const root);
void print_tree(node *const root);
void find_and_print(node *const root, int key, bool verbose);
void find_and_print_range(node *const root, int range1, int range2, bool verbose);
int find_range(node *const root, int key_start, int key_end, bool verbose,
         int returned_keys[], void *returned_pointers[]);
node *find_leaf(node *const root, int key, bool verbose);
record *find(node *root, int key, bool verbose, node **leaf_out);
int cut(int length);

record *make_record(int value);
node *make_node(void);
node *make_leaf(void);
int get_left_index(node *parent, node *left);
node *insert_into_leaf(node *leaf, int key, record *pointer);
node *insert_into_leaf_after_splitting(node *root, node *leaf, int key,
                     record *pointer);
node *insert_into_node(node *root, node *parent,
             int left_index, int key, node *right);
node *insert_into_node_after_splitting(node *root, node *parent,
                     int left_index,
                     int key, node *right);
node *insert_into_parent(node *root, node *left, int key, node *right);
node *insert_into_new_root(node *left, int key, node *right);
node *start_new_tree(int key, record *pointer);
node *insert(node *root, int key, int value);

int get_neighbor_index(node *n);
node *adjust_root(node *root);
node *coalesce_nodes(node *root, node *n, node *neighbor,
           int neighbor_index, int k_prime);
node *redistribute_nodes(node *root, node *n, node *neighbor,
             int neighbor_index,
             int k_prime_index, int k_prime);
node *delete_entry(node *root, node *n, int key, void *pointer);
node *delete (node *root, int key);

void enqueue(node *new_node) {
  node *c;
  if (queue == NULL) {
    queue = new_node;
    queue->next = NULL;
  } else {
    c = queue;
    while (c->next != NULL) {
      c = c->next;
    }
    c->next = new_node;
    new_node->next = NULL;
  }
}

node *dequeue(void) {
  node *n = queue;
  queue = queue->next;
  n->next = NULL;
  return n;
}

void print_leaves(node *const root) {
  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  int i;
  node *c = root;
  while (!c->is_leaf)
    c = c->pointers[0];
  while (true) {
    for (i = 0; i < c->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", c->pointers[i]);
      printf(""%d "", c->keys[i]);
    }
    if (verbose_output)
      printf(""%p "", c->pointers[order - 1]);
    if (c->pointers[order - 1] != NULL) {
      printf("" | "");
      c = c->pointers[order - 1];
    } else
      break;
  }
  printf(""\n"");
}

int height(node *const root) {
  int h = 0;
  node *c = root;
  while (!c->is_leaf) {
    c = c->pointers[0];
    h++;
  }
  return h;
}
int path_to_root(node *const root, node *child) {
  int length = 0;
  node *c = child;
  while (c != root) {
    c = c->parent;
    length++;
  }
  return length;
}

void print_tree(node *const root) {
  node *n = NULL;
  int i = 0;
  int rank = 0;
  int new_rank = 0;

  if (root == NULL) {
    printf(""Empty tree.\n"");
    return;
  }
  queue = NULL;
  enqueue(root);
  while (queue != NULL) {
    n = dequeue();
    if (n->parent != NULL && n == n->parent->pointers[0]) {
      new_rank = path_to_root(root, n);
      if (new_rank != rank) {
        rank = new_rank;
        printf(""\n"");
      }
    }
    if (verbose_output)
      printf(""(%p)"", n);
    for (i = 0; i < n->num_keys; i++) {
      if (verbose_output)
        printf(""%p "", n->pointers[i]);
      printf(""%d "", n->keys[i]);
    }
    if (!n->is_leaf)
      for (i = 0; i <= n->num_keys; i++)
        enqueue(n->pointers[i]);
    if (verbose_output) {
      if (n->is_leaf)
        printf(""%p "", n->pointers[order - 1]);
      else
        printf(""%p "", n->pointers[n->num_keys]);
    }
    printf(""| "");
  }
  printf(""\n"");
}

void find_and_print(node *const root, int key, bool verbose) {
  node *leaf = NULL;
  record *r = find(root, key, verbose, NULL);
  if (r == NULL)
    printf(""Record not found under key %d.\n"", key);
  else
    printf(""Record at %p -- key %d, value %d.\n"",
         r, key, r->value);
}

void find_and_print_range(node *const root, int key_start, int key_end,
              bool verbose) {
  int i;
  int array_size = key_end - key_start + 1;
  int returned_keys[array_size];
  void *returned_pointers[array_size];
  int num_found = find_range(root, key_start, key_end, verbose,
                 returned_keys, returned_pointers);
  if (!num_found)
    printf(""None found.\n"");
  else {
    for (i = 0; i < num_found; i++)
      printf(""Key: %d   Location: %p  Value: %d\n"",
           returned_keys[i],
           returned_pointers[i],
           ((record *)
            returned_pointers[i])
             ->value);
  }
}

int find_range(node *const root, int key_start, int key_end, bool verbose,
         int returned_keys[], void *returned_pointers[]) {
  int i, num_found;
  num_found = 0;
  node *n = find_leaf(root, key_start, verbose);
  if (n == NULL)
    return 0;
  for (i = 0; i < n->num_keys && n->keys[i] < key_start; i++)
    ;
  if (i == n->num_keys)
    return 0;
  while (n != NULL) {
    for (; i < n->num_keys && n->keys[i] <= key_end; i++) {
      returned_keys[num_found] = n->keys[i];
      returned_pointers[num_found] = n->pointers[i];
      num_found++;
    }
    n = n->pointers[order - 1];
    i = 0;
  }
  return num_found;
}

node *find_leaf(node *const root, int key, bool verbose) {
  if (root == NULL) {
    if (verbose)
      printf(""Empty tree.\n"");
    return root;
  }
  int i = 0;
  node *c = root;
  while (!c->is_leaf) {
    if (verbose) {
      printf(""["");
      for (i = 0; i < c->num_keys - 1; i++)
        printf(""%d "", c->keys[i]);
      printf(""%d] "", c->keys[i]);
    }
    i = 0;
    while (i < c->num_keys) {
      if (key >= c->keys[i])
        i++;
      else
        break;
    }
    if (verbose)
      printf(""%d ->\n"", i);
    c = (node *)c->pointers[i];
  }
  if (verbose) {
    printf(""Leaf ["");
    for (i = 0; i < c->num_keys - 1; i++)
      printf(""%d "", c->keys[i]);
    printf(""%d] ->\n"", c->keys[i]);
  }
  return c;
}

record *find(node *root, int key, bool verbose, node **leaf_out) {
  if (root == NULL) {
    if (leaf_out != NULL) {
      *leaf_out = NULL;
    }
    return NULL;
  }

  int i = 0;
  node *leaf = NULL;

  leaf = find_leaf(root, key, verbose);

  for (i = 0; i < leaf->num_keys; i++)
    if (leaf->keys[i] == key)
      break;
  if (leaf_out != NULL) {
    *leaf_out = leaf;
  }
  if (i == leaf->num_keys)
    return NULL;
  else
    return (record *)leaf->pointers[i];
}

int cut(int length) {
  if (length % 2 == 0)
    return length / 2;
  else
    return length / 2 + 1;
}

record *make_record(int value) {
  record *new_record = (record *)malloc(sizeof(record));
  if (new_record == NULL) {
    perror(""Record creation."");
    exit(EXIT_FAILURE);
  } else {
    new_record->value = value;
  }
  return new_record;
}

node *make_node(void) {
  node *new_node;
  new_node = malloc(sizeof(node));
  if (new_node == NULL) {
    perror(""Node creation."");
    exit(EXIT_FAILURE);
  }
  new_node->keys = malloc((order - 1) * sizeof(int));
  if (new_node->keys == NULL) {
    perror(""New node keys array."");
    exit(EXIT_FAILURE);
  }
  new_node->pointers = malloc(order * sizeof(void *));
  if (new_node->pointers == NULL) {
    perror(""New node pointers array."");
    exit(EXIT_FAILURE);
  }
  new_node->is_leaf = false;
  new_node->num_keys = 0;
  new_node->parent = NULL;
  new_node->next = NULL;
  return new_node;
}

node *make_leaf(void) {
  node *leaf = make_node();
  leaf->is_leaf = true;
  return leaf;
}

int get_left_index(node *parent, node *left) {
  int left_index = 0;
  while (left_index <= parent->num_keys &&
       parent->pointers[left_index] != left)
    left_index++;
  return left_index;
}

node *insert_into_leaf(node *leaf, int key, record *pointer) {
  int i, insertion_point;

  insertion_point = 0;
  while (insertion_point < leaf->num_keys && leaf->keys[insertion_point] < key)
    insertion_point++;

  for (i = leaf->num_keys; i > insertion_point; i--) {
    leaf->keys[i] = leaf->keys[i - 1];
    leaf->pointers[i] = leaf->pointers[i - 1];
  }
  leaf->keys[insertion_point] = key;
  leaf->pointers[insertion_point] = pointer;
  leaf->num_keys++;
  return leaf;
}

node *insert_into_leaf_after_splitting(node *root, node *leaf, int key, record *pointer) {
  node *new_leaf;
  int *temp_keys;
  void **temp_pointers;
  int insertion_index, split, new_key, i, j;

  new_leaf = make_leaf();

  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    perror(""Temporary keys array."");
    exit(EXIT_FAILURE);
  }

  temp_pointers = malloc(order * sizeof(void *));
  if (temp_pointers == NULL) {
    perror(""Temporary pointers array."");
    exit(EXIT_FAILURE);
  }

  insertion_index = 0;
  while (insertion_index < order - 1 && leaf->keys[insertion_index] < key)
    insertion_index++;

  for (i = 0, j = 0; i < leaf->num_keys; i++, j++) {
    if (j == insertion_index)
      j++;
    temp_keys[j] = leaf->keys[i];
    temp_pointers[j] = leaf->pointers[i];
  }

  temp_keys[insertion_index] = key;
  temp_pointers[insertion_index] = pointer;

  leaf->num_keys = 0;

  split = cut(order - 1);

  for (i = 0; i < split; i++) {
    leaf->pointers[i] = temp_pointers[i];
    leaf->keys[i] = temp_keys[i];
    leaf->num_keys++;
  }

  for (i = split, j = 0; i < order; i++, j++) {
    new_leaf->pointers[j] = temp_pointers[i];
    new_leaf->keys[j] = temp_keys[i];
    new_leaf->num_keys++;
  }

  free(temp_pointers);
  free(temp_keys);

  new_leaf->pointers[order - 1] = leaf->pointers[order - 1];
  leaf->pointers[order - 1] = new_leaf;

  for (i = leaf->num_keys; i < order - 1; i++)
    leaf->pointers[i] = NULL;
  for (i = new_leaf->num_keys; i < order - 1; i++)
    new_leaf->pointers[i] = NULL;

  new_leaf->parent = leaf->parent;
  new_key = new_leaf->keys[0];

  return insert_into_parent(root, leaf, new_key, new_leaf);
}

node *insert_into_node(node *root, node *n,
             int left_index, int key, node *right) {
  int i;

  for (i = n->num_keys; i > left_index; i--) {
    n->pointers[i + 1] = n->pointers[i];
    n->keys[i] = n->keys[i - 1];
  }
  n->pointers[left_index + 1] = right;
  n->keys[left_index] = key;
  n->num_keys++;
  return root;
}

node *insert_into_node_after_splitting(node *root, node *old_node, int left_index,
                     int key, node *right) {
  int i, j, split, k_prime;
  node *new_node, *child;
  int *temp_keys;
  node **temp_pointers;

  temp_pointers = malloc((order + 1) * sizeof(node *));
  if (temp_pointers == NULL) {
    perror(""Temporary pointers array for splitting nodes."");
    exit(EXIT_FAILURE);
  }
  temp_keys = malloc(order * sizeof(int));
  if (temp_keys == NULL) {
    perror(""Temporary keys array for splitting nodes."");
    exit(EXIT_FAILURE);
  }

  for (i = 0, j = 0; i < old_node->num_keys + 1; i++, j++) {
    if (j == left_index + 1)
      j++;
    temp_pointers[j] = old_node->pointers[i];
  }

  for (i = 0, j = 0; i < old_node->num_keys; i++, j++) {
    if (j == left_index)
      j++;
    temp_keys[j] = old_node->keys[i];
  }

  temp_pointers[left_index + 1] = right;
  temp_keys[left_index] = key;

  split = cut(order);
  new_node = make_node();
  old_node->num_keys = 0;
  for (i = 0; i < split - 1; i++) {
    old_node->pointers[i] = temp_pointers[i];
    old_node->keys[i] = temp_keys[i];
    old_node->num_keys++;
  }
  old_node->pointers[i] = temp_pointers[i];
  k_prime = temp_keys[split - 1];
  for (++i, j = 0; i < order; i++, j++) {
    new_node->pointers[j] = temp_pointers[i];
    new_node->keys[j] = temp_keys[i];
    new_node->num_keys++;
  }
  new_node->pointers[j] = temp_pointers[i];
  free(temp_pointers);
  free(temp_keys);
  new_node->parent = old_node->parent;
  for (i = 0; i <= new_node->num_keys; i++) {
    child = new_node->pointers[i];
    child->parent = new_node;
  }

  return insert_into_parent(root, old_node, k_prime, new_node);
}

node *insert_into_parent(node *root, node *left, int key, node *right) {
  int left_index;
  node *parent;

  parent = left->parent;

  if (parent == NULL)
    return insert_into_new_root(left, key, right);

  left_index = get_left_index(parent, left);

  if (parent->num_keys < order - 1)
    return insert_into_node(root, parent, left_index, key, right);

  return insert_into_node_after_splitting(root, parent, left_index, key, right);
}

node *insert_into_new_root(node *left, int key, node *right) {
  node *root = make_node();
  root->keys[0] = key;
  root->pointers[0] = left;
  root->pointers[1] = right;
  root->num_keys++;
  root->parent = NULL;
  left->parent = root;
  right->parent = root;
  return root;
}

node *start_new_tree(int key, record *pointer) {
  node *root = make_leaf();
  root->keys[0] = key;
  root->pointers[0] = pointer;
  root->pointers[order - 1] = NULL;
  root->parent = NULL;
  root->num_keys++;
  return root;
}

node *insert(node *root, int key, int value) {
  record *record_pointer = NULL;
  node *leaf = NULL;

  record_pointer = find(root, key, false, NULL);
  if (record_pointer != NULL) {
    record_pointer->value = value;
    return root;
  }

  record_pointer = make_record(value);

  if (root == NULL)
    return start_new_tree(key, record_pointer);

  leaf = find_leaf(root, key, false);

  if (leaf->num_keys < order - 1) {
    leaf = insert_into_leaf(leaf, key, record_pointer);
    return root;
  }

  return insert_into_leaf_after_splitting(root, leaf, key, record_pointer);
}

int get_neighbor_index(node *n) {
  int i;
  for (i = 0; i <= n->parent->num_keys; i++)
    if (n->parent->pointers[i] == n)
      return i - 1;

  printf(""Search for nonexistent pointer to node in parent.\n"");
  printf(""Node:  %#lx\n"", (unsigned long)n);
  exit(EXIT_FAILURE);
}

node *remove_entry_from_node(node *n, int key, node *pointer) {
  int i, num_pointers;
  i = 0;
  while (n->keys[i] != key)
    i++;
  for (++i; i < n->num_keys; i++)
    n->keys[i - 1] = n->keys[i];

  num_pointers = n->is_leaf ? n->num_keys : n->num_keys + 1;
  i = 0;
  while (n->pointers[i] != pointer)
    i++;
  for (++i; i < num_pointers; i++)
    n->pointers[i - 1] = n->pointers[i];

  n->num_keys--;

  if (n->is_leaf)
    for (i = n->num_keys; i < order - 1; i++)
      n->pointers[i] = NULL;
  else
    for (i = n->num_keys + 1; i < order; i++)
      n->pointers[i] = NULL;

  return n;
}

node *adjust_root(node *root) {
  node *new_root;

  if (root->num_keys > 0)
    return root;

  if (!root->is_leaf) {
    new_root = root->pointers[0];
    new_root->parent = NULL;
  }

  else
    new_root = NULL;

  free(root->keys);
  free(root->pointers);
  free(root);

  return new_root;
}

node *coalesce_nodes(node *root, node *n, node *neighbor, int neighbor_index, int k_prime) {
  int i, j, neighbor_insertion_index, n_end;
  node *tmp;

  if (neighbor_index == -1) {
    tmp = n;
    n = neighbor;
    neighbor = tmp;
  }

  neighbor_insertion_index = neighbor->num_keys;

  if (!n->is_leaf) {
    neighbor->keys[neighbor_insertion_index] = k_prime;
    neighbor->num_keys++;

    n_end = n->num_keys;

    for (i = neighbor_insertion_index + 1, j = 0; j < n_end; i++, j++) {
      neighbor->keys[i] = n->keys[j];
      neighbor->pointers[i] = n->pointers[j];
      neighbor->num_keys++;
      n->num_keys--;
    }

    neighbor->pointers[i] = n->pointers[j];

    for (i = 0; i < neighbor->num_keys + 1; i++) {
      tmp = (node *)neighbor->pointers[i];
      tmp->parent = neighbor;
    }
  }

  else {
    for (i = neighbor_insertion_index, j = 0; j < n->num_keys; i++, j++) {
      neighbor->keys[i] = n->keys[j];
      neighbor->pointers[i] = n->pointers[j];
      neighbor->num_keys++;
    }
    neighbor->pointers[order - 1] = n->pointers[order - 1];
  }

  root = delete_entry(root, n->parent, k_prime, n);
  free(n->keys);
  free(n->pointers);
  free(n);
  return root;
}

node *redistribute_nodes(node *root, node *n, node *neighbor, int neighbor_index,
             int k_prime_index, int k_prime) {
  int i;
  node *tmp;

  if (neighbor_index != -1) {
    if (!n->is_leaf)
      n->pointers[n->num_keys + 1] = n->pointers[n->num_keys];
    for (i = n->num_keys; i > 0; i--) {
      n->keys[i] = n->keys[i - 1];
      n->pointers[i] = n->pointers[i - 1];
    }
    if (!n->is_leaf) {
      n->pointers[0] = neighbor->pointers[neighbor->num_keys];
      tmp = (node *)n->pointers[0];
      tmp->parent = n;
      neighbor->pointers[neighbor->num_keys] = NULL;
      n->keys[0] = k_prime;
      n->parent->keys[k_prime_index] = neighbor->keys[neighbor->num_keys - 1];
    } else {
      n->pointers[0] = neighbor->pointers[neighbor->num_keys - 1];
      neighbor->pointers[neighbor->num_keys - 1] = NULL;
      n->keys[0] = neighbor->keys[neighbor->num_keys - 1];
      n->parent->keys[k_prime_index] = n->keys[0];
    }
  }

  else {
    if (n->is_leaf) {
      n->keys[n->num_keys] = neighbor->keys[0];
      n->pointers[n->num_keys] = neighbor->pointers[0];
      n->parent->keys[k_prime_index] = neighbor->keys[1];
    } else {
      n->keys[n->num_keys] = k_prime;
      n->pointers[n->num_keys + 1] = neighbor->pointers[0];
      tmp = (node *)n->pointers[n->num_keys + 1];
      tmp->parent = n;
      n->parent->keys[k_prime_index] = neighbor->keys[0];
    }
    for (i = 0; i < neighbor->num_keys - 1; i++) {
      neighbor->keys[i] = neighbor->keys[i + 1];
      neighbor->pointers[i] = neighbor->pointers[i + 1];
    }
    if (!n->is_leaf)
      neighbor->pointers[i] = neighbor->pointers[i + 1];
  }

  n->num_keys++;
  neighbor->num_keys--;

  return root;
}

node *delete_entry(node *root, node *n, int key, void *pointer) {
  int min_keys;
  node *neighbor;
  int neighbor_index;
  int k_prime_index, k_prime;
  int capacity;

  n = remove_entry_from_node(n, key, pointer);

  if (n == root)
    return adjust_root(root);

  min_keys = n->is_leaf ? cut(order - 1) : cut(order) - 1;

  if (n->num_keys >= min_keys)
    return root;

  neighbor_index = get_neighbor_index(n);
  k_prime_index = neighbor_index == -1 ? 0 : neighbor_index;
  k_prime = n->parent->keys[k_prime_index];
  neighbor = neighbor_index == -1 ? n->parent->pointers[1] : n->parent->pointers[neighbor_index];

  capacity = n->is_leaf ? order : order - 1;

  if (neighbor->num_keys + n->num_keys < capacity)
    return coalesce_nodes(root, n, neighbor, neighbor_index, k_prime);
  else
    return redistribute_nodes(root, n, neighbor, neighbor_index, k_prime_index, k_prime);
}

node *delete (node *root, int key) {
  node *key_leaf = NULL;
  record *key_record = NULL;

  key_record = find(root, key, false, &key_leaf);

  if (key_record != NULL && key_leaf != NULL) {
    root = delete_entry(root, key_leaf, key, key_record);
    free(key_record);
  }
  return root;
}

void destroy_tree_nodes(node *root) {
  int i;
  if (root->is_leaf)
    for (i = 0; i < root->num_keys; i++)
      free(root->pointers[i]);
  else
    for (i = 0; i < root->num_keys + 1; i++)
      destroy_tree_nodes(root->pointers[i]);
  free(root->pointers);
  free(root->keys);
  free(root);
}

node *destroy_tree(node *root) {
  destroy_tree_nodes(root);
  return NULL;
}

int main() {
  node *root;
  char instruction;

  root = NULL;

  root = insert(root, 5, 33);
  root = insert(root, 15, 21);
  root = insert(root, 25, 31);
  root = insert(root, 35, 41);
  root = insert(root, 45, 10);

  print_tree(root);

  root = delete (root, 5);

  print_tree(root);
}"
Deletion from a B+ Tree,"Deleting an element on a B+ tree consists of three main events: searching the node where the key to be deleted exists, deleting the key and balancing the tree if required.Underflow is a situation when there is less number of keys in a node than the minimum number of keys it should hold. Before going through the steps below, one must know these facts about a B+ tree of degree m. A node can have a maximum of m children. (i.e. 3) A node can contain a maximum of m - 1 keys. (i.e. 2) A node should have a minimum of ⌈m/2⌉ children. (i.e. 2) A node (except root node) should contain a minimum of ⌈m/2⌉ - 1 keys. (i.e. 1) While deleting a key, we have to take care of the keys present in the internal nodes (i.e. indexes) as well because the values are redundant in a B+ tree. Search the key to be deleted then follow the following steps. The key to be deleted is present only at the leaf node not in the indexes (or internal nodes). There are two cases for it: There is more than the minimum number of keys in the node. Simply delete the key.
		
			Deleting 40 from B-tree There is an exact minimum number of keys in the node. Delete the key and borrow a key from the immediate sibling. Add the median key of the sibling node to the parent.
		
			Deleting 5 from B-tree The key to be deleted is present in the internal nodes as well. Then we have to remove them from the internal nodes as well. There are the following cases for this situation. If there is more than the minimum number of keys in the node, simply delete the key from the leaf node and delete the key from the internal node as well.
		Fill the empty space in the internal node with the inorder successor.
		
			Deleting 45 from B-tree If there is an exact minimum number of keys in the node, then delete the key and borrow a key from its immediate sibling (through the parent).
		Fill the empty space created in the index (internal node) with the borrowed key.
		
			Deleting 35 from B-tree This case is similar to Case II(1) but here, empty space is generated above the immediate parent node.
		After deleting the key, merge the empty space with its sibling.
		Fill the empty space in the grandparent node with the inorder successor.
		
			Deleting 25 from B-tree  In this case, the height of the tree gets shrinked. It is a little complicated.Deleting 55 from the tree below leads to this condition. It can be understood in the illustrations below. Time complexity: Θ(t.logt n) The complexity is dominated by Θ(logt n). ","// Deletion operation on a B+ tree in C++

#include <climits>
#include <fstream>
#include <iostream>
#include <sstream>
using namespace std;
int MAX = 3;

class BPTree;
class Node {
  bool IS_LEAF;
  int *key, size;
  Node **ptr;
  friend class BPTree;

   public:
  Node();
};
class BPTree {
  Node *root;
  void insertInternal(int, Node *, Node *);
  void removeInternal(int, Node *, Node *);
  Node *findParent(Node *, Node *);

   public:
  BPTree();
  void search(int);
  void insert(int);
  void remove(int);
  void display(Node *);
  Node *getRoot();
};
Node::Node() {
  key = new int[MAX];
  ptr = new Node *[MAX + 1];
}
BPTree::BPTree() {
  root = NULL;
}
void BPTree::insert(int x) {
  if (root == NULL) {
    root = new Node;
    root->key[0] = x;
    root->IS_LEAF = true;
    root->size = 1;
  } else {
    Node *cursor = root;
    Node *parent;
    while (cursor->IS_LEAF == false) {
      parent = cursor;
      for (int i = 0; i < cursor->size; i++) {
        if (x < cursor->key[i]) {
          cursor = cursor->ptr[i];
          break;
        }
        if (i == cursor->size - 1) {
          cursor = cursor->ptr[i + 1];
          break;
        }
      }
    }
    if (cursor->size < MAX) {
      int i = 0;
      while (x > cursor->key[i] && i < cursor->size)
        i++;
      for (int j = cursor->size; j > i; j--) {
        cursor->key[j] = cursor->key[j - 1];
      }
      cursor->key[i] = x;
      cursor->size++;
      cursor->ptr[cursor->size] = cursor->ptr[cursor->size - 1];
      cursor->ptr[cursor->size - 1] = NULL;
    } else {
      Node *newLeaf = new Node;
      int virtualNode[MAX + 1];
      for (int i = 0; i < MAX; i++) {
        virtualNode[i] = cursor->key[i];
      }
      int i = 0, j;
      while (x > virtualNode[i] && i < MAX)
        i++;
      for (int j = MAX + 1; j > i; j--) {
        virtualNode[j] = virtualNode[j - 1];
      }
      virtualNode[i] = x;
      newLeaf->IS_LEAF = true;
      cursor->size = (MAX + 1) / 2;
      newLeaf->size = MAX + 1 - (MAX + 1) / 2;
      cursor->ptr[cursor->size] = newLeaf;
      newLeaf->ptr[newLeaf->size] = cursor->ptr[MAX];
      cursor->ptr[MAX] = NULL;
      for (i = 0; i < cursor->size; i++) {
        cursor->key[i] = virtualNode[i];
      }
      for (i = 0, j = cursor->size; i < newLeaf->size; i++, j++) {
        newLeaf->key[i] = virtualNode[j];
      }
      if (cursor == root) {
        Node *newRoot = new Node;
        newRoot->key[0] = newLeaf->key[0];
        newRoot->ptr[0] = cursor;
        newRoot->ptr[1] = newLeaf;
        newRoot->IS_LEAF = false;
        newRoot->size = 1;
        root = newRoot;
      } else {
        insertInternal(newLeaf->key[0], parent, newLeaf);
      }
    }
  }
}
void BPTree::insertInternal(int x, Node *cursor, Node *child) {
  if (cursor->size < MAX) {
    int i = 0;
    while (x > cursor->key[i] && i < cursor->size)
      i++;
    for (int j = cursor->size; j > i; j--) {
      cursor->key[j] = cursor->key[j - 1];
    }
    for (int j = cursor->size + 1; j > i + 1; j--) {
      cursor->ptr[j] = cursor->ptr[j - 1];
    }
    cursor->key[i] = x;
    cursor->size++;
    cursor->ptr[i + 1] = child;
  } else {
    Node *newInternal = new Node;
    int virtualKey[MAX + 1];
    Node *virtualPtr[MAX + 2];
    for (int i = 0; i < MAX; i++) {
      virtualKey[i] = cursor->key[i];
    }
    for (int i = 0; i < MAX + 1; i++) {
      virtualPtr[i] = cursor->ptr[i];
    }
    int i = 0, j;
    while (x > virtualKey[i] && i < MAX)
      i++;
    for (int j = MAX + 1; j > i; j--) {
      virtualKey[j] = virtualKey[j - 1];
    }
    virtualKey[i] = x;
    for (int j = MAX + 2; j > i + 1; j--) {
      virtualPtr[j] = virtualPtr[j - 1];
    }
    virtualPtr[i + 1] = child;
    newInternal->IS_LEAF = false;
    cursor->size = (MAX + 1) / 2;
    newInternal->size = MAX - (MAX + 1) / 2;
    for (i = 0, j = cursor->size + 1; i < newInternal->size; i++, j++) {
      newInternal->key[i] = virtualKey[j];
    }
    for (i = 0, j = cursor->size + 1; i < newInternal->size + 1; i++, j++) {
      newInternal->ptr[i] = virtualPtr[j];
    }
    if (cursor == root) {
      Node *newRoot = new Node;
      newRoot->key[0] = cursor->key[cursor->size];
      newRoot->ptr[0] = cursor;
      newRoot->ptr[1] = newInternal;
      newRoot->IS_LEAF = false;
      newRoot->size = 1;
      root = newRoot;
    } else {
      insertInternal(cursor->key[cursor->size], findParent(root, cursor), newInternal);
    }
  }
}
Node *BPTree::findParent(Node *cursor, Node *child) {
  Node *parent;
  if (cursor->IS_LEAF || (cursor->ptr[0])->IS_LEAF) {
    return NULL;
  }
  for (int i = 0; i < cursor->size + 1; i++) {
    if (cursor->ptr[i] == child) {
      parent = cursor;
      return parent;
    } else {
      parent = findParent(cursor->ptr[i], child);
      if (parent != NULL)
        return parent;
    }
  }
  return parent;
}
void BPTree::remove(int x) {
  if (root == NULL) {
    cout << ""Tree empty\n"";
  } else {
    Node *cursor = root;
    Node *parent;
    int leftSibling, rightSibling;
    while (cursor->IS_LEAF == false) {
      for (int i = 0; i < cursor->size; i++) {
        parent = cursor;
        leftSibling = i - 1;
        rightSibling = i + 1;
        if (x < cursor->key[i]) {
          cursor = cursor->ptr[i];
          break;
        }
        if (i == cursor->size - 1) {
          leftSibling = i;
          rightSibling = i + 2;
          cursor = cursor->ptr[i + 1];
          break;
        }
      }
    }
    bool found = false;
    int pos;
    for (pos = 0; pos < cursor->size; pos++) {
      if (cursor->key[pos] == x) {
        found = true;
        break;
      }
    }
    if (!found) {
      cout << ""Not found\n"";
      return;
    }
    for (int i = pos; i < cursor->size; i++) {
      cursor->key[i] = cursor->key[i + 1];
    }
    cursor->size--;
    if (cursor == root) {
      for (int i = 0; i < MAX + 1; i++) {
        cursor->ptr[i] = NULL;
      }
      if (cursor->size == 0) {
        cout << ""Tree died\n"";
        delete[] cursor->key;
        delete[] cursor->ptr;
        delete cursor;
        root = NULL;
      }
      return;
    }
    cursor->ptr[cursor->size] = cursor->ptr[cursor->size + 1];
    cursor->ptr[cursor->size + 1] = NULL;
    if (cursor->size >= (MAX + 1) / 2) {
      return;
    }
    if (leftSibling >= 0) {
      Node *leftNode = parent->ptr[leftSibling];
      if (leftNode->size >= (MAX + 1) / 2 + 1) {
        for (int i = cursor->size; i > 0; i--) {
          cursor->key[i] = cursor->key[i - 1];
        }
        cursor->size++;
        cursor->ptr[cursor->size] = cursor->ptr[cursor->size - 1];
        cursor->ptr[cursor->size - 1] = NULL;
        cursor->key[0] = leftNode->key[leftNode->size - 1];
        leftNode->size--;
        leftNode->ptr[leftNode->size] = cursor;
        leftNode->ptr[leftNode->size + 1] = NULL;
        parent->key[leftSibling] = cursor->key[0];
        return;
      }
    }
    if (rightSibling <= parent->size) {
      Node *rightNode = parent->ptr[rightSibling];
      if (rightNode->size >= (MAX + 1) / 2 + 1) {
        cursor->size++;
        cursor->ptr[cursor->size] = cursor->ptr[cursor->size - 1];
        cursor->ptr[cursor->size - 1] = NULL;
        cursor->key[cursor->size - 1] = rightNode->key[0];
        rightNode->size--;
        rightNode->ptr[rightNode->size] = rightNode->ptr[rightNode->size + 1];
        rightNode->ptr[rightNode->size + 1] = NULL;
        for (int i = 0; i < rightNode->size; i++) {
          rightNode->key[i] = rightNode->key[i + 1];
        }
        parent->key[rightSibling - 1] = rightNode->key[0];
        return;
      }
    }
    if (leftSibling >= 0) {
      Node *leftNode = parent->ptr[leftSibling];
      for (int i = leftNode->size, j = 0; j < cursor->size; i++, j++) {
        leftNode->key[i] = cursor->key[j];
      }
      leftNode->ptr[leftNode->size] = NULL;
      leftNode->size += cursor->size;
      leftNode->ptr[leftNode->size] = cursor->ptr[cursor->size];
      removeInternal(parent->key[leftSibling], parent, cursor);
      delete[] cursor->key;
      delete[] cursor->ptr;
      delete cursor;
    } else if (rightSibling <= parent->size) {
      Node *rightNode = parent->ptr[rightSibling];
      for (int i = cursor->size, j = 0; j < rightNode->size; i++, j++) {
        cursor->key[i] = rightNode->key[j];
      }
      cursor->ptr[cursor->size] = NULL;
      cursor->size += rightNode->size;
      cursor->ptr[cursor->size] = rightNode->ptr[rightNode->size];
      cout << ""Merging two leaf nodes\n"";
      removeInternal(parent->key[rightSibling - 1], parent, rightNode);
      delete[] rightNode->key;
      delete[] rightNode->ptr;
      delete rightNode;
    }
  }
}
void BPTree::removeInternal(int x, Node *cursor, Node *child) {
  if (cursor == root) {
    if (cursor->size == 1) {
      if (cursor->ptr[1] == child) {
        delete[] child->key;
        delete[] child->ptr;
        delete child;
        root = cursor->ptr[0];
        delete[] cursor->key;
        delete[] cursor->ptr;
        delete cursor;
        cout << ""Changed root node\n"";
        return;
      } else if (cursor->ptr[0] == child) {
        delete[] child->key;
        delete[] child->ptr;
        delete child;
        root = cursor->ptr[1];
        delete[] cursor->key;
        delete[] cursor->ptr;
        delete cursor;
        cout << ""Changed root node\n"";
        return;
      }
    }
  }
  int pos;
  for (pos = 0; pos < cursor->size; pos++) {
    if (cursor->key[pos] == x) {
      break;
    }
  }
  for (int i = pos; i < cursor->size; i++) {
    cursor->key[i] = cursor->key[i + 1];
  }
  for (pos = 0; pos < cursor->size + 1; pos++) {
    if (cursor->ptr[pos] == child) {
      break;
    }
  }
  for (int i = pos; i < cursor->size + 1; i++) {
    cursor->ptr[i] = cursor->ptr[i + 1];
  }
  cursor->size--;
  if (cursor->size >= (MAX + 1) / 2 - 1) {
    return;
  }
  if (cursor == root)
    return;
  Node *parent = findParent(root, cursor);
  int leftSibling, rightSibling;
  for (pos = 0; pos < parent->size + 1; pos++) {
    if (parent->ptr[pos] == cursor) {
      leftSibling = pos - 1;
      rightSibling = pos + 1;
      break;
    }
  }
  if (leftSibling >= 0) {
    Node *leftNode = parent->ptr[leftSibling];
    if (leftNode->size >= (MAX + 1) / 2) {
      for (int i = cursor->size; i > 0; i--) {
        cursor->key[i] = cursor->key[i - 1];
      }
      cursor->key[0] = parent->key[leftSibling];
      parent->key[leftSibling] = leftNode->key[leftNode->size - 1];
      for (int i = cursor->size + 1; i > 0; i--) {
        cursor->ptr[i] = cursor->ptr[i - 1];
      }
      cursor->ptr[0] = leftNode->ptr[leftNode->size];
      cursor->size++;
      leftNode->size--;
      return;
    }
  }
  if (rightSibling <= parent->size) {
    Node *rightNode = parent->ptr[rightSibling];
    if (rightNode->size >= (MAX + 1) / 2) {
      cursor->key[cursor->size] = parent->key[pos];
      parent->key[pos] = rightNode->key[0];
      for (int i = 0; i < rightNode->size - 1; i++) {
        rightNode->key[i] = rightNode->key[i + 1];
      }
      cursor->ptr[cursor->size + 1] = rightNode->ptr[0];
      for (int i = 0; i < rightNode->size; ++i) {
        rightNode->ptr[i] = rightNode->ptr[i + 1];
      }
      cursor->size++;
      rightNode->size--;
      return;
    }
  }
  if (leftSibling >= 0) {
    Node *leftNode = parent->ptr[leftSibling];
    leftNode->key[leftNode->size] = parent->key[leftSibling];
    for (int i = leftNode->size + 1, j = 0; j < cursor->size; j++) {
      leftNode->key[i] = cursor->key[j];
    }
    for (int i = leftNode->size + 1, j = 0; j < cursor->size + 1; j++) {
      leftNode->ptr[i] = cursor->ptr[j];
      cursor->ptr[j] = NULL;
    }
    leftNode->size += cursor->size + 1;
    cursor->size = 0;
    removeInternal(parent->key[leftSibling], parent, cursor);
  } else if (rightSibling <= parent->size) {
    Node *rightNode = parent->ptr[rightSibling];
    cursor->key[cursor->size] = parent->key[rightSibling - 1];
    for (int i = cursor->size + 1, j = 0; j < rightNode->size; j++) {
      cursor->key[i] = rightNode->key[j];
    }
    for (int i = cursor->size + 1, j = 0; j < rightNode->size + 1; j++) {
      cursor->ptr[i] = rightNode->ptr[j];
      rightNode->ptr[j] = NULL;
    }
    cursor->size += rightNode->size + 1;
    rightNode->size = 0;
    removeInternal(parent->key[rightSibling - 1], parent, rightNode);
  }
}
void BPTree::display(Node *cursor) {
  if (cursor != NULL) {
    for (int i = 0; i < cursor->size; i++) {
      cout << cursor->key[i] << "" "";
    }
    cout << ""\n"";
    if (cursor->IS_LEAF != true) {
      for (int i = 0; i < cursor->size + 1; i++) {
        display(cursor->ptr[i]);
      }
    }
  }
}
Node *BPTree::getRoot() {
  return root;
}

int main() {
  BPTree node;
  node.insert(5);
  node.insert(15);
  node.insert(25);
  node.insert(35);
  node.insert(45);

  node.display(node.getRoot());

  node.remove(15);

  node.display(node.getRoot());
}
"
Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. A red-black tree satisfies the following properties: Red/Black Property: Every node is colored, either red or black. Root Property: The root is black. Leaf Property: Every leaf (NIL) is black. Red Property: If a red node has children then, the children are always black. Depth Property: For each node, any simple path from this node to any of its descendant leaf has the same black-depth (the number of black nodes). An example of a red-black tree is: Each node has the following attributes: color
	key
	leftChild
	rightChild
	parent (except root node) color key leftChild rightChild parent (except root node) How the red-black tree maintains the property of self-balancing? The red-black color is meant for balancing the tree. The limitations put on the node colors ensure that any simple path from the root to a leaf is not more than twice as long as any other such path. It helps in maintaining the self-balancing property of the red-black tree. Various operations that can be performed on a red-black tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. Rotation operation is used for maintaining the properties of a red-black tree when they are violated by other operations such as insertion and deletion. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node.  Algorithm Let the initial tree be:
		
			Initial tree If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In right-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial Tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: Let y be the leaf (ie. NIL) and x be the root of the tree. Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree. Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree. Assign the parent of the leaf as a parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild. Assign NULL to the left and rightChild of newNode. Assign RED color to newNode. Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree? This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if the insertion of a newNode violates this property. Do the following while the parent of newNode p is RED. If p is the left child of grandParent gP of z, do the following.
		Case-I:
		
			If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
				Case-II:
			Else if newNode is the right child of p then, assign p to newNode.
			Left-Rotate newNode.
				Case-III:
			Set color of p as BLACK and color of gP as RED.
			Right-Rotate gP. If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode.
				Case-II: Else if newNode is the right child of p then, assign p to newNode. Left-Rotate newNode.
				Case-III: Set color of p as BLACK and color of gP as RED. Right-Rotate gP. Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. Set the root of the tree as BLACK. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Save the color of nodeToBeDeleted in origrinalColor. If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
			Transplant nodeToBeDeleted with x. Assign the right child of nodeToBeDeleted to x. Transplant nodeToBeDeleted with x. Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties. However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recoloring are performed. The following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
			If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w.
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w.
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Assign w to the sibling of x. If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w. Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED. Left-Rotate the parent of x. Assign the rightChild of the parent of x to w. If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w. Set the color of the leftChild of w as BLACK Set the color of w as RED Right-Rotate w. Assign the rightChild of the parent of x to w. If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Set the color of w as the color of the parent of x. Set the color of the parent of x as BLACK. Set the color of the right child of w as BLACK. Left-Rotate the parent of x. Set x as the root of the tree. Else the same as above with right changed to left and vice versa. Set the color of x as BLACK. Please refer to insertion and deletion operations for more explanation with examples. To implement finite maps To implement Java packages: java.util.TreeMap and java.util.TreeSet To implement Standard Template Libraries (STL) in C++: multiset, map, multimap In Linux Kernel","# Implementing Red-Black Tree in Python


import sys


# Node creation
class Node():
    def __init__(self, item):
        self.item = item
        self.parent = None
        self.left = None
        self.right = None
        self.color = 1


class RedBlackTree():
    def __init__(self):
        self.TNULL = Node(0)
        self.TNULL.color = 0
        self.TNULL.left = None
        self.TNULL.right = None
        self.root = self.TNULL

    # Preorder
    def pre_order_helper(self, node):
        if node != TNULL:
            sys.stdout.write(node.item + "" "")
            self.pre_order_helper(node.left)
            self.pre_order_helper(node.right)

    # Inorder
    def in_order_helper(self, node):
        if node != TNULL:
            self.in_order_helper(node.left)
            sys.stdout.write(node.item + "" "")
            self.in_order_helper(node.right)

    # Postorder
    def post_order_helper(self, node):
        if node != TNULL:
            self.post_order_helper(node.left)
            self.post_order_helper(node.right)
            sys.stdout.write(node.item + "" "")

    # Search the tree
    def search_tree_helper(self, node, key):
        if node == TNULL or key == node.item:
            return node

        if key < node.item:
            return self.search_tree_helper(node.left, key)
        return self.search_tree_helper(node.right, key)

    # Balancing the tree after deletion
    def delete_fix(self, x):
        while x != self.root and x.color == 0:
            if x == x.parent.left:
                s = x.parent.right
                if s.color == 1:
                    s.color = 0
                    x.parent.color = 1
                    self.left_rotate(x.parent)
                    s = x.parent.right

                if s.left.color == 0 and s.right.color == 0:
                    s.color = 1
                    x = x.parent
                else:
                    if s.right.color == 0:
                        s.left.color = 0
                        s.color = 1
                        self.right_rotate(s)
                        s = x.parent.right

                    s.color = x.parent.color
                    x.parent.color = 0
                    s.right.color = 0
                    self.left_rotate(x.parent)
                    x = self.root
            else:
                s = x.parent.left
                if s.color == 1:
                    s.color = 0
                    x.parent.color = 1
                    self.right_rotate(x.parent)
                    s = x.parent.left

                if s.right.color == 0 and s.right.color == 0:
                    s.color = 1
                    x = x.parent
                else:
                    if s.left.color == 0:
                        s.right.color = 0
                        s.color = 1
                        self.left_rotate(s)
                        s = x.parent.left

                    s.color = x.parent.color
                    x.parent.color = 0
                    s.left.color = 0
                    self.right_rotate(x.parent)
                    x = self.root
        x.color = 0

    def __rb_transplant(self, u, v):
        if u.parent == None:
            self.root = v
        elif u == u.parent.left:
            u.parent.left = v
        else:
            u.parent.right = v
        v.parent = u.parent

    # Node deletion
    def delete_node_helper(self, node, key):
        z = self.TNULL
        while node != self.TNULL:
            if node.item == key:
                z = node

            if node.item <= key:
                node = node.right
            else:
                node = node.left

        if z == self.TNULL:
            print(""Cannot find key in the tree"")
            return

        y = z
        y_original_color = y.color
        if z.left == self.TNULL:
            x = z.right
            self.__rb_transplant(z, z.right)
        elif (z.right == self.TNULL):
            x = z.left
            self.__rb_transplant(z, z.left)
        else:
            y = self.minimum(z.right)
            y_original_color = y.color
            x = y.right
            if y.parent == z:
                x.parent = y
            else:
                self.__rb_transplant(y, y.right)
                y.right = z.right
                y.right.parent = y

            self.__rb_transplant(z, y)
            y.left = z.left
            y.left.parent = y
            y.color = z.color
        if y_original_color == 0:
            self.delete_fix(x)

    # Balance the tree after insertion
    def fix_insert(self, k):
        while k.parent.color == 1:
            if k.parent == k.parent.parent.right:
                u = k.parent.parent.left
                if u.color == 1:
                    u.color = 0
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    k = k.parent.parent
                else:
                    if k == k.parent.left:
                        k = k.parent
                        self.right_rotate(k)
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    self.left_rotate(k.parent.parent)
            else:
                u = k.parent.parent.right

                if u.color == 1:
                    u.color = 0
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    k = k.parent.parent
                else:
                    if k == k.parent.right:
                        k = k.parent
                        self.left_rotate(k)
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    self.right_rotate(k.parent.parent)
            if k == self.root:
                break
        self.root.color = 0

    # Printing the tree
    def __print_helper(self, node, indent, last):
        if node != self.TNULL:
            sys.stdout.write(indent)
            if last:
                sys.stdout.write(""R----"")
                indent += ""     ""
            else:
                sys.stdout.write(""L----"")
                indent += ""|    ""

            s_color = ""RED"" if node.color == 1 else ""BLACK""
            print(str(node.item) + ""("" + s_color + "")"")
            self.__print_helper(node.left, indent, False)
            self.__print_helper(node.right, indent, True)

    def preorder(self):
        self.pre_order_helper(self.root)

    def inorder(self):
        self.in_order_helper(self.root)

    def postorder(self):
        self.post_order_helper(self.root)

    def searchTree(self, k):
        return self.search_tree_helper(self.root, k)

    def minimum(self, node):
        while node.left != self.TNULL:
            node = node.left
        return node

    def maximum(self, node):
        while node.right != self.TNULL:
            node = node.right
        return node

    def successor(self, x):
        if x.right != self.TNULL:
            return self.minimum(x.right)

        y = x.parent
        while y != self.TNULL and x == y.right:
            x = y
            y = y.parent
        return y

    def predecessor(self,  x):
        if (x.left != self.TNULL):
            return self.maximum(x.left)

        y = x.parent
        while y != self.TNULL and x == y.left:
            x = y
            y = y.parent

        return y

    def left_rotate(self, x):
        y = x.right
        x.right = y.left
        if y.left != self.TNULL:
            y.left.parent = x

        y.parent = x.parent
        if x.parent == None:
            self.root = y
        elif x == x.parent.left:
            x.parent.left = y
        else:
            x.parent.right = y
        y.left = x
        x.parent = y

    def right_rotate(self, x):
        y = x.left
        x.left = y.right
        if y.right != self.TNULL:
            y.right.parent = x

        y.parent = x.parent
        if x.parent == None:
            self.root = y
        elif x == x.parent.right:
            x.parent.right = y
        else:
            x.parent.left = y
        y.right = x
        x.parent = y

    def insert(self, key):
        node = Node(key)
        node.parent = None
        node.item = key
        node.left = self.TNULL
        node.right = self.TNULL
        node.color = 1

        y = None
        x = self.root

        while x != self.TNULL:
            y = x
            if node.item < x.item:
                x = x.left
            else:
                x = x.right

        node.parent = y
        if y == None:
            self.root = node
        elif node.item < y.item:
            y.left = node
        else:
            y.right = node

        if node.parent == None:
            node.color = 0
            return

        if node.parent.parent == None:
            return

        self.fix_insert(node)

    def get_root(self):
        return self.root

    def delete_node(self, item):
        self.delete_node_helper(self.root, item)

    def print_tree(self):
        self.__print_helper(self.root, """", True)


if __name__ == ""__main__"":
    bst = RedBlackTree()

    bst.insert(55)
    bst.insert(40)
    bst.insert(65)
    bst.insert(60)
    bst.insert(75)
    bst.insert(57)

    bst.print_tree()

    print(""\nAfter deleting an element"")
    bst.delete_node(40)
    bst.print_tree()
"
Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. A red-black tree satisfies the following properties: Red/Black Property: Every node is colored, either red or black. Root Property: The root is black. Leaf Property: Every leaf (NIL) is black. Red Property: If a red node has children then, the children are always black. Depth Property: For each node, any simple path from this node to any of its descendant leaf has the same black-depth (the number of black nodes). An example of a red-black tree is: Each node has the following attributes: color
	key
	leftChild
	rightChild
	parent (except root node) color key leftChild rightChild parent (except root node) How the red-black tree maintains the property of self-balancing? The red-black color is meant for balancing the tree. The limitations put on the node colors ensure that any simple path from the root to a leaf is not more than twice as long as any other such path. It helps in maintaining the self-balancing property of the red-black tree. Various operations that can be performed on a red-black tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. Rotation operation is used for maintaining the properties of a red-black tree when they are violated by other operations such as insertion and deletion. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node.  Algorithm Let the initial tree be:
		
			Initial tree If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In right-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial Tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: Let y be the leaf (ie. NIL) and x be the root of the tree. Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree. Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree. Assign the parent of the leaf as a parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild. Assign NULL to the left and rightChild of newNode. Assign RED color to newNode. Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree? This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if the insertion of a newNode violates this property. Do the following while the parent of newNode p is RED. If p is the left child of grandParent gP of z, do the following.
		Case-I:
		
			If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
				Case-II:
			Else if newNode is the right child of p then, assign p to newNode.
			Left-Rotate newNode.
				Case-III:
			Set color of p as BLACK and color of gP as RED.
			Right-Rotate gP. If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode.
				Case-II: Else if newNode is the right child of p then, assign p to newNode. Left-Rotate newNode.
				Case-III: Set color of p as BLACK and color of gP as RED. Right-Rotate gP. Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. Set the root of the tree as BLACK. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Save the color of nodeToBeDeleted in origrinalColor. If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
			Transplant nodeToBeDeleted with x. Assign the right child of nodeToBeDeleted to x. Transplant nodeToBeDeleted with x. Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties. However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recoloring are performed. The following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
			If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w.
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w.
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Assign w to the sibling of x. If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w. Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED. Left-Rotate the parent of x. Assign the rightChild of the parent of x to w. If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w. Set the color of the leftChild of w as BLACK Set the color of w as RED Right-Rotate w. Assign the rightChild of the parent of x to w. If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Set the color of w as the color of the parent of x. Set the color of the parent of x as BLACK. Set the color of the right child of w as BLACK. Left-Rotate the parent of x. Set x as the root of the tree. Else the same as above with right changed to left and vice versa. Set the color of x as BLACK. Please refer to insertion and deletion operations for more explanation with examples. To implement finite maps To implement Java packages: java.util.TreeMap and java.util.TreeSet To implement Standard Template Libraries (STL) in C++: multiset, map, multimap In Linux Kernel","// Implementing Red-Black Tree in Java

class Node {
  int data;
  Node parent;
  Node left;
  Node right;
  int color;
}

public class RedBlackTree {
  private Node root;
  private Node TNULL;

  // Preorder
  private void preOrderHelper(Node node) {
    if (node != TNULL) {
      System.out.print(node.data + "" "");
      preOrderHelper(node.left);
      preOrderHelper(node.right);
    }
  }

  // Inorder
  private void inOrderHelper(Node node) {
    if (node != TNULL) {
      inOrderHelper(node.left);
      System.out.print(node.data + "" "");
      inOrderHelper(node.right);
    }
  }

  // Post order
  private void postOrderHelper(Node node) {
    if (node != TNULL) {
      postOrderHelper(node.left);
      postOrderHelper(node.right);
      System.out.print(node.data + "" "");
    }
  }

  // Search the tree
  private Node searchTreeHelper(Node node, int key) {
    if (node == TNULL || key == node.data) {
      return node;
    }

    if (key < node.data) {
      return searchTreeHelper(node.left, key);
    }
    return searchTreeHelper(node.right, key);
  }

  // Balance the tree after deletion of a node
  private void fixDelete(Node x) {
    Node s;
    while (x != root && x.color == 0) {
      if (x == x.parent.left) {
        s = x.parent.right;
        if (s.color == 1) {
          s.color = 0;
          x.parent.color = 1;
          leftRotate(x.parent);
          s = x.parent.right;
        }

        if (s.left.color == 0 && s.right.color == 0) {
          s.color = 1;
          x = x.parent;
        } else {
          if (s.right.color == 0) {
            s.left.color = 0;
            s.color = 1;
            rightRotate(s);
            s = x.parent.right;
          }

          s.color = x.parent.color;
          x.parent.color = 0;
          s.right.color = 0;
          leftRotate(x.parent);
          x = root;
        }
      } else {
        s = x.parent.left;
        if (s.color == 1) {
          s.color = 0;
          x.parent.color = 1;
          rightRotate(x.parent);
          s = x.parent.left;
        }

        if (s.right.color == 0 && s.right.color == 0) {
          s.color = 1;
          x = x.parent;
        } else {
          if (s.left.color == 0) {
            s.right.color = 0;
            s.color = 1;
            leftRotate(s);
            s = x.parent.left;
          }

          s.color = x.parent.color;
          x.parent.color = 0;
          s.left.color = 0;
          rightRotate(x.parent);
          x = root;
        }
      }
    }
    x.color = 0;
  }

  private void rbTransplant(Node u, Node v) {
    if (u.parent == null) {
      root = v;
    } else if (u == u.parent.left) {
      u.parent.left = v;
    } else {
      u.parent.right = v;
    }
    v.parent = u.parent;
  }

  private void deleteNodeHelper(Node node, int key) {
    Node z = TNULL;
    Node x, y;
    while (node != TNULL) {
      if (node.data == key) {
        z = node;
      }

      if (node.data <= key) {
        node = node.right;
      } else {
        node = node.left;
      }
    }

    if (z == TNULL) {
      System.out.println(""Couldn't find key in the tree"");
      return;
    }

    y = z;
    int yOriginalColor = y.color;
    if (z.left == TNULL) {
      x = z.right;
      rbTransplant(z, z.right);
    } else if (z.right == TNULL) {
      x = z.left;
      rbTransplant(z, z.left);
    } else {
      y = minimum(z.right);
      yOriginalColor = y.color;
      x = y.right;
      if (y.parent == z) {
        x.parent = y;
      } else {
        rbTransplant(y, y.right);
        y.right = z.right;
        y.right.parent = y;
      }

      rbTransplant(z, y);
      y.left = z.left;
      y.left.parent = y;
      y.color = z.color;
    }
    if (yOriginalColor == 0) {
      fixDelete(x);
    }
  }

  // Balance the node after insertion
  private void fixInsert(Node k) {
    Node u;
    while (k.parent.color == 1) {
      if (k.parent == k.parent.parent.right) {
        u = k.parent.parent.left;
        if (u.color == 1) {
          u.color = 0;
          k.parent.color = 0;
          k.parent.parent.color = 1;
          k = k.parent.parent;
        } else {
          if (k == k.parent.left) {
            k = k.parent;
            rightRotate(k);
          }
          k.parent.color = 0;
          k.parent.parent.color = 1;
          leftRotate(k.parent.parent);
        }
      } else {
        u = k.parent.parent.right;

        if (u.color == 1) {
          u.color = 0;
          k.parent.color = 0;
          k.parent.parent.color = 1;
          k = k.parent.parent;
        } else {
          if (k == k.parent.right) {
            k = k.parent;
            leftRotate(k);
          }
          k.parent.color = 0;
          k.parent.parent.color = 1;
          rightRotate(k.parent.parent);
        }
      }
      if (k == root) {
        break;
      }
    }
    root.color = 0;
  }

  private void printHelper(Node root, String indent, boolean last) {
    if (root != TNULL) {
      System.out.print(indent);
      if (last) {
        System.out.print(""R----"");
        indent += ""   "";
      } else {
        System.out.print(""L----"");
        indent += ""|  "";
      }

      String sColor = root.color == 1 ? ""RED"" : ""BLACK"";
      System.out.println(root.data + ""("" + sColor + "")"");
      printHelper(root.left, indent, false);
      printHelper(root.right, indent, true);
    }
  }

  public RedBlackTree() {
    TNULL = new Node();
    TNULL.color = 0;
    TNULL.left = null;
    TNULL.right = null;
    root = TNULL;
  }

  public void preorder() {
    preOrderHelper(this.root);
  }

  public void inorder() {
    inOrderHelper(this.root);
  }

  public void postorder() {
    postOrderHelper(this.root);
  }

  public Node searchTree(int k) {
    return searchTreeHelper(this.root, k);
  }

  public Node minimum(Node node) {
    while (node.left != TNULL) {
      node = node.left;
    }
    return node;
  }

  public Node maximum(Node node) {
    while (node.right != TNULL) {
      node = node.right;
    }
    return node;
  }

  public Node successor(Node x) {
    if (x.right != TNULL) {
      return minimum(x.right);
    }

    Node y = x.parent;
    while (y != TNULL && x == y.right) {
      x = y;
      y = y.parent;
    }
    return y;
  }

  public Node predecessor(Node x) {
    if (x.left != TNULL) {
      return maximum(x.left);
    }

    Node y = x.parent;
    while (y != TNULL && x == y.left) {
      x = y;
      y = y.parent;
    }

    return y;
  }

  public void leftRotate(Node x) {
    Node y = x.right;
    x.right = y.left;
    if (y.left != TNULL) {
      y.left.parent = x;
    }
    y.parent = x.parent;
    if (x.parent == null) {
      this.root = y;
    } else if (x == x.parent.left) {
      x.parent.left = y;
    } else {
      x.parent.right = y;
    }
    y.left = x;
    x.parent = y;
  }

  public void rightRotate(Node x) {
    Node y = x.left;
    x.left = y.right;
    if (y.right != TNULL) {
      y.right.parent = x;
    }
    y.parent = x.parent;
    if (x.parent == null) {
      this.root = y;
    } else if (x == x.parent.right) {
      x.parent.right = y;
    } else {
      x.parent.left = y;
    }
    y.right = x;
    x.parent = y;
  }

  public void insert(int key) {
    Node node = new Node();
    node.parent = null;
    node.data = key;
    node.left = TNULL;
    node.right = TNULL;
    node.color = 1;

    Node y = null;
    Node x = this.root;

    while (x != TNULL) {
      y = x;
      if (node.data < x.data) {
        x = x.left;
      } else {
        x = x.right;
      }
    }

    node.parent = y;
    if (y == null) {
      root = node;
    } else if (node.data < y.data) {
      y.left = node;
    } else {
      y.right = node;
    }

    if (node.parent == null) {
      node.color = 0;
      return;
    }

    if (node.parent.parent == null) {
      return;
    }

    fixInsert(node);
  }

  public Node getRoot() {
    return this.root;
  }

  public void deleteNode(int data) {
    deleteNodeHelper(this.root, data);
  }

  public void printTree() {
    printHelper(this.root, """", true);
  }

  public static void main(String[] args) {
    RedBlackTree bst = new RedBlackTree();
    bst.insert(55);
    bst.insert(40);
    bst.insert(65);
    bst.insert(60);
    bst.insert(75);
    bst.insert(57);
    bst.printTree();

    System.out.println(""\nAfter deleting:"");
    bst.deleteNode(40);
    bst.printTree();
  }
}"
Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. A red-black tree satisfies the following properties: Red/Black Property: Every node is colored, either red or black. Root Property: The root is black. Leaf Property: Every leaf (NIL) is black. Red Property: If a red node has children then, the children are always black. Depth Property: For each node, any simple path from this node to any of its descendant leaf has the same black-depth (the number of black nodes). An example of a red-black tree is: Each node has the following attributes: color
	key
	leftChild
	rightChild
	parent (except root node) color key leftChild rightChild parent (except root node) How the red-black tree maintains the property of self-balancing? The red-black color is meant for balancing the tree. The limitations put on the node colors ensure that any simple path from the root to a leaf is not more than twice as long as any other such path. It helps in maintaining the self-balancing property of the red-black tree. Various operations that can be performed on a red-black tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. Rotation operation is used for maintaining the properties of a red-black tree when they are violated by other operations such as insertion and deletion. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node.  Algorithm Let the initial tree be:
		
			Initial tree If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In right-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial Tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: Let y be the leaf (ie. NIL) and x be the root of the tree. Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree. Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree. Assign the parent of the leaf as a parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild. Assign NULL to the left and rightChild of newNode. Assign RED color to newNode. Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree? This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if the insertion of a newNode violates this property. Do the following while the parent of newNode p is RED. If p is the left child of grandParent gP of z, do the following.
		Case-I:
		
			If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
				Case-II:
			Else if newNode is the right child of p then, assign p to newNode.
			Left-Rotate newNode.
				Case-III:
			Set color of p as BLACK and color of gP as RED.
			Right-Rotate gP. If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode.
				Case-II: Else if newNode is the right child of p then, assign p to newNode. Left-Rotate newNode.
				Case-III: Set color of p as BLACK and color of gP as RED. Right-Rotate gP. Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. Set the root of the tree as BLACK. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Save the color of nodeToBeDeleted in origrinalColor. If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
			Transplant nodeToBeDeleted with x. Assign the right child of nodeToBeDeleted to x. Transplant nodeToBeDeleted with x. Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties. However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recoloring are performed. The following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
			If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w.
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w.
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Assign w to the sibling of x. If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w. Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED. Left-Rotate the parent of x. Assign the rightChild of the parent of x to w. If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w. Set the color of the leftChild of w as BLACK Set the color of w as RED Right-Rotate w. Assign the rightChild of the parent of x to w. If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Set the color of w as the color of the parent of x. Set the color of the parent of x as BLACK. Set the color of the right child of w as BLACK. Left-Rotate the parent of x. Set x as the root of the tree. Else the same as above with right changed to left and vice versa. Set the color of x as BLACK. Please refer to insertion and deletion operations for more explanation with examples. To implement finite maps To implement Java packages: java.util.TreeMap and java.util.TreeSet To implement Standard Template Libraries (STL) in C++: multiset, map, multimap In Linux Kernel","// Implementing Red-Black Tree in C

#include <stdio.h>
#include <stdlib.h>

enum nodeColor {
  RED,
  BLACK
};

struct rbNode {
  int data, color;
  struct rbNode *link[2];
};

struct rbNode *root = NULL;

// Create a red-black tree
struct rbNode *createNode(int data) {
  struct rbNode *newnode;
  newnode = (struct rbNode *)malloc(sizeof(struct rbNode));
  newnode->data = data;
  newnode->color = RED;
  newnode->link[0] = newnode->link[1] = NULL;
  return newnode;
}

// Insert an node
void insertion(int data) {
  struct rbNode *stack[98], *ptr, *newnode, *xPtr, *yPtr;
  int dir[98], ht = 0, index;
  ptr = root;
  if (!root) {
    root = createNode(data);
    return;
  }

  stack[ht] = root;
  dir[ht++] = 0;
  while (ptr != NULL) {
    if (ptr->data == data) {
      printf(""Duplicates Not Allowed!!\n"");
      return;
    }
    index = (data - ptr->data) > 0 ? 1 : 0;
    stack[ht] = ptr;
    ptr = ptr->link[index];
    dir[ht++] = index;
  }
  stack[ht - 1]->link[index] = newnode = createNode(data);
  while ((ht >= 3) && (stack[ht - 1]->color == RED)) {
    if (dir[ht - 2] == 0) {
      yPtr = stack[ht - 2]->link[1];
      if (yPtr != NULL && yPtr->color == RED) {
        stack[ht - 2]->color = RED;
        stack[ht - 1]->color = yPtr->color = BLACK;
        ht = ht - 2;
      } else {
        if (dir[ht - 1] == 0) {
          yPtr = stack[ht - 1];
        } else {
          xPtr = stack[ht - 1];
          yPtr = xPtr->link[1];
          xPtr->link[1] = yPtr->link[0];
          yPtr->link[0] = xPtr;
          stack[ht - 2]->link[0] = yPtr;
        }
        xPtr = stack[ht - 2];
        xPtr->color = RED;
        yPtr->color = BLACK;
        xPtr->link[0] = yPtr->link[1];
        yPtr->link[1] = xPtr;
        if (xPtr == root) {
          root = yPtr;
        } else {
          stack[ht - 3]->link[dir[ht - 3]] = yPtr;
        }
        break;
      }
    } else {
      yPtr = stack[ht - 2]->link[0];
      if ((yPtr != NULL) && (yPtr->color == RED)) {
        stack[ht - 2]->color = RED;
        stack[ht - 1]->color = yPtr->color = BLACK;
        ht = ht - 2;
      } else {
        if (dir[ht - 1] == 1) {
          yPtr = stack[ht - 1];
        } else {
          xPtr = stack[ht - 1];
          yPtr = xPtr->link[0];
          xPtr->link[0] = yPtr->link[1];
          yPtr->link[1] = xPtr;
          stack[ht - 2]->link[1] = yPtr;
        }
        xPtr = stack[ht - 2];
        yPtr->color = BLACK;
        xPtr->color = RED;
        xPtr->link[1] = yPtr->link[0];
        yPtr->link[0] = xPtr;
        if (xPtr == root) {
          root = yPtr;
        } else {
          stack[ht - 3]->link[dir[ht - 3]] = yPtr;
        }
        break;
      }
    }
  }
  root->color = BLACK;
}

// Delete a node
void deletion(int data) {
  struct rbNode *stack[98], *ptr, *xPtr, *yPtr;
  struct rbNode *pPtr, *qPtr, *rPtr;
  int dir[98], ht = 0, diff, i;
  enum nodeColor color;

  if (!root) {
    printf(""Tree not available\n"");
    return;
  }

  ptr = root;
  while (ptr != NULL) {
    if ((data - ptr->data) == 0)
      break;
    diff = (data - ptr->data) > 0 ? 1 : 0;
    stack[ht] = ptr;
    dir[ht++] = diff;
    ptr = ptr->link[diff];
  }

  if (ptr->link[1] == NULL) {
    if ((ptr == root) && (ptr->link[0] == NULL)) {
      free(ptr);
      root = NULL;
    } else if (ptr == root) {
      root = ptr->link[0];
      free(ptr);
    } else {
      stack[ht - 1]->link[dir[ht - 1]] = ptr->link[0];
    }
  } else {
    xPtr = ptr->link[1];
    if (xPtr->link[0] == NULL) {
      xPtr->link[0] = ptr->link[0];
      color = xPtr->color;
      xPtr->color = ptr->color;
      ptr->color = color;

      if (ptr == root) {
        root = xPtr;
      } else {
        stack[ht - 1]->link[dir[ht - 1]] = xPtr;
      }

      dir[ht] = 1;
      stack[ht++] = xPtr;
    } else {
      i = ht++;
      while (1) {
        dir[ht] = 0;
        stack[ht++] = xPtr;
        yPtr = xPtr->link[0];
        if (!yPtr->link[0])
          break;
        xPtr = yPtr;
      }

      dir[i] = 1;
      stack[i] = yPtr;
      if (i > 0)
        stack[i - 1]->link[dir[i - 1]] = yPtr;

      yPtr->link[0] = ptr->link[0];

      xPtr->link[0] = yPtr->link[1];
      yPtr->link[1] = ptr->link[1];

      if (ptr == root) {
        root = yPtr;
      }

      color = yPtr->color;
      yPtr->color = ptr->color;
      ptr->color = color;
    }
  }

  if (ht < 1)
    return;

  if (ptr->color == BLACK) {
    while (1) {
      pPtr = stack[ht - 1]->link[dir[ht - 1]];
      if (pPtr && pPtr->color == RED) {
        pPtr->color = BLACK;
        break;
      }

      if (ht < 2)
        break;

      if (dir[ht - 2] == 0) {
        rPtr = stack[ht - 1]->link[1];

        if (!rPtr)
          break;

        if (rPtr->color == RED) {
          stack[ht - 1]->color = RED;
          rPtr->color = BLACK;
          stack[ht - 1]->link[1] = rPtr->link[0];
          rPtr->link[0] = stack[ht - 1];

          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          dir[ht] = 0;
          stack[ht] = stack[ht - 1];
          stack[ht - 1] = rPtr;
          ht++;

          rPtr = stack[ht - 1]->link[1];
        }

        if ((!rPtr->link[0] || rPtr->link[0]->color == BLACK) &&
          (!rPtr->link[1] || rPtr->link[1]->color == BLACK)) {
          rPtr->color = RED;
        } else {
          if (!rPtr->link[1] || rPtr->link[1]->color == BLACK) {
            qPtr = rPtr->link[0];
            rPtr->color = RED;
            qPtr->color = BLACK;
            rPtr->link[0] = qPtr->link[1];
            qPtr->link[1] = rPtr;
            rPtr = stack[ht - 1]->link[1] = qPtr;
          }
          rPtr->color = stack[ht - 1]->color;
          stack[ht - 1]->color = BLACK;
          rPtr->link[1]->color = BLACK;
          stack[ht - 1]->link[1] = rPtr->link[0];
          rPtr->link[0] = stack[ht - 1];
          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          break;
        }
      } else {
        rPtr = stack[ht - 1]->link[0];
        if (!rPtr)
          break;

        if (rPtr->color == RED) {
          stack[ht - 1]->color = RED;
          rPtr->color = BLACK;
          stack[ht - 1]->link[0] = rPtr->link[1];
          rPtr->link[1] = stack[ht - 1];

          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          dir[ht] = 1;
          stack[ht] = stack[ht - 1];
          stack[ht - 1] = rPtr;
          ht++;

          rPtr = stack[ht - 1]->link[0];
        }
        if ((!rPtr->link[0] || rPtr->link[0]->color == BLACK) &&
          (!rPtr->link[1] || rPtr->link[1]->color == BLACK)) {
          rPtr->color = RED;
        } else {
          if (!rPtr->link[0] || rPtr->link[0]->color == BLACK) {
            qPtr = rPtr->link[1];
            rPtr->color = RED;
            qPtr->color = BLACK;
            rPtr->link[1] = qPtr->link[0];
            qPtr->link[0] = rPtr;
            rPtr = stack[ht - 1]->link[0] = qPtr;
          }
          rPtr->color = stack[ht - 1]->color;
          stack[ht - 1]->color = BLACK;
          rPtr->link[0]->color = BLACK;
          stack[ht - 1]->link[0] = rPtr->link[1];
          rPtr->link[1] = stack[ht - 1];
          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          break;
        }
      }
      ht--;
    }
  }
}

// Print the inorder traversal of the tree
void inorderTraversal(struct rbNode *node) {
  if (node) {
    inorderTraversal(node->link[0]);
    printf(""%d  "", node->data);
    inorderTraversal(node->link[1]);
  }
  return;
}

// Driver code
int main() {
  int ch, data;
  while (1) {
    printf(""1. Insertion\t2. Deletion\n"");
    printf(""3. Traverse\t4. Exit"");
    printf(""\nEnter your choice:"");
    scanf(""%d"", &ch);
    switch (ch) {
      case 1:
        printf(""Enter the element to insert:"");
        scanf(""%d"", &data);
        insertion(data);
        break;
      case 2:
        printf(""Enter the element to delete:"");
        scanf(""%d"", &data);
        deletion(data);
        break;
      case 3:
        inorderTraversal(root);
        printf(""\n"");
        break;
      case 4:
        exit(0);
      default:
        printf(""Not available\n"");
        break;
    }
    printf(""\n"");
  }
  return 0;
}"
Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. A red-black tree satisfies the following properties: Red/Black Property: Every node is colored, either red or black. Root Property: The root is black. Leaf Property: Every leaf (NIL) is black. Red Property: If a red node has children then, the children are always black. Depth Property: For each node, any simple path from this node to any of its descendant leaf has the same black-depth (the number of black nodes). An example of a red-black tree is: Each node has the following attributes: color
	key
	leftChild
	rightChild
	parent (except root node) color key leftChild rightChild parent (except root node) How the red-black tree maintains the property of self-balancing? The red-black color is meant for balancing the tree. The limitations put on the node colors ensure that any simple path from the root to a leaf is not more than twice as long as any other such path. It helps in maintaining the self-balancing property of the red-black tree. Various operations that can be performed on a red-black tree are: In rotation operation, the positions of the nodes of a subtree are interchanged. Rotation operation is used for maintaining the properties of a red-black tree when they are violated by other operations such as insertion and deletion. There are two types of rotations: In left-rotation, the arrangement of the nodes on the right is transformed into the arrangements on the left node.  Algorithm Let the initial tree be:
		
			Initial tree If y has a left subtree, assign x as the parent of the left subtree of y.
		
			Assign x as the parent of the left subtree of y If the parent of x is NULL, make y as the root of the tree. Else if x is the left child of p, make y as the left child of p. Else assign y as the right child of p.
		
			Change the parent of x to that of y Make y as the parent of x.
		
			Assign y as the parent of x. In right-rotation, the arrangement of the nodes on the left is transformed into the arrangements on the right node. Let the initial tree be:
		
			Initial Tree If x has a right subtree, assign y as the parent of the right subtree of x.
		
			Assign y as the parent of the right subtree of x If the parent of y is NULL, make x as the root of the tree. Else if y is the right child of its parent p, make x as the right child of p. Else assign x as the left child of p.
		
			Assign the parent of y as the parent of x Make x as the parent of y.
		
			Assign x as the parent of y In left-right rotation, the arrangements are first shifted to the left and then to the right. Do left rotation on x-y.
		
			Left rotate x-y Do right rotation on y-z.
		
			Right rotate z-y In right-left rotation, the arrangements are first shifted to the right and then to the left. Do right rotation on x-y.
		
			Right rotate x-y Do left rotation on z-y.
		
			Left rotate z-y While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: Let y be the leaf (ie. NIL) and x be the root of the tree. Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree. Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree. Assign the parent of the leaf as a parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild. Assign NULL to the left and rightChild of newNode. Assign RED color to newNode. Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree? This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if the insertion of a newNode violates this property. Do the following while the parent of newNode p is RED. If p is the left child of grandParent gP of z, do the following.
		Case-I:
		
			If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
				Case-II:
			Else if newNode is the right child of p then, assign p to newNode.
			Left-Rotate newNode.
				Case-III:
			Set color of p as BLACK and color of gP as RED.
			Right-Rotate gP. If the color of the right child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode.
				Case-II: Else if newNode is the right child of p then, assign p to newNode. Left-Rotate newNode.
				Case-III: Set color of p as BLACK and color of gP as RED. Right-Rotate gP. Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. Set the root of the tree as BLACK. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Save the color of nodeToBeDeleted in origrinalColor. If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
			Transplant nodeToBeDeleted with x. Assign the right child of nodeToBeDeleted to x. Transplant nodeToBeDeleted with x. Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties. However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recoloring are performed. The following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
			If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w.
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w.
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Assign w to the sibling of x. If the right child of parent of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
					Left-Rotate the parent of x.
					Assign the rightChild of the parent of x to w. Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED. Left-Rotate the parent of x. Assign the rightChild of the parent of x to w. If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
					Right-Rotate w.
					Assign the rightChild of the parent of x to w. Set the color of the leftChild of w as BLACK Set the color of w as RED Right-Rotate w. Assign the rightChild of the parent of x to w. If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of x as BLACK.
					Set the color of the right child of w as BLACK.
					Left-Rotate the parent of x.
					Set x as the root of the tree. Set the color of w as the color of the parent of x. Set the color of the parent of x as BLACK. Set the color of the right child of w as BLACK. Left-Rotate the parent of x. Set x as the root of the tree. Else the same as above with right changed to left and vice versa. Set the color of x as BLACK. Please refer to insertion and deletion operations for more explanation with examples. To implement finite maps To implement Java packages: java.util.TreeMap and java.util.TreeSet To implement Standard Template Libraries (STL) in C++: multiset, map, multimap In Linux Kernel","// Implementing Red-Black Tree in C++

#include <iostream>
using namespace std;

struct Node {
  int data;
  Node *parent;
  Node *left;
  Node *right;
  int color;
};

typedef Node *NodePtr;

class RedBlackTree {
   private:
  NodePtr root;
  NodePtr TNULL;

  void initializeNULLNode(NodePtr node, NodePtr parent) {
    node->data = 0;
    node->parent = parent;
    node->left = nullptr;
    node->right = nullptr;
    node->color = 0;
  }

  // Preorder
  void preOrderHelper(NodePtr node) {
    if (node != TNULL) {
      cout << node->data << "" "";
      preOrderHelper(node->left);
      preOrderHelper(node->right);
    }
  }

  // Inorder
  void inOrderHelper(NodePtr node) {
    if (node != TNULL) {
      inOrderHelper(node->left);
      cout << node->data << "" "";
      inOrderHelper(node->right);
    }
  }

  // Post order
  void postOrderHelper(NodePtr node) {
    if (node != TNULL) {
      postOrderHelper(node->left);
      postOrderHelper(node->right);
      cout << node->data << "" "";
    }
  }

  NodePtr searchTreeHelper(NodePtr node, int key) {
    if (node == TNULL || key == node->data) {
      return node;
    }

    if (key < node->data) {
      return searchTreeHelper(node->left, key);
    }
    return searchTreeHelper(node->right, key);
  }

  // For balancing the tree after deletion
  void deleteFix(NodePtr x) {
    NodePtr s;
    while (x != root && x->color == 0) {
      if (x == x->parent->left) {
        s = x->parent->right;
        if (s->color == 1) {
          s->color = 0;
          x->parent->color = 1;
          leftRotate(x->parent);
          s = x->parent->right;
        }

        if (s->left->color == 0 && s->right->color == 0) {
          s->color = 1;
          x = x->parent;
        } else {
          if (s->right->color == 0) {
            s->left->color = 0;
            s->color = 1;
            rightRotate(s);
            s = x->parent->right;
          }

          s->color = x->parent->color;
          x->parent->color = 0;
          s->right->color = 0;
          leftRotate(x->parent);
          x = root;
        }
      } else {
        s = x->parent->left;
        if (s->color == 1) {
          s->color = 0;
          x->parent->color = 1;
          rightRotate(x->parent);
          s = x->parent->left;
        }

        if (s->right->color == 0 && s->right->color == 0) {
          s->color = 1;
          x = x->parent;
        } else {
          if (s->left->color == 0) {
            s->right->color = 0;
            s->color = 1;
            leftRotate(s);
            s = x->parent->left;
          }

          s->color = x->parent->color;
          x->parent->color = 0;
          s->left->color = 0;
          rightRotate(x->parent);
          x = root;
        }
      }
    }
    x->color = 0;
  }

  void rbTransplant(NodePtr u, NodePtr v) {
    if (u->parent == nullptr) {
      root = v;
    } else if (u == u->parent->left) {
      u->parent->left = v;
    } else {
      u->parent->right = v;
    }
    v->parent = u->parent;
  }

  void deleteNodeHelper(NodePtr node, int key) {
    NodePtr z = TNULL;
    NodePtr x, y;
    while (node != TNULL) {
      if (node->data == key) {
        z = node;
      }

      if (node->data <= key) {
        node = node->right;
      } else {
        node = node->left;
      }
    }

    if (z == TNULL) {
      cout << ""Key not found in the tree"" << endl;
      return;
    }

    y = z;
    int y_original_color = y->color;
    if (z->left == TNULL) {
      x = z->right;
      rbTransplant(z, z->right);
    } else if (z->right == TNULL) {
      x = z->left;
      rbTransplant(z, z->left);
    } else {
      y = minimum(z->right);
      y_original_color = y->color;
      x = y->right;
      if (y->parent == z) {
        x->parent = y;
      } else {
        rbTransplant(y, y->right);
        y->right = z->right;
        y->right->parent = y;
      }

      rbTransplant(z, y);
      y->left = z->left;
      y->left->parent = y;
      y->color = z->color;
    }
    delete z;
    if (y_original_color == 0) {
      deleteFix(x);
    }
  }

  // For balancing the tree after insertion
  void insertFix(NodePtr k) {
    NodePtr u;
    while (k->parent->color == 1) {
      if (k->parent == k->parent->parent->right) {
        u = k->parent->parent->left;
        if (u->color == 1) {
          u->color = 0;
          k->parent->color = 0;
          k->parent->parent->color = 1;
          k = k->parent->parent;
        } else {
          if (k == k->parent->left) {
            k = k->parent;
            rightRotate(k);
          }
          k->parent->color = 0;
          k->parent->parent->color = 1;
          leftRotate(k->parent->parent);
        }
      } else {
        u = k->parent->parent->right;

        if (u->color == 1) {
          u->color = 0;
          k->parent->color = 0;
          k->parent->parent->color = 1;
          k = k->parent->parent;
        } else {
          if (k == k->parent->right) {
            k = k->parent;
            leftRotate(k);
          }
          k->parent->color = 0;
          k->parent->parent->color = 1;
          rightRotate(k->parent->parent);
        }
      }
      if (k == root) {
        break;
      }
    }
    root->color = 0;
  }

  void printHelper(NodePtr root, string indent, bool last) {
    if (root != TNULL) {
      cout << indent;
      if (last) {
        cout << ""R----"";
        indent += ""   "";
      } else {
        cout << ""L----"";
        indent += ""|  "";
      }

      string sColor = root->color ? ""RED"" : ""BLACK"";
      cout << root->data << ""("" << sColor << "")"" << endl;
      printHelper(root->left, indent, false);
      printHelper(root->right, indent, true);
    }
  }

   public:
  RedBlackTree() {
    TNULL = new Node;
    TNULL->color = 0;
    TNULL->left = nullptr;
    TNULL->right = nullptr;
    root = TNULL;
  }

  void preorder() {
    preOrderHelper(this->root);
  }

  void inorder() {
    inOrderHelper(this->root);
  }

  void postorder() {
    postOrderHelper(this->root);
  }

  NodePtr searchTree(int k) {
    return searchTreeHelper(this->root, k);
  }

  NodePtr minimum(NodePtr node) {
    while (node->left != TNULL) {
      node = node->left;
    }
    return node;
  }

  NodePtr maximum(NodePtr node) {
    while (node->right != TNULL) {
      node = node->right;
    }
    return node;
  }

  NodePtr successor(NodePtr x) {
    if (x->right != TNULL) {
      return minimum(x->right);
    }

    NodePtr y = x->parent;
    while (y != TNULL && x == y->right) {
      x = y;
      y = y->parent;
    }
    return y;
  }

  NodePtr predecessor(NodePtr x) {
    if (x->left != TNULL) {
      return maximum(x->left);
    }

    NodePtr y = x->parent;
    while (y != TNULL && x == y->left) {
      x = y;
      y = y->parent;
    }

    return y;
  }

  void leftRotate(NodePtr x) {
    NodePtr y = x->right;
    x->right = y->left;
    if (y->left != TNULL) {
      y->left->parent = x;
    }
    y->parent = x->parent;
    if (x->parent == nullptr) {
      this->root = y;
    } else if (x == x->parent->left) {
      x->parent->left = y;
    } else {
      x->parent->right = y;
    }
    y->left = x;
    x->parent = y;
  }

  void rightRotate(NodePtr x) {
    NodePtr y = x->left;
    x->left = y->right;
    if (y->right != TNULL) {
      y->right->parent = x;
    }
    y->parent = x->parent;
    if (x->parent == nullptr) {
      this->root = y;
    } else if (x == x->parent->right) {
      x->parent->right = y;
    } else {
      x->parent->left = y;
    }
    y->right = x;
    x->parent = y;
  }

  // Inserting a node
  void insert(int key) {
    NodePtr node = new Node;
    node->parent = nullptr;
    node->data = key;
    node->left = TNULL;
    node->right = TNULL;
    node->color = 1;

    NodePtr y = nullptr;
    NodePtr x = this->root;

    while (x != TNULL) {
      y = x;
      if (node->data < x->data) {
        x = x->left;
      } else {
        x = x->right;
      }
    }

    node->parent = y;
    if (y == nullptr) {
      root = node;
    } else if (node->data < y->data) {
      y->left = node;
    } else {
      y->right = node;
    }

    if (node->parent == nullptr) {
      node->color = 0;
      return;
    }

    if (node->parent->parent == nullptr) {
      return;
    }

    insertFix(node);
  }

  NodePtr getRoot() {
    return this->root;
  }

  void deleteNode(int data) {
    deleteNodeHelper(this->root, data);
  }

  void printTree() {
    if (root) {
      printHelper(this->root, """", true);
    }
  }
};

int main() {
  RedBlackTree bst;
  bst.insert(55);
  bst.insert(40);
  bst.insert(65);
  bst.insert(60);
  bst.insert(75);
  bst.insert(57);

  bst.printTree();
  cout << endl
     << ""After deleting"" << endl;
  bst.deleteNode(40);
  bst.printTree();
}"
Insertion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: The newNode be:

		
			New node Let y be the leaf (ie. NIL) and x be the root of the tree. The new node is inserted in the following tree.
		
			Initial tree Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Assign the parent of the leaf as parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild.
		
			New node inserted Assign NULL to the left and rightChild of newNode. Assign RED color to newNode.
		
			Set the color of the newNode red and assign null to the children Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree?  This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if insertion of a newNode violates this property. Do the following until the parent of newNode p is RED. If p is the left child of grandParent gP of newNode, do the following.
		Case-I:
		
			If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change
				
			
			Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode
				
			
			Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change
				
			
			Right-Rotate gP.
				
					Right Rotate If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change Right-Rotate gP.
				
					Right Rotate Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. (This step is performed after coming out of the while loop.)
		Set the root of the tree as BLACK.
		
			Set root's color black The final tree look like this:","# Implementing Red-Black Tree in Python


import sys

# Node creation
class Node():
    def __init__(self, item):
        self.item = item
        self.parent = None
        self.left = None
        self.right = None
        self.color = 1


class RedBlackTree():
    def __init__(self):
        self.TNULL = Node(0)
        self.TNULL.color = 0
        self.TNULL.left = None
        self.TNULL.right = None
        self.root = self.TNULL

    # Preorder
    def pre_order_helper(self, node):
        if node != TNULL:
            sys.stdout.write(node.item + "" "")
            self.pre_order_helper(node.left)
            self.pre_order_helper(node.right)

    # Inorder
    def in_order_helper(self, node):
        if node != TNULL:
            self.in_order_helper(node.left)
            sys.stdout.write(node.item + "" "")
            self.in_order_helper(node.right)

    # Postorder
    def post_order_helper(self, node):
        if node != TNULL:
            self.post_order_helper(node.left)
            self.post_order_helper(node.right)
            sys.stdout.write(node.item + "" "")

    # Search the tree
    def search_tree_helper(self, node, key):
        if node == TNULL or key == node.item:
            return node

        if key < node.item:
            return self.search_tree_helper(node.left, key)
        return self.search_tree_helper(node.right, key)

    # Balance the tree after insertion
    def fix_insert(self, k):
        while k.parent.color == 1:
            if k.parent == k.parent.parent.right:
                u = k.parent.parent.left
                if u.color == 1:
                    u.color = 0
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    k = k.parent.parent
                else:
                    if k == k.parent.left:
                        k = k.parent
                        self.right_rotate(k)
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    self.left_rotate(k.parent.parent)
            else:
                u = k.parent.parent.right

                if u.color == 1:
                    u.color = 0
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    k = k.parent.parent
                else:
                    if k == k.parent.right:
                        k = k.parent
                        self.left_rotate(k)
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    self.right_rotate(k.parent.parent)
            if k == self.root:
                break
        self.root.color = 0

    # Printing the tree
    def __print_helper(self, node, indent, last):
        if node != self.TNULL:
            sys.stdout.write(indent)
            if last:
                sys.stdout.write(""R----"")
                indent += ""     ""
            else:
                sys.stdout.write(""L----"")
                indent += ""|    ""

            s_color = ""RED"" if node.color == 1 else ""BLACK""
            print(str(node.item) + ""("" + s_color + "")"")
            self.__print_helper(node.left, indent, False)
            self.__print_helper(node.right, indent, True)

    def preorder(self):
        self.pre_order_helper(self.root)

    def inorder(self):
        self.in_order_helper(self.root)

    def postorder(self):
        self.post_order_helper(self.root)

    def searchTree(self, k):
        return self.search_tree_helper(self.root, k)

    def minimum(self, node):
        while node.left != self.TNULL:
            node = node.left
        return node

    def maximum(self, node):
        while node.right != self.TNULL:
            node = node.right
        return node

    def successor(self, x):
        if x.right != self.TNULL:
            return self.minimum(x.right)

        y = x.parent
        while y != self.TNULL and x == y.right:
            x = y
            y = y.parent
        return y

    def predecessor(self,  x):
        if (x.left != self.TNULL):
            return self.maximum(x.left)

        y = x.parent
        while y != self.TNULL and x == y.left:
            x = y
            y = y.parent

        return y

    def left_rotate(self, x):
        y = x.right
        x.right = y.left
        if y.left != self.TNULL:
            y.left.parent = x

        y.parent = x.parent
        if x.parent == None:
            self.root = y
        elif x == x.parent.left:
            x.parent.left = y
        else:
            x.parent.right = y
        y.left = x
        x.parent = y

    def right_rotate(self, x):
        y = x.left
        x.left = y.right
        if y.right != self.TNULL:
            y.right.parent = x

        y.parent = x.parent
        if x.parent == None:
            self.root = y
        elif x == x.parent.right:
            x.parent.right = y
        else:
            x.parent.left = y
        y.right = x
        x.parent = y

    def insert(self, key):
        node = Node(key)
        node.parent = None
        node.item = key
        node.left = self.TNULL
        node.right = self.TNULL
        node.color = 1

        y = None
        x = self.root

        while x != self.TNULL:
            y = x
            if node.item < x.item:
                x = x.left
            else:
                x = x.right

        node.parent = y
        if y == None:
            self.root = node
        elif node.item < y.item:
            y.left = node
        else:
            y.right = node

        if node.parent == None:
            node.color = 0
            return

        if node.parent.parent == None:
            return

        self.fix_insert(node)

    def get_root(self):
        return self.root

    def print_tree(self):
        self.__print_helper(self.root, """", True)


if __name__ == ""__main__"":
    bst = RedBlackTree()

    bst.insert(55)
    bst.insert(40)
    bst.insert(65)
    bst.insert(60)
    bst.insert(75)
    bst.insert(57)

    bst.print_tree()"
Insertion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: The newNode be:

		
			New node Let y be the leaf (ie. NIL) and x be the root of the tree. The new node is inserted in the following tree.
		
			Initial tree Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Assign the parent of the leaf as parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild.
		
			New node inserted Assign NULL to the left and rightChild of newNode. Assign RED color to newNode.
		
			Set the color of the newNode red and assign null to the children Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree?  This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if insertion of a newNode violates this property. Do the following until the parent of newNode p is RED. If p is the left child of grandParent gP of newNode, do the following.
		Case-I:
		
			If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change
				
			
			Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode
				
			
			Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change
				
			
			Right-Rotate gP.
				
					Right Rotate If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change Right-Rotate gP.
				
					Right Rotate Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. (This step is performed after coming out of the while loop.)
		Set the root of the tree as BLACK.
		
			Set root's color black The final tree look like this:","// Implementing Red-Black Tree in Java

class Node {
  int data;
  Node parent;
  Node left;
  Node right;
  int color;
}

public class RedBlackTree {
  private Node root;
  private Node TNULL;

  // Preorder
  private void preOrderHelper(Node node) {
    if (node != TNULL) {
      System.out.print(node.data + "" "");
      preOrderHelper(node.left);
      preOrderHelper(node.right);
    }
  }

  // Inorder
  private void inOrderHelper(Node node) {
    if (node != TNULL) {
      inOrderHelper(node.left);
      System.out.print(node.data + "" "");
      inOrderHelper(node.right);
    }
  }

  // Post order
  private void postOrderHelper(Node node) {
    if (node != TNULL) {
      postOrderHelper(node.left);
      postOrderHelper(node.right);
      System.out.print(node.data + "" "");
    }
  }

  // Search the tree
  private Node searchTreeHelper(Node node, int key) {
    if (node == TNULL || key == node.data) {
      return node;
    }

    if (key < node.data) {
      return searchTreeHelper(node.left, key);
    }
    return searchTreeHelper(node.right, key);
  }

  // Balance the tree after deletion of a node
  private void fixDelete(Node x) {
    Node s;
    while (x != root && x.color == 0) {
      if (x == x.parent.left) {
        s = x.parent.right;
        if (s.color == 1) {
          s.color = 0;
          x.parent.color = 1;
          leftRotate(x.parent);
          s = x.parent.right;
        }

        if (s.left.color == 0 && s.right.color == 0) {
          s.color = 1;
          x = x.parent;
        } else {
          if (s.right.color == 0) {
            s.left.color = 0;
            s.color = 1;
            rightRotate(s);
            s = x.parent.right;
          }

          s.color = x.parent.color;
          x.parent.color = 0;
          s.right.color = 0;
          leftRotate(x.parent);
          x = root;
        }
      } else {
        s = x.parent.left;
        if (s.color == 1) {
          s.color = 0;
          x.parent.color = 1;
          rightRotate(x.parent);
          s = x.parent.left;
        }

        if (s.right.color == 0 && s.right.color == 0) {
          s.color = 1;
          x = x.parent;
        } else {
          if (s.left.color == 0) {
            s.right.color = 0;
            s.color = 1;
            leftRotate(s);
            s = x.parent.left;
          }

          s.color = x.parent.color;
          x.parent.color = 0;
          s.left.color = 0;
          rightRotate(x.parent);
          x = root;
        }
      }
    }
    x.color = 0;
  }

  private void rbTransplant(Node u, Node v) {
    if (u.parent == null) {
      root = v;
    } else if (u == u.parent.left) {
      u.parent.left = v;
    } else {
      u.parent.right = v;
    }
    v.parent = u.parent;
  }

  // Balance the node after insertion
  private void fixInsert(Node k) {
    Node u;
    while (k.parent.color == 1) {
      if (k.parent == k.parent.parent.right) {
        u = k.parent.parent.left;
        if (u.color == 1) {
          u.color = 0;
          k.parent.color = 0;
          k.parent.parent.color = 1;
          k = k.parent.parent;
        } else {
          if (k == k.parent.left) {
            k = k.parent;
            rightRotate(k);
          }
          k.parent.color = 0;
          k.parent.parent.color = 1;
          leftRotate(k.parent.parent);
        }
      } else {
        u = k.parent.parent.right;

        if (u.color == 1) {
          u.color = 0;
          k.parent.color = 0;
          k.parent.parent.color = 1;
          k = k.parent.parent;
        } else {
          if (k == k.parent.right) {
            k = k.parent;
            leftRotate(k);
          }
          k.parent.color = 0;
          k.parent.parent.color = 1;
          rightRotate(k.parent.parent);
        }
      }
      if (k == root) {
        break;
      }
    }
    root.color = 0;
  }

  private void printHelper(Node root, String indent, boolean last) {
    if (root != TNULL) {
      System.out.print(indent);
      if (last) {
        System.out.print(""R----"");
        indent += ""   "";
      } else {
        System.out.print(""L----"");
        indent += ""|  "";
      }

      String sColor = root.color == 1 ? ""RED"" : ""BLACK"";
      System.out.println(root.data + ""("" + sColor + "")"");
      printHelper(root.left, indent, false);
      printHelper(root.right, indent, true);
    }
  }

  public RedBlackTree() {
    TNULL = new Node();
    TNULL.color = 0;
    TNULL.left = null;
    TNULL.right = null;
    root = TNULL;
  }

  public void preorder() {
    preOrderHelper(this.root);
  }

  public void inorder() {
    inOrderHelper(this.root);
  }

  public void postorder() {
    postOrderHelper(this.root);
  }

  public Node searchTree(int k) {
    return searchTreeHelper(this.root, k);
  }

  public Node minimum(Node node) {
    while (node.left != TNULL) {
      node = node.left;
    }
    return node;
  }

  public Node maximum(Node node) {
    while (node.right != TNULL) {
      node = node.right;
    }
    return node;
  }

  public Node successor(Node x) {
    if (x.right != TNULL) {
      return minimum(x.right);
    }

    Node y = x.parent;
    while (y != TNULL && x == y.right) {
      x = y;
      y = y.parent;
    }
    return y;
  }

  public Node predecessor(Node x) {
    if (x.left != TNULL) {
      return maximum(x.left);
    }

    Node y = x.parent;
    while (y != TNULL && x == y.left) {
      x = y;
      y = y.parent;
    }

    return y;
  }

  public void leftRotate(Node x) {
    Node y = x.right;
    x.right = y.left;
    if (y.left != TNULL) {
      y.left.parent = x;
    }
    y.parent = x.parent;
    if (x.parent == null) {
      this.root = y;
    } else if (x == x.parent.left) {
      x.parent.left = y;
    } else {
      x.parent.right = y;
    }
    y.left = x;
    x.parent = y;
  }

  public void rightRotate(Node x) {
    Node y = x.left;
    x.left = y.right;
    if (y.right != TNULL) {
      y.right.parent = x;
    }
    y.parent = x.parent;
    if (x.parent == null) {
      this.root = y;
    } else if (x == x.parent.right) {
      x.parent.right = y;
    } else {
      x.parent.left = y;
    }
    y.right = x;
    x.parent = y;
  }

  public void insert(int key) {
    Node node = new Node();
    node.parent = null;
    node.data = key;
    node.left = TNULL;
    node.right = TNULL;
    node.color = 1;

    Node y = null;
    Node x = this.root;

    while (x != TNULL) {
      y = x;
      if (node.data < x.data) {
        x = x.left;
      } else {
        x = x.right;
      }
    }

    node.parent = y;
    if (y == null) {
      root = node;
    } else if (node.data < y.data) {
      y.left = node;
    } else {
      y.right = node;
    }

    if (node.parent == null) {
      node.color = 0;
      return;
    }

    if (node.parent.parent == null) {
      return;
    }

    fixInsert(node);
  }

  public Node getRoot() {
    return this.root;
  }

  public void printTree() {
    printHelper(this.root, """", true);
  }

  public static void main(String[] args) {
    RedBlackTree bst = new RedBlackTree();
    bst.insert(55);
    bst.insert(40);
    bst.insert(65);
    bst.insert(60);
    bst.insert(75);
    bst.insert(57);
    bst.printTree();

  }
}"
Insertion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: The newNode be:

		
			New node Let y be the leaf (ie. NIL) and x be the root of the tree. The new node is inserted in the following tree.
		
			Initial tree Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Assign the parent of the leaf as parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild.
		
			New node inserted Assign NULL to the left and rightChild of newNode. Assign RED color to newNode.
		
			Set the color of the newNode red and assign null to the children Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree?  This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if insertion of a newNode violates this property. Do the following until the parent of newNode p is RED. If p is the left child of grandParent gP of newNode, do the following.
		Case-I:
		
			If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change
				
			
			Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode
				
			
			Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change
				
			
			Right-Rotate gP.
				
					Right Rotate If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change Right-Rotate gP.
				
					Right Rotate Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. (This step is performed after coming out of the while loop.)
		Set the root of the tree as BLACK.
		
			Set root's color black The final tree look like this:","// Implementing Red-Black Tree in C

#include <stdio.h>
#include <stdlib.h>

enum nodeColor {
  RED,
  BLACK
};

struct rbNode {
  int data, color;
  struct rbNode *link[2];
};

struct rbNode *root = NULL;

// Create a red-black tree
struct rbNode *createNode(int data) {
  struct rbNode *newnode;
  newnode = (struct rbNode *)malloc(sizeof(struct rbNode));
  newnode->data = data;
  newnode->color = RED;
  newnode->link[0] = newnode->link[1] = NULL;
  return newnode;
}

// Insert an node
void insertion(int data) {
  struct rbNode *stack[98], *ptr, *newnode, *xPtr, *yPtr;
  int dir[98], ht = 0, index;
  ptr = root;
  if (!root) {
    root = createNode(data);
    return;
  }

  stack[ht] = root;
  dir[ht++] = 0;
  while (ptr != NULL) {
    if (ptr->data == data) {
      printf(""Duplicates Not Allowed!!\n"");
      return;
    }
    index = (data - ptr->data) > 0 ? 1 : 0;
    stack[ht] = ptr;
    ptr = ptr->link[index];
    dir[ht++] = index;
  }
  stack[ht - 1]->link[index] = newnode = createNode(data);
  while ((ht >= 3) && (stack[ht - 1]->color == RED)) {
    if (dir[ht - 2] == 0) {
      yPtr = stack[ht - 2]->link[1];
      if (yPtr != NULL && yPtr->color == RED) {
        stack[ht - 2]->color = RED;
        stack[ht - 1]->color = yPtr->color = BLACK;
        ht = ht - 2;
      } else {
        if (dir[ht - 1] == 0) {
          yPtr = stack[ht - 1];
        } else {
          xPtr = stack[ht - 1];
          yPtr = xPtr->link[1];
          xPtr->link[1] = yPtr->link[0];
          yPtr->link[0] = xPtr;
          stack[ht - 2]->link[0] = yPtr;
        }
        xPtr = stack[ht - 2];
        xPtr->color = RED;
        yPtr->color = BLACK;
        xPtr->link[0] = yPtr->link[1];
        yPtr->link[1] = xPtr;
        if (xPtr == root) {
          root = yPtr;
        } else {
          stack[ht - 3]->link[dir[ht - 3]] = yPtr;
        }
        break;
      }
    } else {
      yPtr = stack[ht - 2]->link[0];
      if ((yPtr != NULL) && (yPtr->color == RED)) {
        stack[ht - 2]->color = RED;
        stack[ht - 1]->color = yPtr->color = BLACK;
        ht = ht - 2;
      } else {
        if (dir[ht - 1] == 1) {
          yPtr = stack[ht - 1];
        } else {
          xPtr = stack[ht - 1];
          yPtr = xPtr->link[0];
          xPtr->link[0] = yPtr->link[1];
          yPtr->link[1] = xPtr;
          stack[ht - 2]->link[1] = yPtr;
        }
        xPtr = stack[ht - 2];
        yPtr->color = BLACK;
        xPtr->color = RED;
        xPtr->link[1] = yPtr->link[0];
        yPtr->link[0] = xPtr;
        if (xPtr == root) {
          root = yPtr;
        } else {
          stack[ht - 3]->link[dir[ht - 3]] = yPtr;
        }
        break;
      }
    }
  }
  root->color = BLACK;
}

// Delete a node
void deletion(int data) {
  struct rbNode *stack[98], *ptr, *xPtr, *yPtr;
  struct rbNode *pPtr, *qPtr, *rPtr;
  int dir[98], ht = 0, diff, i;
  enum nodeColor color;

  if (!root) {
    printf(""Tree not available\n"");
    return;
  }

  ptr = root;
  while (ptr != NULL) {
    if ((data - ptr->data) == 0)
      break;
    diff = (data - ptr->data) > 0 ? 1 : 0;
    stack[ht] = ptr;
    dir[ht++] = diff;
    ptr = ptr->link[diff];
  }

  if (ptr->link[1] == NULL) {
    if ((ptr == root) && (ptr->link[0] == NULL)) {
      free(ptr);
      root = NULL;
    } else if (ptr == root) {
      root = ptr->link[0];
      free(ptr);
    } else {
      stack[ht - 1]->link[dir[ht - 1]] = ptr->link[0];
    }
  } else {
    xPtr = ptr->link[1];
    if (xPtr->link[0] == NULL) {
      xPtr->link[0] = ptr->link[0];
      color = xPtr->color;
      xPtr->color = ptr->color;
      ptr->color = color;

      if (ptr == root) {
        root = xPtr;
      } else {
        stack[ht - 1]->link[dir[ht - 1]] = xPtr;
      }

      dir[ht] = 1;
      stack[ht++] = xPtr;
    } else {
      i = ht++;
      while (1) {
        dir[ht] = 0;
        stack[ht++] = xPtr;
        yPtr = xPtr->link[0];
        if (!yPtr->link[0])
          break;
        xPtr = yPtr;
      }

      dir[i] = 1;
      stack[i] = yPtr;
      if (i > 0)
        stack[i - 1]->link[dir[i - 1]] = yPtr;

      yPtr->link[0] = ptr->link[0];

      xPtr->link[0] = yPtr->link[1];
      yPtr->link[1] = ptr->link[1];

      if (ptr == root) {
        root = yPtr;
      }

      color = yPtr->color;
      yPtr->color = ptr->color;
      ptr->color = color;
    }
  }

  if (ht < 1)
    return;

  if (ptr->color == BLACK) {
    while (1) {
      pPtr = stack[ht - 1]->link[dir[ht - 1]];
      if (pPtr && pPtr->color == RED) {
        pPtr->color = BLACK;
        break;
      }

      if (ht < 2)
        break;

      if (dir[ht - 2] == 0) {
        rPtr = stack[ht - 1]->link[1];

        if (!rPtr)
          break;

        if (rPtr->color == RED) {
          stack[ht - 1]->color = RED;
          rPtr->color = BLACK;
          stack[ht - 1]->link[1] = rPtr->link[0];
          rPtr->link[0] = stack[ht - 1];

          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          dir[ht] = 0;
          stack[ht] = stack[ht - 1];
          stack[ht - 1] = rPtr;
          ht++;

          rPtr = stack[ht - 1]->link[1];
        }

        if ((!rPtr->link[0] || rPtr->link[0]->color == BLACK) &&
          (!rPtr->link[1] || rPtr->link[1]->color == BLACK)) {
          rPtr->color = RED;
        } else {
          if (!rPtr->link[1] || rPtr->link[1]->color == BLACK) {
            qPtr = rPtr->link[0];
            rPtr->color = RED;
            qPtr->color = BLACK;
            rPtr->link[0] = qPtr->link[1];
            qPtr->link[1] = rPtr;
            rPtr = stack[ht - 1]->link[1] = qPtr;
          }
          rPtr->color = stack[ht - 1]->color;
          stack[ht - 1]->color = BLACK;
          rPtr->link[1]->color = BLACK;
          stack[ht - 1]->link[1] = rPtr->link[0];
          rPtr->link[0] = stack[ht - 1];
          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          break;
        }
      } else {
        rPtr = stack[ht - 1]->link[0];
        if (!rPtr)
          break;

        if (rPtr->color == RED) {
          stack[ht - 1]->color = RED;
          rPtr->color = BLACK;
          stack[ht - 1]->link[0] = rPtr->link[1];
          rPtr->link[1] = stack[ht - 1];

          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          dir[ht] = 1;
          stack[ht] = stack[ht - 1];
          stack[ht - 1] = rPtr;
          ht++;

          rPtr = stack[ht - 1]->link[0];
        }
        if ((!rPtr->link[0] || rPtr->link[0]->color == BLACK) &&
          (!rPtr->link[1] || rPtr->link[1]->color == BLACK)) {
          rPtr->color = RED;
        } else {
          if (!rPtr->link[0] || rPtr->link[0]->color == BLACK) {
            qPtr = rPtr->link[1];
            rPtr->color = RED;
            qPtr->color = BLACK;
            rPtr->link[1] = qPtr->link[0];
            qPtr->link[0] = rPtr;
            rPtr = stack[ht - 1]->link[0] = qPtr;
          }
          rPtr->color = stack[ht - 1]->color;
          stack[ht - 1]->color = BLACK;
          rPtr->link[0]->color = BLACK;
          stack[ht - 1]->link[0] = rPtr->link[1];
          rPtr->link[1] = stack[ht - 1];
          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          break;
        }
      }
      ht--;
    }
  }
}

// Print the inorder traversal of the tree
void inorderTraversal(struct rbNode *node) {
  if (node) {
    inorderTraversal(node->link[0]);
    printf(""%d  "", node->data);
    inorderTraversal(node->link[1]);
  }
  return;
}

// Driver code
int main() {
  int ch, data;
  while (1) {
    printf(""1. Insertion\t2. Deletion\n"");
    printf(""3. Traverse\t4. Exit"");
    printf(""\nEnter your choice:"");
    scanf(""%d"", &ch);
    switch (ch) {
      case 1:
        printf(""Enter the element to insert:"");
        scanf(""%d"", &data);
        insertion(data);
        break;
      case 2:
        printf(""Enter the element to delete:"");
        scanf(""%d"", &data);
        deletion(data);
        break;
      case 3:
        inorderTraversal(root);
        printf(""\n"");
        break;
      case 4:
        exit(0);
      default:
        printf(""Not available\n"");
        break;
    }
    printf(""\n"");
  }
  return 0;
}"
Insertion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. While inserting a new node, the new node is always inserted as a RED node. After insertion of a new node, if the tree is violating the properties of the red-black tree then, we do the following operations. Recolor Rotation Following steps are followed for inserting a new element into a red-black tree: The newNode be:

		
			New node Let y be the leaf (ie. NIL) and x be the root of the tree. The new node is inserted in the following tree.
		
			Initial tree Check if the tree is empty (ie. whether x is NIL). If yes, insert newNode as a root node and color it black. Else, repeat steps following steps until leaf (NIL) is reached.
		
			Compare newKey with rootKey.
			If newKey is greater than rootKey, traverse through the right subtree.
			Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Compare newKey with rootKey. If newKey is greater than rootKey, traverse through the right subtree. Else traverse through the left subtree.
				
					Path leading to the node where newNode is to be inserted Assign the parent of the leaf as parent of newNode. If leafKey is greater than newKey, make newNode as rightChild. Else, make newNode as leftChild.
		
			New node inserted Assign NULL to the left and rightChild of newNode. Assign RED color to newNode.
		
			Set the color of the newNode red and assign null to the children Call InsertFix-algorithm to maintain the property of red-black tree if violated. Why newly inserted nodes are always red in a red-black tree?  This is because inserting a red node does not violate the depth property of a red-black tree. If you attach a red node to a red node, then the rule is violated but it is easier to fix this problem than the problem introduced by violating the depth property. This algorithm is used for maintaining the property of a red-black tree if insertion of a newNode violates this property. Do the following until the parent of newNode p is RED. If p is the left child of grandParent gP of newNode, do the following.
		Case-I:
		
			If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change
				
			
			Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode
				
			
			Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III:
			(Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change
				
			
			Right-Rotate gP.
				
					Right Rotate If the color of the right child of gP of newNode is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
				
					Color change Assign gP to newNode.
				
					Reassigning newNode
				
				
				Case-II: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Else if newNode is the right child of p then, assign p to newNode.
				
					Assigning parent of newNode as newNode Left-Rotate newNode.
				
					Left Rotate
				
				
				Case-III: (Before moving on to this step, while loop is checked. If conditions are not satisfied, it the loop is broken.)
				Set color of p as BLACK and color of gP as RED.
				
					Color change Right-Rotate gP.
				
					Right Rotate Else, do the following.
		
			If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED.
			Assign gP to newNode.
			Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode.
			Set color of p as BLACK and color of gP as RED.
			Left-Rotate gP. If the color of the left child of gP of z is RED, set the color of both the children of gP as BLACK and the color of gP as RED. Assign gP to newNode. Else if newNode is the left child of p then, assign p to newNode and Right-Rotate newNode. Set color of p as BLACK and color of gP as RED. Left-Rotate gP. (This step is performed after coming out of the while loop.)
		Set the root of the tree as BLACK.
		
			Set root's color black The final tree look like this:","// Implementing Red-Black Tree in C++

#include <iostream>
using namespace std;

struct Node {
  int data;
  Node *parent;
  Node *left;
  Node *right;
  int color;
};

typedef Node *NodePtr;

class RedBlackTree {
   private:
  NodePtr root;
  NodePtr TNULL;

  void initializeNULLNode(NodePtr node, NodePtr parent) {
    node->data = 0;
    node->parent = parent;
    node->left = nullptr;
    node->right = nullptr;
    node->color = 0;
  }

  // Preorder
  void preOrderHelper(NodePtr node) {
    if (node != TNULL) {
      cout << node->data << "" "";
      preOrderHelper(node->left);
      preOrderHelper(node->right);
    }
  }

  // Inorder
  void inOrderHelper(NodePtr node) {
    if (node != TNULL) {
      inOrderHelper(node->left);
      cout << node->data << "" "";
      inOrderHelper(node->right);
    }
  }

  // Post order
  void postOrderHelper(NodePtr node) {
    if (node != TNULL) {
      postOrderHelper(node->left);
      postOrderHelper(node->right);
      cout << node->data << "" "";
    }
  }

  NodePtr searchTreeHelper(NodePtr node, int key) {
    if (node == TNULL || key == node->data) {
      return node;
    }

    if (key < node->data) {
      return searchTreeHelper(node->left, key);
    }
    return searchTreeHelper(node->right, key);
  }

  // For balancing the tree after deletion
  void deleteFix(NodePtr x) {
    NodePtr s;
    while (x != root && x->color == 0) {
      if (x == x->parent->left) {
        s = x->parent->right;
        if (s->color == 1) {
          s->color = 0;
          x->parent->color = 1;
          leftRotate(x->parent);
          s = x->parent->right;
        }

        if (s->left->color == 0 && s->right->color == 0) {
          s->color = 1;
          x = x->parent;
        } else {
          if (s->right->color == 0) {
            s->left->color = 0;
            s->color = 1;
            rightRotate(s);
            s = x->parent->right;
          }

          s->color = x->parent->color;
          x->parent->color = 0;
          s->right->color = 0;
          leftRotate(x->parent);
          x = root;
        }
      } else {
        s = x->parent->left;
        if (s->color == 1) {
          s->color = 0;
          x->parent->color = 1;
          rightRotate(x->parent);
          s = x->parent->left;
        }

        if (s->right->color == 0 && s->right->color == 0) {
          s->color = 1;
          x = x->parent;
        } else {
          if (s->left->color == 0) {
            s->right->color = 0;
            s->color = 1;
            leftRotate(s);
            s = x->parent->left;
          }

          s->color = x->parent->color;
          x->parent->color = 0;
          s->left->color = 0;
          rightRotate(x->parent);
          x = root;
        }
      }
    }
    x->color = 0;
  }

  void rbTransplant(NodePtr u, NodePtr v) {
    if (u->parent == nullptr) {
      root = v;
    } else if (u == u->parent->left) {
      u->parent->left = v;
    } else {
      u->parent->right = v;
    }
    v->parent = u->parent;
  }

  void deleteNodeHelper(NodePtr node, int key) {
    NodePtr z = TNULL;
    NodePtr x, y;
    while (node != TNULL) {
      if (node->data == key) {
        z = node;
      }

      if (node->data <= key) {
        node = node->right;
      } else {
        node = node->left;
      }
    }

    if (z == TNULL) {
      cout << ""Key not found in the tree"" << endl;
      return;
    }

    y = z;
    int y_original_color = y->color;
    if (z->left == TNULL) {
      x = z->right;
      rbTransplant(z, z->right);
    } else if (z->right == TNULL) {
      x = z->left;
      rbTransplant(z, z->left);
    } else {
      y = minimum(z->right);
      y_original_color = y->color;
      x = y->right;
      if (y->parent == z) {
        x->parent = y;
      } else {
        rbTransplant(y, y->right);
        y->right = z->right;
        y->right->parent = y;
      }

      rbTransplant(z, y);
      y->left = z->left;
      y->left->parent = y;
      y->color = z->color;
    }
    delete z;
    if (y_original_color == 0) {
      deleteFix(x);
    }
  }

  // For balancing the tree after insertion
  void insertFix(NodePtr k) {
    NodePtr u;
    while (k->parent->color == 1) {
      if (k->parent == k->parent->parent->right) {
        u = k->parent->parent->left;
        if (u->color == 1) {
          u->color = 0;
          k->parent->color = 0;
          k->parent->parent->color = 1;
          k = k->parent->parent;
        } else {
          if (k == k->parent->left) {
            k = k->parent;
            rightRotate(k);
          }
          k->parent->color = 0;
          k->parent->parent->color = 1;
          leftRotate(k->parent->parent);
        }
      } else {
        u = k->parent->parent->right;

        if (u->color == 1) {
          u->color = 0;
          k->parent->color = 0;
          k->parent->parent->color = 1;
          k = k->parent->parent;
        } else {
          if (k == k->parent->right) {
            k = k->parent;
            leftRotate(k);
          }
          k->parent->color = 0;
          k->parent->parent->color = 1;
          rightRotate(k->parent->parent);
        }
      }
      if (k == root) {
        break;
      }
    }
    root->color = 0;
  }

  void printHelper(NodePtr root, string indent, bool last) {
    if (root != TNULL) {
      cout << indent;
      if (last) {
        cout << ""R----"";
        indent += ""   "";
      } else {
        cout << ""L----"";
        indent += ""|  "";
      }

      string sColor = root->color ? ""RED"" : ""BLACK"";
      cout << root->data << ""("" << sColor << "")"" << endl;
      printHelper(root->left, indent, false);
      printHelper(root->right, indent, true);
    }
  }

   public:
  RedBlackTree() {
    TNULL = new Node;
    TNULL->color = 0;
    TNULL->left = nullptr;
    TNULL->right = nullptr;
    root = TNULL;
  }

  void preorder() {
    preOrderHelper(this->root);
  }

  void inorder() {
    inOrderHelper(this->root);
  }

  void postorder() {
    postOrderHelper(this->root);
  }

  NodePtr searchTree(int k) {
    return searchTreeHelper(this->root, k);
  }

  NodePtr minimum(NodePtr node) {
    while (node->left != TNULL) {
      node = node->left;
    }
    return node;
  }

  NodePtr maximum(NodePtr node) {
    while (node->right != TNULL) {
      node = node->right;
    }
    return node;
  }

  NodePtr successor(NodePtr x) {
    if (x->right != TNULL) {
      return minimum(x->right);
    }

    NodePtr y = x->parent;
    while (y != TNULL && x == y->right) {
      x = y;
      y = y->parent;
    }
    return y;
  }

  NodePtr predecessor(NodePtr x) {
    if (x->left != TNULL) {
      return maximum(x->left);
    }

    NodePtr y = x->parent;
    while (y != TNULL && x == y->left) {
      x = y;
      y = y->parent;
    }

    return y;
  }

  void leftRotate(NodePtr x) {
    NodePtr y = x->right;
    x->right = y->left;
    if (y->left != TNULL) {
      y->left->parent = x;
    }
    y->parent = x->parent;
    if (x->parent == nullptr) {
      this->root = y;
    } else if (x == x->parent->left) {
      x->parent->left = y;
    } else {
      x->parent->right = y;
    }
    y->left = x;
    x->parent = y;
  }

  void rightRotate(NodePtr x) {
    NodePtr y = x->left;
    x->left = y->right;
    if (y->right != TNULL) {
      y->right->parent = x;
    }
    y->parent = x->parent;
    if (x->parent == nullptr) {
      this->root = y;
    } else if (x == x->parent->right) {
      x->parent->right = y;
    } else {
      x->parent->left = y;
    }
    y->right = x;
    x->parent = y;
  }

  // Inserting a node
  void insert(int key) {
    NodePtr node = new Node;
    node->parent = nullptr;
    node->data = key;
    node->left = TNULL;
    node->right = TNULL;
    node->color = 1;

    NodePtr y = nullptr;
    NodePtr x = this->root;

    while (x != TNULL) {
      y = x;
      if (node->data < x->data) {
        x = x->left;
      } else {
        x = x->right;
      }
    }

    node->parent = y;
    if (y == nullptr) {
      root = node;
    } else if (node->data < y->data) {
      y->left = node;
    } else {
      y->right = node;
    }

    if (node->parent == nullptr) {
      node->color = 0;
      return;
    }

    if (node->parent->parent == nullptr) {
      return;
    }

    insertFix(node);
  }

  NodePtr getRoot() {
    return this->root;
  }

  void deleteNode(int data) {
    deleteNodeHelper(this->root, data);
  }

  void printTree() {
    if (root) {
      printHelper(this->root, """", true);
    }
  }
};

int main() {
  RedBlackTree bst;
  bst.insert(55);
  bst.insert(40);
  bst.insert(65);
  bst.insert(60);
  bst.insert(75);
  bst.insert(57);

  bst.printTree();
  cout << endl
     << ""After deleting"" << endl;
  bst.deleteNode(40);
  bst.printTree();
}"
Deletion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. Deleting a node may or may not disrupt the red-black properties of a red-black tree. If this action violates the red-black properties, then a fixing algorithm is used to regain the red-black properties. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Let the nodeToBeDeleted be:

		
			Node to be deleted Save the color of nodeToBeDeleted in origrinalColor.
		
			Saving original color If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild
				
			
			Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties.  However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recolorings are performed. Following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
				
					Assigning w
				
			
			If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Assign w to the sibling of x.
				
					Assigning w If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the leftChild of w as BLACK Set the color of w as RED
						
							Color change Right-Rotate w.
						
							Right rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Set the color of w as the color of the parent of x. Set the color of the parent of parent of x as BLACK. Set the color of the right child of w as BLACK.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Set x as the root of the tree.
						
							Set x as root Else same as above with right changed to left and vice versa. Set the color of x as BLACK. The workflow of the above cases can be understood with the help of the flowchart below.","# Implementing Red-Black Tree in Python


import sys


# Node creation
class Node():
    def __init__(self, item):
        self.item = item
        self.parent = None
        self.left = None
        self.right = None
        self.color = 1


class RedBlackTree():
    def __init__(self):
        self.TNULL = Node(0)
        self.TNULL.color = 0
        self.TNULL.left = None
        self.TNULL.right = None
        self.root = self.TNULL

    # Preorder
    def pre_order_helper(self, node):
        if node != TNULL:
            sys.stdout.write(node.item + "" "")
            self.pre_order_helper(node.left)
            self.pre_order_helper(node.right)

    # Inorder
    def in_order_helper(self, node):
        if node != TNULL:
            self.in_order_helper(node.left)
            sys.stdout.write(node.item + "" "")
            self.in_order_helper(node.right)

    # Postorder
    def post_order_helper(self, node):
        if node != TNULL:
            self.post_order_helper(node.left)
            self.post_order_helper(node.right)
            sys.stdout.write(node.item + "" "")

    # Search the tree
    def search_tree_helper(self, node, key):
        if node == TNULL or key == node.item:
            return node

        if key < node.item:
            return self.search_tree_helper(node.left, key)
        return self.search_tree_helper(node.right, key)

    # Balancing the tree after deletion
    def delete_fix(self, x):
        while x != self.root and x.color == 0:
            if x == x.parent.left:
                s = x.parent.right
                if s.color == 1:
                    s.color = 0
                    x.parent.color = 1
                    self.left_rotate(x.parent)
                    s = x.parent.right

                if s.left.color == 0 and s.right.color == 0:
                    s.color = 1
                    x = x.parent
                else:
                    if s.right.color == 0:
                        s.left.color = 0
                        s.color = 1
                        self.right_rotate(s)
                        s = x.parent.right

                    s.color = x.parent.color
                    x.parent.color = 0
                    s.right.color = 0
                    self.left_rotate(x.parent)
                    x = self.root
            else:
                s = x.parent.left
                if s.color == 1:
                    s.color = 0
                    x.parent.color = 1
                    self.right_rotate(x.parent)
                    s = x.parent.left

                if s.right.color == 0 and s.left.color == 0:
                    s.color = 1
                    x = x.parent
                else:
                    if s.left.color == 0:
                        s.right.color = 0
                        s.color = 1
                        self.left_rotate(s)
                        s = x.parent.left

                    s.color = x.parent.color
                    x.parent.color = 0
                    s.left.color = 0
                    self.right_rotate(x.parent)
                    x = self.root
        x.color = 0

    def __rb_transplant(self, u, v):
        if u.parent == None:
            self.root = v
        elif u == u.parent.left:
            u.parent.left = v
        else:
            u.parent.right = v
        v.parent = u.parent

    # Node deletion
    def delete_node_helper(self, node, key):
        z = self.TNULL
        while node != self.TNULL:
            if node.item == key:
                z = node

            if node.item <= key:
                node = node.right
            else:
                node = node.left

        if z == self.TNULL:
            print(""Cannot find key in the tree"")
            return

        y = z
        y_original_color = y.color
        if z.left == self.TNULL:
            x = z.right
            self.__rb_transplant(z, z.right)
        elif (z.right == self.TNULL):
            x = z.left
            self.__rb_transplant(z, z.left)
        else:
            y = self.minimum(z.right)
            y_original_color = y.color
            x = y.right
            if y.parent == z:
                x.parent = y
            else:
                self.__rb_transplant(y, y.right)
                y.right = z.right
                y.right.parent = y

            self.__rb_transplant(z, y)
            y.left = z.left
            y.left.parent = y
            y.color = z.color
        if y_original_color == 0:
            self.delete_fix(x)

    # Balance the tree after insertion
    def fix_insert(self, k):
        while k.parent.color == 1:
            if k.parent == k.parent.parent.right:
                u = k.parent.parent.left
                if u.color == 1:
                    u.color = 0
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    k = k.parent.parent
                else:
                    if k == k.parent.left:
                        k = k.parent
                        self.right_rotate(k)
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    self.left_rotate(k.parent.parent)
            else:
                u = k.parent.parent.right

                if u.color == 1:
                    u.color = 0
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    k = k.parent.parent
                else:
                    if k == k.parent.right:
                        k = k.parent
                        self.left_rotate(k)
                    k.parent.color = 0
                    k.parent.parent.color = 1
                    self.right_rotate(k.parent.parent)
            if k == self.root:
                break
        self.root.color = 0

    # Printing the tree
    def __print_helper(self, node, indent, last):
        if node != self.TNULL:
            sys.stdout.write(indent)
            if last:
                sys.stdout.write(""R----"")
                indent += ""     ""
            else:
                sys.stdout.write(""L----"")
                indent += ""|    ""

            s_color = ""RED"" if node.color == 1 else ""BLACK""
            print(str(node.item) + ""("" + s_color + "")"")
            self.__print_helper(node.left, indent, False)
            self.__print_helper(node.right, indent, True)

    def preorder(self):
        self.pre_order_helper(self.root)

    def inorder(self):
        self.in_order_helper(self.root)

    def postorder(self):
        self.post_order_helper(self.root)

    def searchTree(self, k):
        return self.search_tree_helper(self.root, k)

    def minimum(self, node):
        while node.left != self.TNULL:
            node = node.left
        return node

    def maximum(self, node):
        while node.right != self.TNULL:
            node = node.right
        return node

    def successor(self, x):
        if x.right != self.TNULL:
            return self.minimum(x.right)

        y = x.parent
        while y != self.TNULL and x == y.right:
            x = y
            y = y.parent
        return y

    def predecessor(self,  x):
        if (x.left != self.TNULL):
            return self.maximum(x.left)

        y = x.parent
        while y != self.TNULL and x == y.left:
            x = y
            y = y.parent

        return y

    def left_rotate(self, x):
        y = x.right
        x.right = y.left
        if y.left != self.TNULL:
            y.left.parent = x

        y.parent = x.parent
        if x.parent == None:
            self.root = y
        elif x == x.parent.left:
            x.parent.left = y
        else:
            x.parent.right = y
        y.left = x
        x.parent = y

    def right_rotate(self, x):
        y = x.left
        x.left = y.right
        if y.right != self.TNULL:
            y.right.parent = x

        y.parent = x.parent
        if x.parent == None:
            self.root = y
        elif x == x.parent.right:
            x.parent.right = y
        else:
            x.parent.left = y
        y.right = x
        x.parent = y

    def insert(self, key):
        node = Node(key)
        node.parent = None
        node.item = key
        node.left = self.TNULL
        node.right = self.TNULL
        node.color = 1

        y = None
        x = self.root

        while x != self.TNULL:
            y = x
            if node.item < x.item:
                x = x.left
            else:
                x = x.right

        node.parent = y
        if y == None:
            self.root = node
        elif node.item < y.item:
            y.left = node
        else:
            y.right = node

        if node.parent == None:
            node.color = 0
            return

        if node.parent.parent == None:
            return

        self.fix_insert(node)

    def get_root(self):
        return self.root

    def delete_node(self, item):
        self.delete_node_helper(self.root, item)

    def print_tree(self):
        self.__print_helper(self.root, """", True)


if __name__ == ""__main__"":
    bst = RedBlackTree()

    bst.insert(55)
    bst.insert(40)
    bst.insert(65)
    bst.insert(60)
    bst.insert(75)
    bst.insert(57)

    bst.print_tree()

    print(""\nAfter deleting an element"")
    bst.delete_node(40)
    bst.print_tree()
"
Deletion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. Deleting a node may or may not disrupt the red-black properties of a red-black tree. If this action violates the red-black properties, then a fixing algorithm is used to regain the red-black properties. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Let the nodeToBeDeleted be:

		
			Node to be deleted Save the color of nodeToBeDeleted in origrinalColor.
		
			Saving original color If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild
				
			
			Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties.  However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recolorings are performed. Following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
				
					Assigning w
				
			
			If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Assign w to the sibling of x.
				
					Assigning w If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the leftChild of w as BLACK Set the color of w as RED
						
							Color change Right-Rotate w.
						
							Right rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Set the color of w as the color of the parent of x. Set the color of the parent of parent of x as BLACK. Set the color of the right child of w as BLACK.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Set x as the root of the tree.
						
							Set x as root Else same as above with right changed to left and vice versa. Set the color of x as BLACK. The workflow of the above cases can be understood with the help of the flowchart below.","// Implementing Red-Black Tree in Java

class Node {
  int data;
  Node parent;
  Node left;
  Node right;
  int color;
}

public class RedBlackTree {
  private Node root;
  private Node TNULL;

  // Preorder
  private void preOrderHelper(Node node) {
    if (node != TNULL) {
      System.out.print(node.data + "" "");
      preOrderHelper(node.left);
      preOrderHelper(node.right);
    }
  }

  // Inorder
  private void inOrderHelper(Node node) {
    if (node != TNULL) {
      inOrderHelper(node.left);
      System.out.print(node.data + "" "");
      inOrderHelper(node.right);
    }
  }

  // Post order
  private void postOrderHelper(Node node) {
    if (node != TNULL) {
      postOrderHelper(node.left);
      postOrderHelper(node.right);
      System.out.print(node.data + "" "");
    }
  }

  // Search the tree
  private Node searchTreeHelper(Node node, int key) {
    if (node == TNULL || key == node.data) {
      return node;
    }

    if (key < node.data) {
      return searchTreeHelper(node.left, key);
    }
    return searchTreeHelper(node.right, key);
  }

  // Balance the tree after deletion of a node
  private void fixDelete(Node x) {
    Node s;
    while (x != root && x.color == 0) {
      if (x == x.parent.left) {
        s = x.parent.right;
        if (s.color == 1) {
          s.color = 0;
          x.parent.color = 1;
          leftRotate(x.parent);
          s = x.parent.right;
        }

        if (s.left.color == 0 && s.right.color == 0) {
          s.color = 1;
          x = x.parent;
        } else {
          if (s.right.color == 0) {
            s.left.color = 0;
            s.color = 1;
            rightRotate(s);
            s = x.parent.right;
          }

          s.color = x.parent.color;
          x.parent.color = 0;
          s.right.color = 0;
          leftRotate(x.parent);
          x = root;
        }
      } else {
        s = x.parent.left;
        if (s.color == 1) {
          s.color = 0;
          x.parent.color = 1;
          rightRotate(x.parent);
          s = x.parent.left;
        }

        if (s.right.color == 0 && s.right.color == 0) {
          s.color = 1;
          x = x.parent;
        } else {
          if (s.left.color == 0) {
            s.right.color = 0;
            s.color = 1;
            leftRotate(s);
            s = x.parent.left;
          }

          s.color = x.parent.color;
          x.parent.color = 0;
          s.left.color = 0;
          rightRotate(x.parent);
          x = root;
        }
      }
    }
    x.color = 0;
  }

  private void rbTransplant(Node u, Node v) {
    if (u.parent == null) {
      root = v;
    } else if (u == u.parent.left) {
      u.parent.left = v;
    } else {
      u.parent.right = v;
    }
    v.parent = u.parent;
  }

  private void deleteNodeHelper(Node node, int key) {
    Node z = TNULL;
    Node x, y;
    while (node != TNULL) {
      if (node.data == key) {
        z = node;
      }

      if (node.data <= key) {
        node = node.right;
      } else {
        node = node.left;
      }
    }

    if (z == TNULL) {
      System.out.println(""Couldn't find key in the tree"");
      return;
    }

    y = z;
    int yOriginalColor = y.color;
    if (z.left == TNULL) {
      x = z.right;
      rbTransplant(z, z.right);
    } else if (z.right == TNULL) {
      x = z.left;
      rbTransplant(z, z.left);
    } else {
      y = minimum(z.right);
      yOriginalColor = y.color;
      x = y.right;
      if (y.parent == z) {
        x.parent = y;
      } else {
        rbTransplant(y, y.right);
        y.right = z.right;
        y.right.parent = y;
      }

      rbTransplant(z, y);
      y.left = z.left;
      y.left.parent = y;
      y.color = z.color;
    }
    if (yOriginalColor == 0) {
      fixDelete(x);
    }
  }

  // Balance the node after insertion
  private void fixInsert(Node k) {
    Node u;
    while (k.parent.color == 1) {
      if (k.parent == k.parent.parent.right) {
        u = k.parent.parent.left;
        if (u.color == 1) {
          u.color = 0;
          k.parent.color = 0;
          k.parent.parent.color = 1;
          k = k.parent.parent;
        } else {
          if (k == k.parent.left) {
            k = k.parent;
            rightRotate(k);
          }
          k.parent.color = 0;
          k.parent.parent.color = 1;
          leftRotate(k.parent.parent);
        }
      } else {
        u = k.parent.parent.right;

        if (u.color == 1) {
          u.color = 0;
          k.parent.color = 0;
          k.parent.parent.color = 1;
          k = k.parent.parent;
        } else {
          if (k == k.parent.right) {
            k = k.parent;
            leftRotate(k);
          }
          k.parent.color = 0;
          k.parent.parent.color = 1;
          rightRotate(k.parent.parent);
        }
      }
      if (k == root) {
        break;
      }
    }
    root.color = 0;
  }

  private void printHelper(Node root, String indent, boolean last) {
    if (root != TNULL) {
      System.out.print(indent);
      if (last) {
        System.out.print(""R----"");
        indent += ""   "";
      } else {
        System.out.print(""L----"");
        indent += ""|  "";
      }

      String sColor = root.color == 1 ? ""RED"" : ""BLACK"";
      System.out.println(root.data + ""("" + sColor + "")"");
      printHelper(root.left, indent, false);
      printHelper(root.right, indent, true);
    }
  }

  public RedBlackTree() {
    TNULL = new Node();
    TNULL.color = 0;
    TNULL.left = null;
    TNULL.right = null;
    root = TNULL;
  }

  public void preorder() {
    preOrderHelper(this.root);
  }

  public void inorder() {
    inOrderHelper(this.root);
  }

  public void postorder() {
    postOrderHelper(this.root);
  }

  public Node searchTree(int k) {
    return searchTreeHelper(this.root, k);
  }

  public Node minimum(Node node) {
    while (node.left != TNULL) {
      node = node.left;
    }
    return node;
  }

  public Node maximum(Node node) {
    while (node.right != TNULL) {
      node = node.right;
    }
    return node;
  }

  public Node successor(Node x) {
    if (x.right != TNULL) {
      return minimum(x.right);
    }

    Node y = x.parent;
    while (y != TNULL && x == y.right) {
      x = y;
      y = y.parent;
    }
    return y;
  }

  public Node predecessor(Node x) {
    if (x.left != TNULL) {
      return maximum(x.left);
    }

    Node y = x.parent;
    while (y != TNULL && x == y.left) {
      x = y;
      y = y.parent;
    }

    return y;
  }

  public void leftRotate(Node x) {
    Node y = x.right;
    x.right = y.left;
    if (y.left != TNULL) {
      y.left.parent = x;
    }
    y.parent = x.parent;
    if (x.parent == null) {
      this.root = y;
    } else if (x == x.parent.left) {
      x.parent.left = y;
    } else {
      x.parent.right = y;
    }
    y.left = x;
    x.parent = y;
  }

  public void rightRotate(Node x) {
    Node y = x.left;
    x.left = y.right;
    if (y.right != TNULL) {
      y.right.parent = x;
    }
    y.parent = x.parent;
    if (x.parent == null) {
      this.root = y;
    } else if (x == x.parent.right) {
      x.parent.right = y;
    } else {
      x.parent.left = y;
    }
    y.right = x;
    x.parent = y;
  }

  public void insert(int key) {
    Node node = new Node();
    node.parent = null;
    node.data = key;
    node.left = TNULL;
    node.right = TNULL;
    node.color = 1;

    Node y = null;
    Node x = this.root;

    while (x != TNULL) {
      y = x;
      if (node.data < x.data) {
        x = x.left;
      } else {
        x = x.right;
      }
    }

    node.parent = y;
    if (y == null) {
      root = node;
    } else if (node.data < y.data) {
      y.left = node;
    } else {
      y.right = node;
    }

    if (node.parent == null) {
      node.color = 0;
      return;
    }

    if (node.parent.parent == null) {
      return;
    }

    fixInsert(node);
  }

  public Node getRoot() {
    return this.root;
  }

  public void deleteNode(int data) {
    deleteNodeHelper(this.root, data);
  }

  public void printTree() {
    printHelper(this.root, """", true);
  }

  public static void main(String[] args) {
    RedBlackTree bst = new RedBlackTree();
    bst.insert(55);
    bst.insert(40);
    bst.insert(65);
    bst.insert(60);
    bst.insert(75);
    bst.insert(57);
    bst.printTree();

    System.out.println(""\nAfter deleting:"");
    bst.deleteNode(40);
    bst.printTree();
  }
}"
Deletion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. Deleting a node may or may not disrupt the red-black properties of a red-black tree. If this action violates the red-black properties, then a fixing algorithm is used to regain the red-black properties. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Let the nodeToBeDeleted be:

		
			Node to be deleted Save the color of nodeToBeDeleted in origrinalColor.
		
			Saving original color If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild
				
			
			Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties.  However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recolorings are performed. Following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
				
					Assigning w
				
			
			If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Assign w to the sibling of x.
				
					Assigning w If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the leftChild of w as BLACK Set the color of w as RED
						
							Color change Right-Rotate w.
						
							Right rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Set the color of w as the color of the parent of x. Set the color of the parent of parent of x as BLACK. Set the color of the right child of w as BLACK.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Set x as the root of the tree.
						
							Set x as root Else same as above with right changed to left and vice versa. Set the color of x as BLACK. The workflow of the above cases can be understood with the help of the flowchart below.","// Implementing Red-Black Tree in C

#include <stdio.h>
#include <stdlib.h>

enum nodeColor {
  RED,
  BLACK
};

struct rbNode {
  int data, color;
  struct rbNode *link[2];
};

struct rbNode *root = NULL;

// Create a red-black tree
struct rbNode *createNode(int data) {
  struct rbNode *newnode;
  newnode = (struct rbNode *)malloc(sizeof(struct rbNode));
  newnode->data = data;
  newnode->color = RED;
  newnode->link[0] = newnode->link[1] = NULL;
  return newnode;
}

// Insert an node
void insertion(int data) {
  struct rbNode *stack[98], *ptr, *newnode, *xPtr, *yPtr;
  int dir[98], ht = 0, index;
  ptr = root;
  if (!root) {
    root = createNode(data);
    return;
  }

  stack[ht] = root;
  dir[ht++] = 0;
  while (ptr != NULL) {
    if (ptr->data == data) {
      printf(""Duplicates Not Allowed!!\n"");
      return;
    }
    index = (data - ptr->data) > 0 ? 1 : 0;
    stack[ht] = ptr;
    ptr = ptr->link[index];
    dir[ht++] = index;
  }
  stack[ht - 1]->link[index] = newnode = createNode(data);
  while ((ht >= 3) && (stack[ht - 1]->color == RED)) {
    if (dir[ht - 2] == 0) {
      yPtr = stack[ht - 2]->link[1];
      if (yPtr != NULL && yPtr->color == RED) {
        stack[ht - 2]->color = RED;
        stack[ht - 1]->color = yPtr->color = BLACK;
        ht = ht - 2;
      } else {
        if (dir[ht - 1] == 0) {
          yPtr = stack[ht - 1];
        } else {
          xPtr = stack[ht - 1];
          yPtr = xPtr->link[1];
          xPtr->link[1] = yPtr->link[0];
          yPtr->link[0] = xPtr;
          stack[ht - 2]->link[0] = yPtr;
        }
        xPtr = stack[ht - 2];
        xPtr->color = RED;
        yPtr->color = BLACK;
        xPtr->link[0] = yPtr->link[1];
        yPtr->link[1] = xPtr;
        if (xPtr == root) {
          root = yPtr;
        } else {
          stack[ht - 3]->link[dir[ht - 3]] = yPtr;
        }
        break;
      }
    } else {
      yPtr = stack[ht - 2]->link[0];
      if ((yPtr != NULL) && (yPtr->color == RED)) {
        stack[ht - 2]->color = RED;
        stack[ht - 1]->color = yPtr->color = BLACK;
        ht = ht - 2;
      } else {
        if (dir[ht - 1] == 1) {
          yPtr = stack[ht - 1];
        } else {
          xPtr = stack[ht - 1];
          yPtr = xPtr->link[0];
          xPtr->link[0] = yPtr->link[1];
          yPtr->link[1] = xPtr;
          stack[ht - 2]->link[1] = yPtr;
        }
        xPtr = stack[ht - 2];
        yPtr->color = BLACK;
        xPtr->color = RED;
        xPtr->link[1] = yPtr->link[0];
        yPtr->link[0] = xPtr;
        if (xPtr == root) {
          root = yPtr;
        } else {
          stack[ht - 3]->link[dir[ht - 3]] = yPtr;
        }
        break;
      }
    }
  }
  root->color = BLACK;
}

// Delete a node
void deletion(int data) {
  struct rbNode *stack[98], *ptr, *xPtr, *yPtr;
  struct rbNode *pPtr, *qPtr, *rPtr;
  int dir[98], ht = 0, diff, i;
  enum nodeColor color;

  if (!root) {
    printf(""Tree not available\n"");
    return;
  }

  ptr = root;
  while (ptr != NULL) {
    if ((data - ptr->data) == 0)
      break;
    diff = (data - ptr->data) > 0 ? 1 : 0;
    stack[ht] = ptr;
    dir[ht++] = diff;
    ptr = ptr->link[diff];
  }

  if (ptr->link[1] == NULL) {
    if ((ptr == root) && (ptr->link[0] == NULL)) {
      free(ptr);
      root = NULL;
    } else if (ptr == root) {
      root = ptr->link[0];
      free(ptr);
    } else {
      stack[ht - 1]->link[dir[ht - 1]] = ptr->link[0];
    }
  } else {
    xPtr = ptr->link[1];
    if (xPtr->link[0] == NULL) {
      xPtr->link[0] = ptr->link[0];
      color = xPtr->color;
      xPtr->color = ptr->color;
      ptr->color = color;

      if (ptr == root) {
        root = xPtr;
      } else {
        stack[ht - 1]->link[dir[ht - 1]] = xPtr;
      }

      dir[ht] = 1;
      stack[ht++] = xPtr;
    } else {
      i = ht++;
      while (1) {
        dir[ht] = 0;
        stack[ht++] = xPtr;
        yPtr = xPtr->link[0];
        if (!yPtr->link[0])
          break;
        xPtr = yPtr;
      }

      dir[i] = 1;
      stack[i] = yPtr;
      if (i > 0)
        stack[i - 1]->link[dir[i - 1]] = yPtr;

      yPtr->link[0] = ptr->link[0];

      xPtr->link[0] = yPtr->link[1];
      yPtr->link[1] = ptr->link[1];

      if (ptr == root) {
        root = yPtr;
      }

      color = yPtr->color;
      yPtr->color = ptr->color;
      ptr->color = color;
    }
  }

  if (ht < 1)
    return;

  if (ptr->color == BLACK) {
    while (1) {
      pPtr = stack[ht - 1]->link[dir[ht - 1]];
      if (pPtr && pPtr->color == RED) {
        pPtr->color = BLACK;
        break;
      }

      if (ht < 2)
        break;

      if (dir[ht - 2] == 0) {
        rPtr = stack[ht - 1]->link[1];

        if (!rPtr)
          break;

        if (rPtr->color == RED) {
          stack[ht - 1]->color = RED;
          rPtr->color = BLACK;
          stack[ht - 1]->link[1] = rPtr->link[0];
          rPtr->link[0] = stack[ht - 1];

          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          dir[ht] = 0;
          stack[ht] = stack[ht - 1];
          stack[ht - 1] = rPtr;
          ht++;

          rPtr = stack[ht - 1]->link[1];
        }

        if ((!rPtr->link[0] || rPtr->link[0]->color == BLACK) &&
          (!rPtr->link[1] || rPtr->link[1]->color == BLACK)) {
          rPtr->color = RED;
        } else {
          if (!rPtr->link[1] || rPtr->link[1]->color == BLACK) {
            qPtr = rPtr->link[0];
            rPtr->color = RED;
            qPtr->color = BLACK;
            rPtr->link[0] = qPtr->link[1];
            qPtr->link[1] = rPtr;
            rPtr = stack[ht - 1]->link[1] = qPtr;
          }
          rPtr->color = stack[ht - 1]->color;
          stack[ht - 1]->color = BLACK;
          rPtr->link[1]->color = BLACK;
          stack[ht - 1]->link[1] = rPtr->link[0];
          rPtr->link[0] = stack[ht - 1];
          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          break;
        }
      } else {
        rPtr = stack[ht - 1]->link[0];
        if (!rPtr)
          break;

        if (rPtr->color == RED) {
          stack[ht - 1]->color = RED;
          rPtr->color = BLACK;
          stack[ht - 1]->link[0] = rPtr->link[1];
          rPtr->link[1] = stack[ht - 1];

          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          dir[ht] = 1;
          stack[ht] = stack[ht - 1];
          stack[ht - 1] = rPtr;
          ht++;

          rPtr = stack[ht - 1]->link[0];
        }
        if ((!rPtr->link[0] || rPtr->link[0]->color == BLACK) &&
          (!rPtr->link[1] || rPtr->link[1]->color == BLACK)) {
          rPtr->color = RED;
        } else {
          if (!rPtr->link[0] || rPtr->link[0]->color == BLACK) {
            qPtr = rPtr->link[1];
            rPtr->color = RED;
            qPtr->color = BLACK;
            rPtr->link[1] = qPtr->link[0];
            qPtr->link[0] = rPtr;
            rPtr = stack[ht - 1]->link[0] = qPtr;
          }
          rPtr->color = stack[ht - 1]->color;
          stack[ht - 1]->color = BLACK;
          rPtr->link[0]->color = BLACK;
          stack[ht - 1]->link[0] = rPtr->link[1];
          rPtr->link[1] = stack[ht - 1];
          if (stack[ht - 1] == root) {
            root = rPtr;
          } else {
            stack[ht - 2]->link[dir[ht - 2]] = rPtr;
          }
          break;
        }
      }
      ht--;
    }
  }
}

// Print the inorder traversal of the tree
void inorderTraversal(struct rbNode *node) {
  if (node) {
    inorderTraversal(node->link[0]);
    printf(""%d  "", node->data);
    inorderTraversal(node->link[1]);
  }
  return;
}

// Driver code
int main() {
  int ch, data;
  while (1) {
    printf(""1. Insertion\t2. Deletion\n"");
    printf(""3. Traverse\t4. Exit"");
    printf(""\nEnter your choice:"");
    scanf(""%d"", &ch);
    switch (ch) {
      case 1:
        printf(""Enter the element to insert:"");
        scanf(""%d"", &data);
        insertion(data);
        break;
      case 2:
        printf(""Enter the element to delete:"");
        scanf(""%d"", &data);
        deletion(data);
        break;
      case 3:
        inorderTraversal(root);
        printf(""\n"");
        break;
      case 4:
        exit(0);
      default:
        printf(""Not available\n"");
        break;
    }
    printf(""\n"");
  }
  return 0;
}"
Deletion in a Red-Black Tree,"Red-Black tree is a self-balancing binary search tree in which each node contains an extra bit for denoting the color of the node, either red or black. Before reading this article, please refer to the article on red-black tree. Deleting a node may or may not disrupt the red-black properties of a red-black tree. If this action violates the red-black properties, then a fixing algorithm is used to regain the red-black properties. This operation removes a node from the tree. After deleting a node, the red-black property is maintained again. Let the nodeToBeDeleted be:

		
			Node to be deleted Save the color of nodeToBeDeleted in origrinalColor.
		
			Saving original color If the left child of nodeToBeDeleted is NULL
		
			Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild
				
			
			Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Assign the right child of nodeToBeDeleted to x.
				
					Assign x to the rightChild Transplant nodeToBeDeleted with x.
				
					Transplant nodeToBeDeleted with x Else if the right child of nodeToBeDeleted is NULL
		
			Assign the left child of nodeToBeDeleted into x.
			Transplant nodeToBeDeleted with x. Assign the left child of nodeToBeDeleted into x. Transplant nodeToBeDeleted with x. Else
		
			Assign the minimum of right subtree of noteToBeDeleted into y.
			Save the color of y in originalColor.
			Assign the rightChild of y into x.
			If y is a child of nodeToBeDeleted, then set the parent of x as y.
			Else, transplant y with rightChild of y.
			Transplant nodeToBeDeleted with y.
			Set the color of y with originalColor. Assign the minimum of right subtree of noteToBeDeleted into y. Save the color of y in originalColor. Assign the rightChild of y into x. If y is a child of nodeToBeDeleted, then set the parent of x as y. Else, transplant y with rightChild of y. Transplant nodeToBeDeleted with y. Set the color of y with originalColor. If the originalColor is BLACK, call DeleteFix(x). This algorithm is implemented when a black node is deleted because it violates the black depth property of the red-black tree. This violation is corrected by assuming that node x (which is occupying y's original position) has an extra black. This makes node x neither red nor black. It is either doubly black or black-and-red. This violates the red-black properties.  However, the color attribute of x is not changed rather the extra black is represented in x's pointing to the node. The extra black can be removed if It reaches the root node. If x points to a red-black node. In this case, x is colored black. Suitable rotations and recolorings are performed. Following algorithm retains the properties of a red-black tree. Do the following until the x is not the root of the tree and the color of x is BLACK If x is the left child of its parent then,
		
			Assign w to the sibling of x.
				
					Assigning w
				
			
			If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x.
				
			
			Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w
						
					
				
			
			If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Assign w to the sibling of x.
				
					Assigning w If the sibling of x is RED,
				Case-I:
				
					Set the color of the right child of the parent of x as BLACK.
					Set the color of the parent of x as RED.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the right child of the parent of x as BLACK. Set the color of the parent of x as RED.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If the color of both the right and the leftChild of w is BLACK,
				Case-II:
				
					Set the color of w as RED
					Assign the parent of x to x. Set the color of w as RED Assign the parent of x to x. Else if the color of the rightChild of w is BLACK
				Case-III:
				
					Set the color of the leftChild of w as BLACK
					Set the color of w as RED
						
							Color change
						
					
					Right-Rotate w.
						
							Right rotate
						
					
					Assign the rightChild of the parent of x to w.
						
							Reassign w Set the color of the leftChild of w as BLACK Set the color of w as RED
						
							Color change Right-Rotate w.
						
							Right rotate Assign the rightChild of the parent of x to w.
						
							Reassign w If any of the above cases do not occur, then do the following.
				Case-IV:
				
					Set the color of w as the color of the parent of x.
					Set the color of the parent of parent of x as BLACK.
					Set the color of the right child of w as BLACK.
						
							Color change
						
					
					Left-Rotate the parent of x.
						
							Left-rotate
						
					
					Set x as the root of the tree.
						
							Set x as root Set the color of w as the color of the parent of x. Set the color of the parent of parent of x as BLACK. Set the color of the right child of w as BLACK.
						
							Color change Left-Rotate the parent of x.
						
							Left-rotate Set x as the root of the tree.
						
							Set x as root Else same as above with right changed to left and vice versa. Set the color of x as BLACK. The workflow of the above cases can be understood with the help of the flowchart below.","// Implementing Red-Black Tree in C++

#include <iostream>
using namespace std;

struct Node {
  int data;
  Node *parent;
  Node *left;
  Node *right;
  int color;
};

typedef Node *NodePtr;

class RedBlackTree {
   private:
  NodePtr root;
  NodePtr TNULL;

  void initializeNULLNode(NodePtr node, NodePtr parent) {
    node->data = 0;
    node->parent = parent;
    node->left = nullptr;
    node->right = nullptr;
    node->color = 0;
  }

  // Preorder
  void preOrderHelper(NodePtr node) {
    if (node != TNULL) {
      cout << node->data << "" "";
      preOrderHelper(node->left);
      preOrderHelper(node->right);
    }
  }

  // Inorder
  void inOrderHelper(NodePtr node) {
    if (node != TNULL) {
      inOrderHelper(node->left);
      cout << node->data << "" "";
      inOrderHelper(node->right);
    }
  }

  // Post order
  void postOrderHelper(NodePtr node) {
    if (node != TNULL) {
      postOrderHelper(node->left);
      postOrderHelper(node->right);
      cout << node->data << "" "";
    }
  }

  NodePtr searchTreeHelper(NodePtr node, int key) {
    if (node == TNULL || key == node->data) {
      return node;
    }

    if (key < node->data) {
      return searchTreeHelper(node->left, key);
    }
    return searchTreeHelper(node->right, key);
  }

  // For balancing the tree after deletion
  void deleteFix(NodePtr x) {
    NodePtr s;
    while (x != root && x->color == 0) {
      if (x == x->parent->left) {
        s = x->parent->right;
        if (s->color == 1) {
          s->color = 0;
          x->parent->color = 1;
          leftRotate(x->parent);
          s = x->parent->right;
        }

        if (s->left->color == 0 && s->right->color == 0) {
          s->color = 1;
          x = x->parent;
        } else {
          if (s->right->color == 0) {
            s->left->color = 0;
            s->color = 1;
            rightRotate(s);
            s = x->parent->right;
          }

          s->color = x->parent->color;
          x->parent->color = 0;
          s->right->color = 0;
          leftRotate(x->parent);
          x = root;
        }
      } else {
        s = x->parent->left;
        if (s->color == 1) {
          s->color = 0;
          x->parent->color = 1;
          rightRotate(x->parent);
          s = x->parent->left;
        }

        if (s->right->color == 0 && s->right->color == 0) {
          s->color = 1;
          x = x->parent;
        } else {
          if (s->left->color == 0) {
            s->right->color = 0;
            s->color = 1;
            leftRotate(s);
            s = x->parent->left;
          }

          s->color = x->parent->color;
          x->parent->color = 0;
          s->left->color = 0;
          rightRotate(x->parent);
          x = root;
        }
      }
    }
    x->color = 0;
  }

  void rbTransplant(NodePtr u, NodePtr v) {
    if (u->parent == nullptr) {
      root = v;
    } else if (u == u->parent->left) {
      u->parent->left = v;
    } else {
      u->parent->right = v;
    }
    v->parent = u->parent;
  }

  void deleteNodeHelper(NodePtr node, int key) {
    NodePtr z = TNULL;
    NodePtr x, y;
    while (node != TNULL) {
      if (node->data == key) {
        z = node;
      }

      if (node->data <= key) {
        node = node->right;
      } else {
        node = node->left;
      }
    }

    if (z == TNULL) {
      cout << ""Key not found in the tree"" << endl;
      return;
    }

    y = z;
    int y_original_color = y->color;
    if (z->left == TNULL) {
      x = z->right;
      rbTransplant(z, z->right);
    } else if (z->right == TNULL) {
      x = z->left;
      rbTransplant(z, z->left);
    } else {
      y = minimum(z->right);
      y_original_color = y->color;
      x = y->right;
      if (y->parent == z) {
        x->parent = y;
      } else {
        rbTransplant(y, y->right);
        y->right = z->right;
        y->right->parent = y;
      }

      rbTransplant(z, y);
      y->left = z->left;
      y->left->parent = y;
      y->color = z->color;
    }
    delete z;
    if (y_original_color == 0) {
      deleteFix(x);
    }
  }

  // For balancing the tree after insertion
  void insertFix(NodePtr k) {
    NodePtr u;
    while (k->parent->color == 1) {
      if (k->parent == k->parent->parent->right) {
        u = k->parent->parent->left;
        if (u->color == 1) {
          u->color = 0;
          k->parent->color = 0;
          k->parent->parent->color = 1;
          k = k->parent->parent;
        } else {
          if (k == k->parent->left) {
            k = k->parent;
            rightRotate(k);
          }
          k->parent->color = 0;
          k->parent->parent->color = 1;
          leftRotate(k->parent->parent);
        }
      } else {
        u = k->parent->parent->right;

        if (u->color == 1) {
          u->color = 0;
          k->parent->color = 0;
          k->parent->parent->color = 1;
          k = k->parent->parent;
        } else {
          if (k == k->parent->right) {
            k = k->parent;
            leftRotate(k);
          }
          k->parent->color = 0;
          k->parent->parent->color = 1;
          rightRotate(k->parent->parent);
        }
      }
      if (k == root) {
        break;
      }
    }
    root->color = 0;
  }

  void printHelper(NodePtr root, string indent, bool last) {
    if (root != TNULL) {
      cout << indent;
      if (last) {
        cout << ""R----"";
        indent += ""   "";
      } else {
        cout << ""L----"";
        indent += ""|  "";
      }

      string sColor = root->color ? ""RED"" : ""BLACK"";
      cout << root->data << ""("" << sColor << "")"" << endl;
      printHelper(root->left, indent, false);
      printHelper(root->right, indent, true);
    }
  }

   public:
  RedBlackTree() {
    TNULL = new Node;
    TNULL->color = 0;
    TNULL->left = nullptr;
    TNULL->right = nullptr;
    root = TNULL;
  }

  void preorder() {
    preOrderHelper(this->root);
  }

  void inorder() {
    inOrderHelper(this->root);
  }

  void postorder() {
    postOrderHelper(this->root);
  }

  NodePtr searchTree(int k) {
    return searchTreeHelper(this->root, k);
  }

  NodePtr minimum(NodePtr node) {
    while (node->left != TNULL) {
      node = node->left;
    }
    return node;
  }

  NodePtr maximum(NodePtr node) {
    while (node->right != TNULL) {
      node = node->right;
    }
    return node;
  }

  NodePtr successor(NodePtr x) {
    if (x->right != TNULL) {
      return minimum(x->right);
    }

    NodePtr y = x->parent;
    while (y != TNULL && x == y->right) {
      x = y;
      y = y->parent;
    }
    return y;
  }

  NodePtr predecessor(NodePtr x) {
    if (x->left != TNULL) {
      return maximum(x->left);
    }

    NodePtr y = x->parent;
    while (y != TNULL && x == y->left) {
      x = y;
      y = y->parent;
    }

    return y;
  }

  void leftRotate(NodePtr x) {
    NodePtr y = x->right;
    x->right = y->left;
    if (y->left != TNULL) {
      y->left->parent = x;
    }
    y->parent = x->parent;
    if (x->parent == nullptr) {
      this->root = y;
    } else if (x == x->parent->left) {
      x->parent->left = y;
    } else {
      x->parent->right = y;
    }
    y->left = x;
    x->parent = y;
  }

  void rightRotate(NodePtr x) {
    NodePtr y = x->left;
    x->left = y->right;
    if (y->right != TNULL) {
      y->right->parent = x;
    }
    y->parent = x->parent;
    if (x->parent == nullptr) {
      this->root = y;
    } else if (x == x->parent->right) {
      x->parent->right = y;
    } else {
      x->parent->left = y;
    }
    y->right = x;
    x->parent = y;
  }

  // Inserting a node
  void insert(int key) {
    NodePtr node = new Node;
    node->parent = nullptr;
    node->data = key;
    node->left = TNULL;
    node->right = TNULL;
    node->color = 1;

    NodePtr y = nullptr;
    NodePtr x = this->root;

    while (x != TNULL) {
      y = x;
      if (node->data < x->data) {
        x = x->left;
      } else {
        x = x->right;
      }
    }

    node->parent = y;
    if (y == nullptr) {
      root = node;
    } else if (node->data < y->data) {
      y->left = node;
    } else {
      y->right = node;
    }

    if (node->parent == nullptr) {
      node->color = 0;
      return;
    }

    if (node->parent->parent == nullptr) {
      return;
    }

    insertFix(node);
  }

  NodePtr getRoot() {
    return this->root;
  }

  void deleteNode(int data) {
    deleteNodeHelper(this->root, data);
  }

  void printTree() {
    if (root) {
      printHelper(this->root, """", true);
    }
  }
};

int main() {
  RedBlackTree bst;
  bst.insert(55);
  bst.insert(40);
  bst.insert(65);
  bst.insert(60);
  bst.insert(75);
  bst.insert(57);

  bst.printTree();
  cout << endl
     << ""After deleting"" << endl;
  bst.deleteNode(40);
  bst.printTree();
}"
Strongly Connected Components,"A strongly connected component is the portion of a directed graph in which there is a path from each vertex to another vertex. It is applicable only on a directed graph. For example: Let us take the graph below. The strongly connected components of the above graph are: You can observe that in the first strongly connected component, every vertex can reach the other vertex through the directed path.  These components can be found using Kosaraju's Algorithm. Kosaraju's Algorithm is based on the depth-first search algorithm implemented twice. Three steps are involved. Perform a depth first search on the whole graph.
		
		Let us start from vertex-0, visit all of its child vertices, and mark the visited vertices as done. If a vertex leads to an already visited vertex, then push this vertex to the stack.
		
		For example: Starting from vertex-0, go to vertex-1, vertex-2, and then to vertex-3. Vertex-3 leads to already visited vertex-0, so push the source vertex (ie. vertex-3) into the stack.
		
			DFS on the graph
		
		
		Go to the previous vertex (vertex-2) and visit its child vertices i.e. vertex-4, vertex-5, vertex-6 and vertex-7 sequentially. Since there is nowhere to go from vertex-7, push it into the stack.
		
			DFS on the graph
		
		
		Go to the previous vertex (vertex-6) and visit its child vertices. But, all of its child vertices are visited, so push it into the stack.
		
			Stacking
		
		
		Similarly, a final stack is created.
		
			Final Stack Reverse the original graph.
		
			DFS on reversed graph Perform depth-first search on the reversed graph.
		
		Start from the top vertex of the stack. Traverse through all of its child vertices. Once the already visited vertex is reached, one strongly connected component is formed.
		
		For example: Pop vertex-0 from the stack. Starting from vertex-0, traverse through its child vertices (vertex-0, vertex-1, vertex-2, vertex-3 in sequence) and mark them as visited. The child of vertex-3 is already visited, so these visited vertices form one strongly connected component.
		
			Start from the top and traverse through all the vertices
		
		
		Go to the stack and pop the top vertex if already visited. Otherwise, choose the top vertex from the stack and traverse through its child vertices as presented above.
		
			Pop the top vertex if already visited
		
		 

		
			Strongly connected component Thus, the strongly connected components are:
		
			All strongly connected components Kosaraju's algorithm runs in linear time i.e. O(V+E). Vehicle routing applications
	Maps
	Model-checking in formal verification Vehicle routing applications Maps Model-checking in formal verification","# Kosaraju's algorithm to find strongly connected components in Python


from collections import defaultdict

class Graph:

    def __init__(self, vertex):
        self.V = vertex
        self.graph = defaultdict(list)

    # Add edge into the graph
    def add_edge(self, s, d):
        self.graph[s].append(d)

    # dfs
    def dfs(self, d, visited_vertex):
        visited_vertex[d] = True
        print(d, end='')
        for i in self.graph[d]:
            if not visited_vertex[i]:
                self.dfs(i, visited_vertex)

    def fill_order(self, d, visited_vertex, stack):
        visited_vertex[d] = True
        for i in self.graph[d]:
            if not visited_vertex[i]:
                self.fill_order(i, visited_vertex, stack)
        stack = stack.append(d)

    # transpose the matrix
    def transpose(self):
        g = Graph(self.V)

        for i in self.graph:
            for j in self.graph[i]:
                g.add_edge(j, i)
        return g

    # Print stongly connected components
    def print_scc(self):
        stack = []
        visited_vertex = [False] * (self.V)

        for i in range(self.V):
            if not visited_vertex[i]:
                self.fill_order(i, visited_vertex, stack)

        gr = self.transpose()

        visited_vertex = [False] * (self.V)

        while stack:
            i = stack.pop()
            if not visited_vertex[i]:
                gr.dfs(i, visited_vertex)
                print("""")


g = Graph(8)
g.add_edge(0, 1)
g.add_edge(1, 2)
g.add_edge(2, 3)
g.add_edge(2, 4)
g.add_edge(3, 0)
g.add_edge(4, 5)
g.add_edge(5, 6)
g.add_edge(6, 4)
g.add_edge(6, 7)

print(""Strongly Connected Components:"")
g.print_scc()"
Strongly Connected Components,"A strongly connected component is the portion of a directed graph in which there is a path from each vertex to another vertex. It is applicable only on a directed graph. For example: Let us take the graph below. The strongly connected components of the above graph are: You can observe that in the first strongly connected component, every vertex can reach the other vertex through the directed path.  These components can be found using Kosaraju's Algorithm. Kosaraju's Algorithm is based on the depth-first search algorithm implemented twice. Three steps are involved. Perform a depth first search on the whole graph.
		
		Let us start from vertex-0, visit all of its child vertices, and mark the visited vertices as done. If a vertex leads to an already visited vertex, then push this vertex to the stack.
		
		For example: Starting from vertex-0, go to vertex-1, vertex-2, and then to vertex-3. Vertex-3 leads to already visited vertex-0, so push the source vertex (ie. vertex-3) into the stack.
		
			DFS on the graph
		
		
		Go to the previous vertex (vertex-2) and visit its child vertices i.e. vertex-4, vertex-5, vertex-6 and vertex-7 sequentially. Since there is nowhere to go from vertex-7, push it into the stack.
		
			DFS on the graph
		
		
		Go to the previous vertex (vertex-6) and visit its child vertices. But, all of its child vertices are visited, so push it into the stack.
		
			Stacking
		
		
		Similarly, a final stack is created.
		
			Final Stack Reverse the original graph.
		
			DFS on reversed graph Perform depth-first search on the reversed graph.
		
		Start from the top vertex of the stack. Traverse through all of its child vertices. Once the already visited vertex is reached, one strongly connected component is formed.
		
		For example: Pop vertex-0 from the stack. Starting from vertex-0, traverse through its child vertices (vertex-0, vertex-1, vertex-2, vertex-3 in sequence) and mark them as visited. The child of vertex-3 is already visited, so these visited vertices form one strongly connected component.
		
			Start from the top and traverse through all the vertices
		
		
		Go to the stack and pop the top vertex if already visited. Otherwise, choose the top vertex from the stack and traverse through its child vertices as presented above.
		
			Pop the top vertex if already visited
		
		 

		
			Strongly connected component Thus, the strongly connected components are:
		
			All strongly connected components Kosaraju's algorithm runs in linear time i.e. O(V+E). Vehicle routing applications
	Maps
	Model-checking in formal verification Vehicle routing applications Maps Model-checking in formal verification","// Kosaraju's algorithm to find strongly connected components in Java

import java.util.*;
import java.util.LinkedList;

class Graph {
	private int V;
	private LinkedList<Integer> adj[];

	// Create a graph
	Graph(int s) {
		V = s;
		adj = new LinkedList[s];
		for (int i = 0; i < s; ++i)
			adj[i] = new LinkedList();
	}

  // Add edge
	void addEdge(int s, int d) {
		adj[s].add(d);
	}

	// DFS
	void DFSUtil(int s, boolean visitedVertices[]) {
		visitedVertices[s] = true;
		System.out.print(s + "" "");
		int n;

		Iterator<Integer> i = adj[s].iterator();
		while (i.hasNext()) {
			n = i.next();
			if (!visitedVertices[n])
				DFSUtil(n, visitedVertices);
		}
	}

	// Transpose the graph
	Graph Transpose() {
		Graph g = new Graph(V);
		for (int s = 0; s < V; s++) {
			Iterator<Integer> i = adj[s].listIterator();
			while (i.hasNext())
				g.adj[i.next()].add(s);
		}
		return g;
	}

	void fillOrder(int s, boolean visitedVertices[], Stack stack) {
		visitedVertices[s] = true;

		Iterator<Integer> i = adj[s].iterator();
		while (i.hasNext()) {
			int n = i.next();
			if (!visitedVertices[n])
				fillOrder(n, visitedVertices, stack);
		}
		stack.push(new Integer(s));
	}

	// Print strongly connected component
	void printSCC() {
		Stack stack = new Stack();

		boolean visitedVertices[] = new boolean[V];
		for (int i = 0; i < V; i++)
			visitedVertices[i] = false;

		for (int i = 0; i < V; i++)
			if (visitedVertices[i] == false)
				fillOrder(i, visitedVertices, stack);

		Graph gr = Transpose();

		for (int i = 0; i < V; i++)
			visitedVertices[i] = false;

		while (stack.empty() == false) {
			int s = (int) stack.pop();

			if (visitedVertices[s] == false) {
				gr.DFSUtil(s, visitedVertices);
				System.out.println();
			}
		}
	}

	public static void main(String args[]) {
		Graph g = new Graph(8);
		g.addEdge(0, 1);
		g.addEdge(1, 2);
		g.addEdge(2, 3);
		g.addEdge(2, 4);
		g.addEdge(3, 0);
		g.addEdge(4, 5);
		g.addEdge(5, 6);
		g.addEdge(6, 4);
		g.addEdge(6, 7);

		System.out.println(""Strongly Connected Components:"");
		g.printSCC();
	}
}"
Strongly Connected Components,"A strongly connected component is the portion of a directed graph in which there is a path from each vertex to another vertex. It is applicable only on a directed graph. For example: Let us take the graph below. The strongly connected components of the above graph are: You can observe that in the first strongly connected component, every vertex can reach the other vertex through the directed path.  These components can be found using Kosaraju's Algorithm. Kosaraju's Algorithm is based on the depth-first search algorithm implemented twice. Three steps are involved. Perform a depth first search on the whole graph.
		
		Let us start from vertex-0, visit all of its child vertices, and mark the visited vertices as done. If a vertex leads to an already visited vertex, then push this vertex to the stack.
		
		For example: Starting from vertex-0, go to vertex-1, vertex-2, and then to vertex-3. Vertex-3 leads to already visited vertex-0, so push the source vertex (ie. vertex-3) into the stack.
		
			DFS on the graph
		
		
		Go to the previous vertex (vertex-2) and visit its child vertices i.e. vertex-4, vertex-5, vertex-6 and vertex-7 sequentially. Since there is nowhere to go from vertex-7, push it into the stack.
		
			DFS on the graph
		
		
		Go to the previous vertex (vertex-6) and visit its child vertices. But, all of its child vertices are visited, so push it into the stack.
		
			Stacking
		
		
		Similarly, a final stack is created.
		
			Final Stack Reverse the original graph.
		
			DFS on reversed graph Perform depth-first search on the reversed graph.
		
		Start from the top vertex of the stack. Traverse through all of its child vertices. Once the already visited vertex is reached, one strongly connected component is formed.
		
		For example: Pop vertex-0 from the stack. Starting from vertex-0, traverse through its child vertices (vertex-0, vertex-1, vertex-2, vertex-3 in sequence) and mark them as visited. The child of vertex-3 is already visited, so these visited vertices form one strongly connected component.
		
			Start from the top and traverse through all the vertices
		
		
		Go to the stack and pop the top vertex if already visited. Otherwise, choose the top vertex from the stack and traverse through its child vertices as presented above.
		
			Pop the top vertex if already visited
		
		 

		
			Strongly connected component Thus, the strongly connected components are:
		
			All strongly connected components Kosaraju's algorithm runs in linear time i.e. O(V+E). Vehicle routing applications
	Maps
	Model-checking in formal verification Vehicle routing applications Maps Model-checking in formal verification","// Kosaraju's algorithm to find strongly connected components in C++

#include <iostream>
#include <list>
#include <stack>

using namespace std;

class Graph {
  int V;
  list<int> *adj;
  void fillOrder(int s, bool visitedV[], stack<int> &Stack);
  void DFS(int s, bool visitedV[]);

   public:
  Graph(int V);
  void addEdge(int s, int d);
  void printSCC();
  Graph transpose();
};

Graph::Graph(int V) {
  this->V = V;
  adj = new list<int>[V];
}

// DFS
void Graph::DFS(int s, bool visitedV[]) {
  visitedV[s] = true;
  cout << s << "" "";

  list<int>::iterator i;
  for (i = adj[s].begin(); i != adj[s].end(); ++i)
    if (!visitedV[*i])
      DFS(*i, visitedV);
}

// Transpose
Graph Graph::transpose() {
  Graph g(V);
  for (int s = 0; s < V; s++) {
    list<int>::iterator i;
    for (i = adj[s].begin(); i != adj[s].end(); ++i) {
      g.adj[*i].push_back(s);
    }
  }
  return g;
}

// Add edge into the graph
void Graph::addEdge(int s, int d) {
  adj[s].push_back(d);
}

void Graph::fillOrder(int s, bool visitedV[], stack<int> &Stack) {
  visitedV[s] = true;

  list<int>::iterator i;
  for (i = adj[s].begin(); i != adj[s].end(); ++i)
    if (!visitedV[*i])
      fillOrder(*i, visitedV, Stack);

  Stack.push(s);
}

// Print strongly connected component
void Graph::printSCC() {
  stack<int> Stack;

  bool *visitedV = new bool[V];
  for (int i = 0; i < V; i++)
    visitedV[i] = false;

  for (int i = 0; i < V; i++)
    if (visitedV[i] == false)
      fillOrder(i, visitedV, Stack);

  Graph gr = transpose();

  for (int i = 0; i < V; i++)
    visitedV[i] = false;

  while (Stack.empty() == false) {
    int s = Stack.top();
    Stack.pop();

    if (visitedV[s] == false) {
      gr.DFS(s, visitedV);
      cout << endl;
    }
  }
}

int main() {
  Graph g(8);
  g.addEdge(0, 1);
  g.addEdge(1, 2);
  g.addEdge(2, 3);
  g.addEdge(2, 4);
  g.addEdge(3, 0);
  g.addEdge(4, 5);
  g.addEdge(5, 6);
  g.addEdge(6, 4);
  g.addEdge(6, 7);

  cout << ""Strongly Connected Components:\n"";
  g.printSCC();
}"
"Graph Adjacency Matrix (With code examples in C++, Java and Python)","An adjacency matrix is a way of representing a graph as a matrix of booleans (0's and 1's). A finite graph can be represented in the form of a square matrix on a computer, where the boolean value of the matrix indicates if there is a direct path between two vertices. For example, we have a graph below. We can represent this graph in matrix form like below. Each cell in the above table/matrix is represented as Aij, where i and j are vertices. The value of Aij is either 1 or 0 depending on whether there is an edge from vertex i to vertex j.  If there is a path from i to j, then the value of Aij is 1 otherwise its 0. For instance, there is a path from vertex 1 to vertex 2, so A12 is 1 and there is no path from vertex 1 to 3, so A13 is 0. In case of undirected graphs, the matrix is symmetric about the diagonal because of every edge (i,j), there is also an edge (j,i). The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations.
	If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices.
	The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU.
	By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations. If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices. The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU. By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks.
	While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks. While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. If you know how to create two-dimensional arrays, you also know how to create an adjacency matrix. Creating routing table in networks
	Navigation tasks Creating routing table in networks Navigation tasks","# Adjacency Matrix representation in Python


class Graph(object):

    # Initialize the matrix
    def __init__(self, size):
        self.adjMatrix = []
        for i in range(size):
            self.adjMatrix.append([0 for i in range(size)])
        self.size = size

    # Add edges
    def add_edge(self, v1, v2):
        if v1 == v2:
            print(""Same vertex %d and %d"" % (v1, v2))
        self.adjMatrix[v1][v2] = 1
        self.adjMatrix[v2][v1] = 1

    # Remove edges
    def remove_edge(self, v1, v2):
        if self.adjMatrix[v1][v2] == 0:
            print(""No edge between %d and %d"" % (v1, v2))
            return
        self.adjMatrix[v1][v2] = 0
        self.adjMatrix[v2][v1] = 0

    def __len__(self):
        return self.size

    # Print the matrix
    def print_matrix(self):
        for row in self.adjMatrix:
            for val in row:
                print('{:4}'.format(val)),
            print


def main():
    g = Graph(5)
    g.add_edge(0, 1)
    g.add_edge(0, 2)
    g.add_edge(1, 2)
    g.add_edge(2, 0)
    g.add_edge(2, 3)

    g.print_matrix()


if __name__ == '__main__':
    main()"
"Graph Adjacency Matrix (With code examples in C++, Java and Python)","An adjacency matrix is a way of representing a graph as a matrix of booleans (0's and 1's). A finite graph can be represented in the form of a square matrix on a computer, where the boolean value of the matrix indicates if there is a direct path between two vertices. For example, we have a graph below. We can represent this graph in matrix form like below. Each cell in the above table/matrix is represented as Aij, where i and j are vertices. The value of Aij is either 1 or 0 depending on whether there is an edge from vertex i to vertex j.  If there is a path from i to j, then the value of Aij is 1 otherwise its 0. For instance, there is a path from vertex 1 to vertex 2, so A12 is 1 and there is no path from vertex 1 to 3, so A13 is 0. In case of undirected graphs, the matrix is symmetric about the diagonal because of every edge (i,j), there is also an edge (j,i). The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations.
	If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices.
	The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU.
	By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations. If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices. The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU. By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks.
	While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks. While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. If you know how to create two-dimensional arrays, you also know how to create an adjacency matrix. Creating routing table in networks
	Navigation tasks Creating routing table in networks Navigation tasks","// Adjacency Matrix representation in Java

public class Graph {
  private boolean adjMatrix[][];
  private int numVertices;

  // Initialize the matrix
  public Graph(int numVertices) {
    this.numVertices = numVertices;
    adjMatrix = new boolean[numVertices][numVertices];
  }

  // Add edges
  public void addEdge(int i, int j) {
    adjMatrix[i][j] = true;
    adjMatrix[j][i] = true;
  }

  // Remove edges
  public void removeEdge(int i, int j) {
    adjMatrix[i][j] = false;
    adjMatrix[j][i] = false;
  }

  // Print the matrix
  public String toString() {
    StringBuilder s = new StringBuilder();
    for (int i = 0; i < numVertices; i++) {
      s.append(i + "": "");
      for (boolean j : adjMatrix[i]) {
        s.append((j ? 1 : 0) + "" "");
      }
      s.append(""\n"");
    }
    return s.toString();
  }

  public static void main(String args[]) {
    Graph g = new Graph(4);

    g.addEdge(0, 1);
    g.addEdge(0, 2);
    g.addEdge(1, 2);
    g.addEdge(2, 0);
    g.addEdge(2, 3);

    System.out.print(g.toString());
  }
}"
"Graph Adjacency Matrix (With code examples in C++, Java and Python)","An adjacency matrix is a way of representing a graph as a matrix of booleans (0's and 1's). A finite graph can be represented in the form of a square matrix on a computer, where the boolean value of the matrix indicates if there is a direct path between two vertices. For example, we have a graph below. We can represent this graph in matrix form like below. Each cell in the above table/matrix is represented as Aij, where i and j are vertices. The value of Aij is either 1 or 0 depending on whether there is an edge from vertex i to vertex j.  If there is a path from i to j, then the value of Aij is 1 otherwise its 0. For instance, there is a path from vertex 1 to vertex 2, so A12 is 1 and there is no path from vertex 1 to 3, so A13 is 0. In case of undirected graphs, the matrix is symmetric about the diagonal because of every edge (i,j), there is also an edge (j,i). The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations.
	If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices.
	The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU.
	By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations. If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices. The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU. By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks.
	While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks. While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. If you know how to create two-dimensional arrays, you also know how to create an adjacency matrix. Creating routing table in networks
	Navigation tasks Creating routing table in networks Navigation tasks","// Adjacency Matrix representation in C

#include <stdio.h>
#define V 4

// Initialize the matrix to zero
void init(int arr[][V]) {
  int i, j;
  for (i = 0; i < V; i++)
    for (j = 0; j < V; j++)
      arr[i][j] = 0;
}

// Add edges
void addEdge(int arr[][V], int i, int j) {
  arr[i][j] = 1;
  arr[j][i] = 1;
}

// Print the matrix
void printAdjMatrix(int arr[][V]) {
  int i, j;

  for (i = 0; i < V; i++) {
    printf(""%d: "", i);
    for (j = 0; j < V; j++) {
      printf(""%d "", arr[i][j]);
    }
    printf(""\n"");
  }
}

int main() {
  int adjMatrix[V][V];

  init(adjMatrix);
  addEdge(adjMatrix, 0, 1);
  addEdge(adjMatrix, 0, 2);
  addEdge(adjMatrix, 1, 2);
  addEdge(adjMatrix, 2, 0);
  addEdge(adjMatrix, 2, 3);

  printAdjMatrix(adjMatrix);

  return 0;
}"
"Graph Adjacency Matrix (With code examples in C++, Java and Python)","An adjacency matrix is a way of representing a graph as a matrix of booleans (0's and 1's). A finite graph can be represented in the form of a square matrix on a computer, where the boolean value of the matrix indicates if there is a direct path between two vertices. For example, we have a graph below. We can represent this graph in matrix form like below. Each cell in the above table/matrix is represented as Aij, where i and j are vertices. The value of Aij is either 1 or 0 depending on whether there is an edge from vertex i to vertex j.  If there is a path from i to j, then the value of Aij is 1 otherwise its 0. For instance, there is a path from vertex 1 to vertex 2, so A12 is 1 and there is no path from vertex 1 to 3, so A13 is 0. In case of undirected graphs, the matrix is symmetric about the diagonal because of every edge (i,j), there is also an edge (j,i). The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations.
	If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices.
	The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU.
	By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The basic operations like adding an edge, removing an edge, and checking whether there is an edge from vertex i to vertex j are extremely time efficient, constant time operations. If the graph is dense and the number of edges is large, an adjacency matrix should be the first choice. Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices. The biggest advantage, however, comes from the use of matrices. The recent advances in hardware enable us to perform even expensive matrix operations on the GPU. By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks.
	While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. The VxV space requirement of the adjacency matrix makes it a memory hog. Graphs out in the wild usually don't have too many connections and this is the major reason why adjacency lists are the better choice for most tasks. While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation. If you know how to create two-dimensional arrays, you also know how to create an adjacency matrix. Creating routing table in networks
	Navigation tasks Creating routing table in networks Navigation tasks","// Adjacency Matrix representation in C++

#include <iostream>
using namespace std;

class Graph {
   private:
  bool** adjMatrix;
  int numVertices;

   public:
  // Initialize the matrix to zero
  Graph(int numVertices) {
    this->numVertices = numVertices;
    adjMatrix = new bool*[numVertices];
    for (int i = 0; i < numVertices; i++) {
      adjMatrix[i] = new bool[numVertices];
      for (int j = 0; j < numVertices; j++)
        adjMatrix[i][j] = false;
    }
  }

  // Add edges
  void addEdge(int i, int j) {
    adjMatrix[i][j] = true;
    adjMatrix[j][i] = true;
  }

  // Remove edges
  void removeEdge(int i, int j) {
    adjMatrix[i][j] = false;
    adjMatrix[j][i] = false;
  }

  // Print the martix
  void toString() {
    for (int i = 0; i < numVertices; i++) {
      cout << i << "" : "";
      for (int j = 0; j < numVertices; j++)
        cout << adjMatrix[i][j] << "" "";
      cout << ""\n"";
    }
  }

  ~Graph() {
    for (int i = 0; i < numVertices; i++)
      delete[] adjMatrix[i];
    delete[] adjMatrix;
  }
};

int main() {
  Graph g(4);

  g.addEdge(0, 1);
  g.addEdge(0, 2);
  g.addEdge(1, 2);
  g.addEdge(2, 0);
  g.addEdge(2, 3);

  g.toString();
}"
"Adjacency List (With Code in C, C++, Java and Python)","An adjacency list represents a graph as an array of linked lists. The index of the array represents a vertex and each element in its linked list represents the other vertices that form an edge with the vertex. For example, we have a graph below. We can represent this graph in the form of a linked list on a computer as shown below. Here, 0, 1, 2, 3 are the vertices and each of them forms a linked list with all of its adjacent vertices. For instance, vertex 1 has two adjacent vertices 0 and 2. Therefore, 1 is linked with 0 and 2 in the figure above. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space.
	It also helps to find all the vertices adjacent to a vertex easily. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space. It also helps to find all the vertices adjacent to a vertex easily. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. The simplest adjacency list needs a node data structure to store a vertex and a graph data structure to organize the nodes. We stay close to the basic definition of a graph - a collection of vertices and edges {V, E}. For simplicity, we use an unlabeled graph as opposed to a labeled one i.e. the vertices are identified by their indices 0,1,2,3. Let's dig into the data structures at play here.  Don't let the struct node** adjLists overwhelm you. All we are saying is we want to store a pointer to struct node*. This is because we don't know how many vertices the graph will have and so we cannot create an array of Linked Lists at compile time. It is the same structure but by using the in-built list STL data structures of C++, we make the structure a bit cleaner. We are also able to abstract the details of the implementation. We use Java Collections to store the Array of Linked Lists. The type of LinkedList is determined by what data you want to store in it. For a labeled graph, you could store a dictionary instead of an Integer There is a reason Python gets so much love. A simple dictionary of vertices and its edges is a sufficient representation of a graph. You can make the vertex itself as complex as you want. It is faster to use adjacency lists for graphs having less number of edges. It is faster to use adjacency lists for graphs having less number of edges.","# Adjascency List representation in Python


class AdjNode:
    def __init__(self, value):
        self.vertex = value
        self.next = None


class Graph:
    def __init__(self, num):
        self.V = num
        self.graph = [None] * self.V

    # Add edges
    def add_edge(self, s, d):
        node = AdjNode(d)
        node.next = self.graph[s]
        self.graph[s] = node

        node = AdjNode(s)
        node.next = self.graph[d]
        self.graph[d] = node

    # Print the graph
    def print_agraph(self):
        for i in range(self.V):
            print(""Vertex "" + str(i) + "":"", end="""")
            temp = self.graph[i]
            while temp:
                print("" -> {}"".format(temp.vertex), end="""")
                temp = temp.next
            print("" \n"")


if __name__ == ""__main__"":
    V = 5

    # Create graph and edges
    graph = Graph(V)
    graph.add_edge(0, 1)
    graph.add_edge(0, 2)
    graph.add_edge(0, 3)
    graph.add_edge(1, 2)

    graph.print_agraph()"
"Adjacency List (With Code in C, C++, Java and Python)","An adjacency list represents a graph as an array of linked lists. The index of the array represents a vertex and each element in its linked list represents the other vertices that form an edge with the vertex. For example, we have a graph below. We can represent this graph in the form of a linked list on a computer as shown below. Here, 0, 1, 2, 3 are the vertices and each of them forms a linked list with all of its adjacent vertices. For instance, vertex 1 has two adjacent vertices 0 and 2. Therefore, 1 is linked with 0 and 2 in the figure above. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space.
	It also helps to find all the vertices adjacent to a vertex easily. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space. It also helps to find all the vertices adjacent to a vertex easily. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. The simplest adjacency list needs a node data structure to store a vertex and a graph data structure to organize the nodes. We stay close to the basic definition of a graph - a collection of vertices and edges {V, E}. For simplicity, we use an unlabeled graph as opposed to a labeled one i.e. the vertices are identified by their indices 0,1,2,3. Let's dig into the data structures at play here.  Don't let the struct node** adjLists overwhelm you. All we are saying is we want to store a pointer to struct node*. This is because we don't know how many vertices the graph will have and so we cannot create an array of Linked Lists at compile time. It is the same structure but by using the in-built list STL data structures of C++, we make the structure a bit cleaner. We are also able to abstract the details of the implementation. We use Java Collections to store the Array of Linked Lists. The type of LinkedList is determined by what data you want to store in it. For a labeled graph, you could store a dictionary instead of an Integer There is a reason Python gets so much love. A simple dictionary of vertices and its edges is a sufficient representation of a graph. You can make the vertex itself as complex as you want. It is faster to use adjacency lists for graphs having less number of edges. It is faster to use adjacency lists for graphs having less number of edges.","// Adjascency List representation in Java

import java.util.*;

class Graph {

  // Add edge
  static void addEdge(ArrayList<ArrayList<Integer>> am, int s, int d) {
    am.get(s).add(d);
    am.get(d).add(s);
  }

  public static void main(String[] args) {

    // Create the graph
    int V = 5;
    ArrayList<ArrayList<Integer>> am = new ArrayList<ArrayList<Integer>>(V);

    for (int i = 0; i < V; i++)
      am.add(new ArrayList<Integer>());

    // Add edges
    addEdge(am, 0, 1);
    addEdge(am, 0, 2);
    addEdge(am, 0, 3);
    addEdge(am, 1, 2);

    printGraph(am);
  }

  // Print the graph
  static void printGraph(ArrayList<ArrayList<Integer>> am) {
    for (int i = 0; i < am.size(); i++) {
      System.out.println(""\nVertex "" + i + "":"");
      for (int j = 0; j < am.get(i).size(); j++) {
        System.out.print("" -> "" + am.get(i).get(j));
      }
      System.out.println();
    }
  }
}"
"Adjacency List (With Code in C, C++, Java and Python)","An adjacency list represents a graph as an array of linked lists. The index of the array represents a vertex and each element in its linked list represents the other vertices that form an edge with the vertex. For example, we have a graph below. We can represent this graph in the form of a linked list on a computer as shown below. Here, 0, 1, 2, 3 are the vertices and each of them forms a linked list with all of its adjacent vertices. For instance, vertex 1 has two adjacent vertices 0 and 2. Therefore, 1 is linked with 0 and 2 in the figure above. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space.
	It also helps to find all the vertices adjacent to a vertex easily. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space. It also helps to find all the vertices adjacent to a vertex easily. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. The simplest adjacency list needs a node data structure to store a vertex and a graph data structure to organize the nodes. We stay close to the basic definition of a graph - a collection of vertices and edges {V, E}. For simplicity, we use an unlabeled graph as opposed to a labeled one i.e. the vertices are identified by their indices 0,1,2,3. Let's dig into the data structures at play here.  Don't let the struct node** adjLists overwhelm you. All we are saying is we want to store a pointer to struct node*. This is because we don't know how many vertices the graph will have and so we cannot create an array of Linked Lists at compile time. It is the same structure but by using the in-built list STL data structures of C++, we make the structure a bit cleaner. We are also able to abstract the details of the implementation. We use Java Collections to store the Array of Linked Lists. The type of LinkedList is determined by what data you want to store in it. For a labeled graph, you could store a dictionary instead of an Integer There is a reason Python gets so much love. A simple dictionary of vertices and its edges is a sufficient representation of a graph. You can make the vertex itself as complex as you want. It is faster to use adjacency lists for graphs having less number of edges. It is faster to use adjacency lists for graphs having less number of edges.","// Adjascency List representation in C

#include <stdio.h>
#include <stdlib.h>

struct node {
  int vertex;
  struct node* next;
};
struct node* createNode(int);

struct Graph {
  int numVertices;
  struct node** adjLists;
};

// Create a node
struct node* createNode(int v) {
  struct node* newNode = malloc(sizeof(struct node));
  newNode->vertex = v;
  newNode->next = NULL;
  return newNode;
}

// Create a graph
struct Graph* createAGraph(int vertices) {
  struct Graph* graph = malloc(sizeof(struct Graph));
  graph->numVertices = vertices;

  graph->adjLists = malloc(vertices * sizeof(struct node*));

  int i;
  for (i = 0; i < vertices; i++)
    graph->adjLists[i] = NULL;

  return graph;
}

// Add edge
void addEdge(struct Graph* graph, int s, int d) {
  // Add edge from s to d
  struct node* newNode = createNode(d);
  newNode->next = graph->adjLists[s];
  graph->adjLists[s] = newNode;

  // Add edge from d to s
  newNode = createNode(s);
  newNode->next = graph->adjLists[d];
  graph->adjLists[d] = newNode;
}

// Print the graph
void printGraph(struct Graph* graph) {
  int v;
  for (v = 0; v < graph->numVertices; v++) {
    struct node* temp = graph->adjLists[v];
    printf(""\n Vertex %d\n: "", v);
    while (temp) {
      printf(""%d -> "", temp->vertex);
      temp = temp->next;
    }
    printf(""\n"");
  }
}

int main() {
  struct Graph* graph = createAGraph(4);
  addEdge(graph, 0, 1);
  addEdge(graph, 0, 2);
  addEdge(graph, 0, 3);
  addEdge(graph, 1, 2);

  printGraph(graph);

  return 0;
}"
"Adjacency List (With Code in C, C++, Java and Python)","An adjacency list represents a graph as an array of linked lists. The index of the array represents a vertex and each element in its linked list represents the other vertices that form an edge with the vertex. For example, we have a graph below. We can represent this graph in the form of a linked list on a computer as shown below. Here, 0, 1, 2, 3 are the vertices and each of them forms a linked list with all of its adjacent vertices. For instance, vertex 1 has two adjacent vertices 0 and 2. Therefore, 1 is linked with 0 and 2 in the figure above. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space.
	It also helps to find all the vertices adjacent to a vertex easily. An adjacency list is efficient in terms of storage because we only need to store the values for the edges. For a sparse graph with millions of vertices and edges, this can mean a lot of saved space. It also helps to find all the vertices adjacent to a vertex easily. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. Finding the adjacent list is not quicker than the adjacency matrix because all the connected nodes must be first explored to find them. The simplest adjacency list needs a node data structure to store a vertex and a graph data structure to organize the nodes. We stay close to the basic definition of a graph - a collection of vertices and edges {V, E}. For simplicity, we use an unlabeled graph as opposed to a labeled one i.e. the vertices are identified by their indices 0,1,2,3. Let's dig into the data structures at play here.  Don't let the struct node** adjLists overwhelm you. All we are saying is we want to store a pointer to struct node*. This is because we don't know how many vertices the graph will have and so we cannot create an array of Linked Lists at compile time. It is the same structure but by using the in-built list STL data structures of C++, we make the structure a bit cleaner. We are also able to abstract the details of the implementation. We use Java Collections to store the Array of Linked Lists. The type of LinkedList is determined by what data you want to store in it. For a labeled graph, you could store a dictionary instead of an Integer There is a reason Python gets so much love. A simple dictionary of vertices and its edges is a sufficient representation of a graph. You can make the vertex itself as complex as you want. It is faster to use adjacency lists for graphs having less number of edges. It is faster to use adjacency lists for graphs having less number of edges.","// Adjascency List representation in C++

#include <bits/stdc++.h>
using namespace std;

// Add edge
void addEdge(vector<int> adj[], int s, int d) {
  adj[s].push_back(d);
  adj[d].push_back(s);
}

// Print the graph
void printGraph(vector<int> adj[], int V) {
  for (int d = 0; d < V; ++d) {
    cout << ""\n Vertex ""
       << d << "":"";
    for (auto x : adj[d])
      cout << ""-> "" << x;
    printf(""\n"");
  }
}

int main() {
  int V = 5;

  // Create a graph
  vector<int> adj[V];

  // Add edges
  addEdge(adj, 0, 1);
  addEdge(adj, 0, 2);
  addEdge(adj, 0, 3);
  addEdge(adj, 1, 2);
  printGraph(adj, V);
}"
Depth First Search (DFS) Algorithm,"Depth first Search or Depth first traversal is a recursive algorithm for searching all the vertices of a graph or tree data structure. Traversal means visiting all the nodes of a graph. A standard DFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The DFS algorithm works as follows: Start by putting any one of the graph's vertices on top of a stack. Take the top item of the stack and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the top of the stack. Keep repeating steps 2 and 3 until the stack is empty. Let's see how the Depth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the DFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the top of stack i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the top of the stack and visit it. After we visit the last element 3, it doesn't have any unvisited adjacent nodes, so we have completed the Depth First Traversal of the graph. The pseudocode for DFS is shown below. In the init() function, notice that we run the DFS function on every node. This is because the graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the DFS algorithm on every node. The code for the Depth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the DFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). For finding the path To test if the graph is bipartite For finding the strongly connected components of a graph For detecting cycles in a graph","# DFS algorithm in Python


# DFS algorithm
def dfs(graph, start, visited=None):
    if visited is None:
        visited = set()
    visited.add(start)

    print(start)

    for next in graph[start] - visited:
        dfs(graph, next, visited)
    return visited


graph = {'0': set(['1', '2']),
         '1': set(['0', '3', '4']),
         '2': set(['0']),
         '3': set(['1']),
         '4': set(['2', '3'])}

dfs(graph, '0')"
Depth First Search (DFS) Algorithm,"Depth first Search or Depth first traversal is a recursive algorithm for searching all the vertices of a graph or tree data structure. Traversal means visiting all the nodes of a graph. A standard DFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The DFS algorithm works as follows: Start by putting any one of the graph's vertices on top of a stack. Take the top item of the stack and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the top of the stack. Keep repeating steps 2 and 3 until the stack is empty. Let's see how the Depth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the DFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the top of stack i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the top of the stack and visit it. After we visit the last element 3, it doesn't have any unvisited adjacent nodes, so we have completed the Depth First Traversal of the graph. The pseudocode for DFS is shown below. In the init() function, notice that we run the DFS function on every node. This is because the graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the DFS algorithm on every node. The code for the Depth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the DFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). For finding the path To test if the graph is bipartite For finding the strongly connected components of a graph For detecting cycles in a graph","// DFS algorithm in Java

import java.util.*;

class Graph {
  private LinkedList<Integer> adjLists[];
  private boolean visited[];

  // Graph creation
  Graph(int vertices) {
    adjLists = new LinkedList[vertices];
    visited = new boolean[vertices];

    for (int i = 0; i < vertices; i++)
      adjLists[i] = new LinkedList<Integer>();
  }

  // Add edges
  void addEdge(int src, int dest) {
    adjLists[src].add(dest);
  }

  // DFS algorithm
  void DFS(int vertex) {
    visited[vertex] = true;
    System.out.print(vertex + "" "");

    Iterator<Integer> ite = adjLists[vertex].listIterator();
    while (ite.hasNext()) {
      int adj = ite.next();
      if (!visited[adj])
        DFS(adj);
    }
  }

  public static void main(String args[]) {
    Graph g = new Graph(4);

    g.addEdge(0, 1);
    g.addEdge(0, 2);
    g.addEdge(1, 2);
    g.addEdge(2, 3);

    System.out.println(""Following is Depth First Traversal"");

    g.DFS(2);
  }
}"
Depth First Search (DFS) Algorithm,"Depth first Search or Depth first traversal is a recursive algorithm for searching all the vertices of a graph or tree data structure. Traversal means visiting all the nodes of a graph. A standard DFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The DFS algorithm works as follows: Start by putting any one of the graph's vertices on top of a stack. Take the top item of the stack and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the top of the stack. Keep repeating steps 2 and 3 until the stack is empty. Let's see how the Depth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the DFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the top of stack i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the top of the stack and visit it. After we visit the last element 3, it doesn't have any unvisited adjacent nodes, so we have completed the Depth First Traversal of the graph. The pseudocode for DFS is shown below. In the init() function, notice that we run the DFS function on every node. This is because the graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the DFS algorithm on every node. The code for the Depth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the DFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). For finding the path To test if the graph is bipartite For finding the strongly connected components of a graph For detecting cycles in a graph","// DFS algorithm in C

#include <stdio.h>
#include <stdlib.h>

struct node {
  int vertex;
  struct node* next;
};

struct node* createNode(int v);

struct Graph {
  int numVertices;
  int* visited;

  // We need int** to store a two dimensional array.
  // Similary, we need struct node** to store an array of Linked lists
  struct node** adjLists;
};

// DFS algo
void DFS(struct Graph* graph, int vertex) {
  struct node* adjList = graph->adjLists[vertex];
  struct node* temp = adjList;

  graph->visited[vertex] = 1;
  printf(""Visited %d \n"", vertex);

  while (temp != NULL) {
    int connectedVertex = temp->vertex;

    if (graph->visited[connectedVertex] == 0) {
      DFS(graph, connectedVertex);
    }
    temp = temp->next;
  }
}

// Create a node
struct node* createNode(int v) {
  struct node* newNode = malloc(sizeof(struct node));
  newNode->vertex = v;
  newNode->next = NULL;
  return newNode;
}

// Create graph
struct Graph* createGraph(int vertices) {
  struct Graph* graph = malloc(sizeof(struct Graph));
  graph->numVertices = vertices;

  graph->adjLists = malloc(vertices * sizeof(struct node*));

  graph->visited = malloc(vertices * sizeof(int));

  int i;
  for (i = 0; i < vertices; i++) {
    graph->adjLists[i] = NULL;
    graph->visited[i] = 0;
  }
  return graph;
}

// Add edge
void addEdge(struct Graph* graph, int src, int dest) {
  // Add edge from src to dest
  struct node* newNode = createNode(dest);
  newNode->next = graph->adjLists[src];
  graph->adjLists[src] = newNode;

  // Add edge from dest to src
  newNode = createNode(src);
  newNode->next = graph->adjLists[dest];
  graph->adjLists[dest] = newNode;
}

// Print the graph
void printGraph(struct Graph* graph) {
  int v;
  for (v = 0; v < graph->numVertices; v++) {
    struct node* temp = graph->adjLists[v];
    printf(""\n Adjacency list of vertex %d\n "", v);
    while (temp) {
      printf(""%d -> "", temp->vertex);
      temp = temp->next;
    }
    printf(""\n"");
  }
}

int main() {
  struct Graph* graph = createGraph(4);
  addEdge(graph, 0, 1);
  addEdge(graph, 0, 2);
  addEdge(graph, 1, 2);
  addEdge(graph, 2, 3);

  printGraph(graph);

  DFS(graph, 2);

  return 0;
}"
Depth First Search (DFS) Algorithm,"Depth first Search or Depth first traversal is a recursive algorithm for searching all the vertices of a graph or tree data structure. Traversal means visiting all the nodes of a graph. A standard DFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The DFS algorithm works as follows: Start by putting any one of the graph's vertices on top of a stack. Take the top item of the stack and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the top of the stack. Keep repeating steps 2 and 3 until the stack is empty. Let's see how the Depth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the DFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the top of stack i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the top of the stack and visit it. After we visit the last element 3, it doesn't have any unvisited adjacent nodes, so we have completed the Depth First Traversal of the graph. The pseudocode for DFS is shown below. In the init() function, notice that we run the DFS function on every node. This is because the graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the DFS algorithm on every node. The code for the Depth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the DFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). For finding the path To test if the graph is bipartite For finding the strongly connected components of a graph For detecting cycles in a graph","// DFS algorithm in C++

#include <iostream>
#include <list>
using namespace std;

class Graph {
  int numVertices;
  list<int> *adjLists;
  bool *visited;

   public:
  Graph(int V);
  void addEdge(int src, int dest);
  void DFS(int vertex);
};

// Initialize graph
Graph::Graph(int vertices) {
  numVertices = vertices;
  adjLists = new list<int>[vertices];
  visited = new bool[vertices];
}

// Add edges
void Graph::addEdge(int src, int dest) {
  adjLists[src].push_front(dest);
}

// DFS algorithm
void Graph::DFS(int vertex) {
  visited[vertex] = true;
  list<int> adjList = adjLists[vertex];

  cout << vertex << "" "";

  list<int>::iterator i;
  for (i = adjList.begin(); i != adjList.end(); ++i)
    if (!visited[*i])
      DFS(*i);
}

int main() {
  Graph g(4);
  g.addEdge(0, 1);
  g.addEdge(0, 2);
  g.addEdge(1, 2);
  g.addEdge(2, 3);

  g.DFS(2);

  return 0;
}"
"BFS Graph Algorithm(With code in C, C++, Java and Python)","Traversal means visiting all the nodes of a graph. Breadth First Traversal or Breadth First Search is a recursive algorithm for searching all the vertices of a graph or tree data structure. A standard BFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The algorithm works as follows: Start by putting any one of the graph's vertices at the back of a queue. Take the front item of the queue and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the back of the queue. Keep repeating steps 2 and 3 until the queue is empty. The graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the BFS algorithm on every node Let's see how the Breadth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the BFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the front of queue i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the back of the queue and visit 3, which is at the front of the queue. Only 4 remains in the queue since the only adjacent node of 3 i.e. 0 is already visited. We visit it. Since the queue is empty, we have completed the Breadth First Traversal of the graph. The code for the Breadth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the BFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). To build index by search index For GPS navigation Path finding algorithms In Ford-Fulkerson algorithm to find maximum flow in a network Cycle detection in an undirected graph In minimum spanning tree","# BFS algorithm in Python


import collections

# BFS algorithm
def bfs(graph, root):

    visited, queue = set(), collections.deque([root])
    visited.add(root)

    while queue:

        # Dequeue a vertex from queue
        vertex = queue.popleft()
        print(str(vertex) + "" "", end="""")

        # If not visited, mark it as visited, and
        # enqueue it
        for neighbour in graph[vertex]:
            if neighbour not in visited:
                visited.add(neighbour)
                queue.append(neighbour)


if __name__ == '__main__':
    graph = {0: [1, 2], 1: [2], 2: [3], 3: [1, 2]}
    print(""Following is Breadth First Traversal: "")
    bfs(graph, 0)
"
"BFS Graph Algorithm(With code in C, C++, Java and Python)","Traversal means visiting all the nodes of a graph. Breadth First Traversal or Breadth First Search is a recursive algorithm for searching all the vertices of a graph or tree data structure. A standard BFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The algorithm works as follows: Start by putting any one of the graph's vertices at the back of a queue. Take the front item of the queue and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the back of the queue. Keep repeating steps 2 and 3 until the queue is empty. The graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the BFS algorithm on every node Let's see how the Breadth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the BFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the front of queue i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the back of the queue and visit 3, which is at the front of the queue. Only 4 remains in the queue since the only adjacent node of 3 i.e. 0 is already visited. We visit it. Since the queue is empty, we have completed the Breadth First Traversal of the graph. The code for the Breadth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the BFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). To build index by search index For GPS navigation Path finding algorithms In Ford-Fulkerson algorithm to find maximum flow in a network Cycle detection in an undirected graph In minimum spanning tree","// BFS algorithm in Java

import java.util.*;

public class Graph {
  private int V;
  private LinkedList<Integer> adj[];

  // Create a graph
  Graph(int v) {
    V = v;
    adj = new LinkedList[v];
    for (int i = 0; i < v; ++i)
      adj[i] = new LinkedList();
  }

  // Add edges to the graph
  void addEdge(int v, int w) {
    adj[v].add(w);
  }

  // BFS algorithm
  void BFS(int s) {

    boolean visited[] = new boolean[V];

    LinkedList<Integer> queue = new LinkedList();

    visited[s] = true;
    queue.add(s);

    while (queue.size() != 0) {
      s = queue.poll();
      System.out.print(s + "" "");

      Iterator<Integer> i = adj[s].listIterator();
      while (i.hasNext()) {
        int n = i.next();
        if (!visited[n]) {
          visited[n] = true;
          queue.add(n);
        }
      }
    }
  }

  public static void main(String args[]) {
    Graph g = new Graph(4);

    g.addEdge(0, 1);
    g.addEdge(0, 2);
    g.addEdge(1, 2);
    g.addEdge(2, 0);
    g.addEdge(2, 3);
    g.addEdge(3, 3);

    System.out.println(""Following is Breadth First Traversal "" + ""(starting from vertex 2)"");

    g.BFS(2);
  }
}"
"BFS Graph Algorithm(With code in C, C++, Java and Python)","Traversal means visiting all the nodes of a graph. Breadth First Traversal or Breadth First Search is a recursive algorithm for searching all the vertices of a graph or tree data structure. A standard BFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The algorithm works as follows: Start by putting any one of the graph's vertices at the back of a queue. Take the front item of the queue and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the back of the queue. Keep repeating steps 2 and 3 until the queue is empty. The graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the BFS algorithm on every node Let's see how the Breadth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the BFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the front of queue i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the back of the queue and visit 3, which is at the front of the queue. Only 4 remains in the queue since the only adjacent node of 3 i.e. 0 is already visited. We visit it. Since the queue is empty, we have completed the Breadth First Traversal of the graph. The code for the Breadth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the BFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). To build index by search index For GPS navigation Path finding algorithms In Ford-Fulkerson algorithm to find maximum flow in a network Cycle detection in an undirected graph In minimum spanning tree","// BFS algorithm in C

#include <stdio.h>
#include <stdlib.h>
#define SIZE 40

struct queue {
  int items[SIZE];
  int front;
  int rear;
};

struct queue* createQueue();
void enqueue(struct queue* q, int);
int dequeue(struct queue* q);
void display(struct queue* q);
int isEmpty(struct queue* q);
void printQueue(struct queue* q);

struct node {
  int vertex;
  struct node* next;
};

struct node* createNode(int);

struct Graph {
  int numVertices;
  struct node** adjLists;
  int* visited;
};

// BFS algorithm
void bfs(struct Graph* graph, int startVertex) {
  struct queue* q = createQueue();

  graph->visited[startVertex] = 1;
  enqueue(q, startVertex);

  while (!isEmpty(q)) {
    printQueue(q);
    int currentVertex = dequeue(q);
    printf(""Visited %d\n"", currentVertex);

    struct node* temp = graph->adjLists[currentVertex];

    while (temp) {
      int adjVertex = temp->vertex;

      if (graph->visited[adjVertex] == 0) {
        graph->visited[adjVertex] = 1;
        enqueue(q, adjVertex);
      }
      temp = temp->next;
    }
  }
}

// Creating a node
struct node* createNode(int v) {
  struct node* newNode = malloc(sizeof(struct node));
  newNode->vertex = v;
  newNode->next = NULL;
  return newNode;
}

// Creating a graph
struct Graph* createGraph(int vertices) {
  struct Graph* graph = malloc(sizeof(struct Graph));
  graph->numVertices = vertices;

  graph->adjLists = malloc(vertices * sizeof(struct node*));
  graph->visited = malloc(vertices * sizeof(int));

  int i;
  for (i = 0; i < vertices; i++) {
    graph->adjLists[i] = NULL;
    graph->visited[i] = 0;
  }

  return graph;
}

// Add edge
void addEdge(struct Graph* graph, int src, int dest) {
  // Add edge from src to dest
  struct node* newNode = createNode(dest);
  newNode->next = graph->adjLists[src];
  graph->adjLists[src] = newNode;

  // Add edge from dest to src
  newNode = createNode(src);
  newNode->next = graph->adjLists[dest];
  graph->adjLists[dest] = newNode;
}

// Create a queue
struct queue* createQueue() {
  struct queue* q = malloc(sizeof(struct queue));
  q->front = -1;
  q->rear = -1;
  return q;
}

// Check if the queue is empty
int isEmpty(struct queue* q) {
  if (q->rear == -1)
    return 1;
  else
    return 0;
}

// Adding elements into queue
void enqueue(struct queue* q, int value) {
  if (q->rear == SIZE - 1)
    printf(""\nQueue is Full!!"");
  else {
    if (q->front == -1)
      q->front = 0;
    q->rear++;
    q->items[q->rear] = value;
  }
}

// Removing elements from queue
int dequeue(struct queue* q) {
  int item;
  if (isEmpty(q)) {
    printf(""Queue is empty"");
    item = -1;
  } else {
    item = q->items[q->front];
    q->front++;
    if (q->front > q->rear) {
      printf(""Resetting queue "");
      q->front = q->rear = -1;
    }
  }
  return item;
}

// Print the queue
void printQueue(struct queue* q) {
  int i = q->front;

  if (isEmpty(q)) {
    printf(""Queue is empty"");
  } else {
    printf(""\nQueue contains \n"");
    for (i = q->front; i < q->rear + 1; i++) {
      printf(""%d "", q->items[i]);
    }
  }
}

int main() {
  struct Graph* graph = createGraph(6);
  addEdge(graph, 0, 1);
  addEdge(graph, 0, 2);
  addEdge(graph, 1, 2);
  addEdge(graph, 1, 4);
  addEdge(graph, 1, 3);
  addEdge(graph, 2, 4);
  addEdge(graph, 3, 4);

  bfs(graph, 0);

  return 0;
}"
"BFS Graph Algorithm(With code in C, C++, Java and Python)","Traversal means visiting all the nodes of a graph. Breadth First Traversal or Breadth First Search is a recursive algorithm for searching all the vertices of a graph or tree data structure. A standard BFS implementation puts each vertex of the graph into one of two categories: Visited Not Visited The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The algorithm works as follows: Start by putting any one of the graph's vertices at the back of a queue. Take the front item of the queue and add it to the visited list. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the back of the queue. Keep repeating steps 2 and 3 until the queue is empty. The graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the BFS algorithm on every node Let's see how the Breadth First Search algorithm works with an example. We use an undirected graph with 5 vertices. We start from vertex 0, the BFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack. Next, we visit the element at the front of queue i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.  Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the back of the queue and visit 3, which is at the front of the queue. Only 4 remains in the queue since the only adjacent node of 3 i.e. 0 is already visited. We visit it. Since the queue is empty, we have completed the Breadth First Traversal of the graph. The code for the Breadth First Search Algorithm with an example is shown below. The code has been simplified so that we can focus on the algorithm rather than other details. The time complexity of the BFS algorithm is represented in the form of O(V + E), where V is the number of nodes and E is the number of edges. The space complexity of the algorithm is O(V). To build index by search index For GPS navigation Path finding algorithms In Ford-Fulkerson algorithm to find maximum flow in a network Cycle detection in an undirected graph In minimum spanning tree","// BFS algorithm in C++

#include <iostream>
#include <list>

using namespace std;

class Graph {
  int numVertices;
  list<int>* adjLists;
  bool* visited;

   public:
  Graph(int vertices);
  void addEdge(int src, int dest);
  void BFS(int startVertex);
};

// Create a graph with given vertices,
// and maintain an adjacency list
Graph::Graph(int vertices) {
  numVertices = vertices;
  adjLists = new list<int>[vertices];
}

// Add edges to the graph
void Graph::addEdge(int src, int dest) {
  adjLists[src].push_back(dest);
  adjLists[dest].push_back(src);
}

// BFS algorithm
void Graph::BFS(int startVertex) {
  visited = new bool[numVertices];
  for (int i = 0; i < numVertices; i++)
    visited[i] = false;

  list<int> queue;

  visited[startVertex] = true;
  queue.push_back(startVertex);

  list<int>::iterator i;

  while (!queue.empty()) {
    int currVertex = queue.front();
    cout << ""Visited "" << currVertex << "" "";
    queue.pop_front();

    for (i = adjLists[currVertex].begin(); i != adjLists[currVertex].end(); ++i) {
      int adjVertex = *i;
      if (!visited[adjVertex]) {
        visited[adjVertex] = true;
        queue.push_back(adjVertex);
      }
    }
  }
}

int main() {
  Graph g(4);
  g.addEdge(0, 1);
  g.addEdge(0, 2);
  g.addEdge(1, 2);
  g.addEdge(2, 0);
  g.addEdge(2, 3);
  g.addEdge(3, 3);

  g.BFS(2);

  return 0;
}"
Bellman Ford's Algorithm,"It is similar to Dijkstra's algorithm but it can work with graphs in which edges can have negative weights. Negative weight edges might seem useless at first but they can explain a lot of phenomena like cashflow, the heat released/absorbed in a chemical reaction, etc. For instance, if there are different ways to reach from one chemical A to another chemical B, each method will have sub-reactions involving both heat dissipation and absorption. If we want to find the set of reactions where minimum energy is required, then we will need to be able to factor in the heat absorption as negative weights and heat dissipation as positive weights. Negative weight edges can create negative weight cycles i.e. a cycle that will reduce the total path distance by coming back to the same point. Shortest path algorithms like Dijkstra's Algorithm that aren't able to detect such a cycle can give an incorrect result because they can go through a negative weight cycle and reduce the path length. Bellman Ford algorithm works by overestimating the length of the path from the starting vertex to all other vertices. Then it iteratively relaxes those estimates by finding new paths that are shorter than the previously overestimated paths.  By doing this repeatedly for all vertices, we can guarantee that the result is optimized. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length. Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. Bellman Ford's algorithm and Dijkstra's algorithm are very similar in structure. While Dijkstra looks only to the immediate neighbors of a vertex, Bellman goes through each edge in every iteration. And, the space complexity is O(V). For calculating shortest paths in routing algorithms For finding the shortest path","# Bellman Ford Algorithm in Python


class Graph:

    def __init__(self, vertices):
        self.V = vertices   # Total number of vertices in the graph
        self.graph = []     # Array of edges

    # Add edges
    def add_edge(self, s, d, w):
        self.graph.append([s, d, w])

    # Print the solution
    def print_solution(self, dist):
        print(""Vertex Distance from Source"")
        for i in range(self.V):
            print(""{0}\t\t{1}"".format(i, dist[i]))

    def bellman_ford(self, src):

        # Step 1: fill the distance array and predecessor array
        dist = [float(""Inf"")] * self.V
        # Mark the source vertex
        dist[src] = 0

        # Step 2: relax edges |V| - 1 times
        for _ in range(self.V - 1):
            for s, d, w in self.graph:
                if dist[s] != float(""Inf"") and dist[s] + w < dist[d]:
                    dist[d] = dist[s] + w

        # Step 3: detect negative cycle
        # if value changes then we have a negative cycle in the graph
        # and we cannot find the shortest distances
        for s, d, w in self.graph:
            if dist[s] != float(""Inf"") and dist[s] + w < dist[d]:
                print(""Graph contains negative weight cycle"")
                return

        # No negative weight cycle found!
        # Print the distance and predecessor array
        self.print_solution(dist)


g = Graph(5)
g.add_edge(0, 1, 5)
g.add_edge(0, 2, 4)
g.add_edge(1, 3, 3)
g.add_edge(2, 1, 6)
g.add_edge(3, 2, 2)

g.bellman_ford(0)"
Bellman Ford's Algorithm,"It is similar to Dijkstra's algorithm but it can work with graphs in which edges can have negative weights. Negative weight edges might seem useless at first but they can explain a lot of phenomena like cashflow, the heat released/absorbed in a chemical reaction, etc. For instance, if there are different ways to reach from one chemical A to another chemical B, each method will have sub-reactions involving both heat dissipation and absorption. If we want to find the set of reactions where minimum energy is required, then we will need to be able to factor in the heat absorption as negative weights and heat dissipation as positive weights. Negative weight edges can create negative weight cycles i.e. a cycle that will reduce the total path distance by coming back to the same point. Shortest path algorithms like Dijkstra's Algorithm that aren't able to detect such a cycle can give an incorrect result because they can go through a negative weight cycle and reduce the path length. Bellman Ford algorithm works by overestimating the length of the path from the starting vertex to all other vertices. Then it iteratively relaxes those estimates by finding new paths that are shorter than the previously overestimated paths.  By doing this repeatedly for all vertices, we can guarantee that the result is optimized. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length. Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. Bellman Ford's algorithm and Dijkstra's algorithm are very similar in structure. While Dijkstra looks only to the immediate neighbors of a vertex, Bellman goes through each edge in every iteration. And, the space complexity is O(V). For calculating shortest paths in routing algorithms For finding the shortest path","// Bellman Ford Algorithm in Java

class CreateGraph {

  // CreateGraph - it consists of edges
  class CreateEdge {
    int s, d, w;

    CreateEdge() {
      s = d = w = 0;
    }
  };

  int V, E;
  CreateEdge edge[];

  // Creates a graph with V vertices and E edges
  CreateGraph(int v, int e) {
    V = v;
    E = e;
    edge = new CreateEdge[e];
    for (int i = 0; i < e; ++i)
      edge[i] = new CreateEdge();
  }

  void BellmanFord(CreateGraph graph, int s) {
    int V = graph.V, E = graph.E;
    int dist[] = new int[V];

    // Step 1: fill the distance array and predecessor array
    for (int i = 0; i < V; ++i)
      dist[i] = Integer.MAX_VALUE;

    // Mark the source vertex
    dist[s] = 0;

    // Step 2: relax edges |V| - 1 times
    for (int i = 1; i < V; ++i) {
      for (int j = 0; j < E; ++j) {
        // Get the edge data
        int u = graph.edge[j].s;
        int v = graph.edge[j].d;
        int w = graph.edge[j].w;
        if (dist[u] != Integer.MAX_VALUE && dist[u] + w < dist[v])
          dist[v] = dist[u] + w;
      }
    }

    // Step 3: detect negative cycle
    // if value changes then we have a negative cycle in the graph
    // and we cannot find the shortest distances
    for (int j = 0; j < E; ++j) {
      int u = graph.edge[j].s;
      int v = graph.edge[j].d;
      int w = graph.edge[j].w;
      if (dist[u] != Integer.MAX_VALUE && dist[u] + w < dist[v]) {
        System.out.println(""CreateGraph contains negative w cycle"");
        return;
      }
    }

    // No negative w cycle found!
    // Print the distance and predecessor array
    printSolution(dist, V);
  }

  // Print the solution
  void printSolution(int dist[], int V) {
    System.out.println(""Vertex Distance from Source"");
    for (int i = 0; i < V; ++i)
      System.out.println(i + ""\t\t"" + dist[i]);
  }

  public static void main(String[] args) {
    int V = 5; // Total vertices
    int E = 8; // Total Edges

    CreateGraph graph = new CreateGraph(V, E);

    // edge 0 --> 1
    graph.edge[0].s = 0;
    graph.edge[0].d = 1;
    graph.edge[0].w = 5;

    // edge 0 --> 2
    graph.edge[1].s = 0;
    graph.edge[1].d = 2;
    graph.edge[1].w = 4;

    // edge 1 --> 3
    graph.edge[2].s = 1;
    graph.edge[2].d = 3;
    graph.edge[2].w = 3;

    // edge 2 --> 1
    graph.edge[3].s = 2;
    graph.edge[3].d = 1;
    graph.edge[3].w = 6;

    // edge 3 --> 2
    graph.edge[4].s = 3;
    graph.edge[4].d = 2;
    graph.edge[4].w = 2;

    graph.BellmanFord(graph, 0); // 0 is the source vertex
  }
}"
Bellman Ford's Algorithm,"It is similar to Dijkstra's algorithm but it can work with graphs in which edges can have negative weights. Negative weight edges might seem useless at first but they can explain a lot of phenomena like cashflow, the heat released/absorbed in a chemical reaction, etc. For instance, if there are different ways to reach from one chemical A to another chemical B, each method will have sub-reactions involving both heat dissipation and absorption. If we want to find the set of reactions where minimum energy is required, then we will need to be able to factor in the heat absorption as negative weights and heat dissipation as positive weights. Negative weight edges can create negative weight cycles i.e. a cycle that will reduce the total path distance by coming back to the same point. Shortest path algorithms like Dijkstra's Algorithm that aren't able to detect such a cycle can give an incorrect result because they can go through a negative weight cycle and reduce the path length. Bellman Ford algorithm works by overestimating the length of the path from the starting vertex to all other vertices. Then it iteratively relaxes those estimates by finding new paths that are shorter than the previously overestimated paths.  By doing this repeatedly for all vertices, we can guarantee that the result is optimized. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length. Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. Bellman Ford's algorithm and Dijkstra's algorithm are very similar in structure. While Dijkstra looks only to the immediate neighbors of a vertex, Bellman goes through each edge in every iteration. And, the space complexity is O(V). For calculating shortest paths in routing algorithms For finding the shortest path","// Bellman Ford Algorithm in C

#include <stdio.h>
#include <stdlib.h>

#define INFINITY 99999

//struct for the edges of the graph
struct Edge {
  int u;  //start vertex of the edge
  int v;  //end vertex of the edge
  int w;  //weight of the edge (u,v)
};

//Graph - it consists of edges
struct Graph {
  int V;        //total number of vertices in the graph
  int E;        //total number of edges in the graph
  struct Edge *edge;  //array of edges
};

void bellmanford(struct Graph *g, int source);
void display(int arr[], int size);

int main(void) {
  //create graph
  struct Graph *g = (struct Graph *)malloc(sizeof(struct Graph));
  g->V = 4;  //total vertices
  g->E = 5;  //total edges

  //array of edges for graph
  g->edge = (struct Edge *)malloc(g->E * sizeof(struct Edge));

  //------- adding the edges of the graph
  /*
		edge(u, v)
		where 	u = start vertex of the edge (u,v)
				v = end vertex of the edge (u,v)
		
		w is the weight of the edge (u,v)
	*/

  //edge 0 --> 1
  g->edge[0].u = 0;
  g->edge[0].v = 1;
  g->edge[0].w = 5;

  //edge 0 --> 2
  g->edge[1].u = 0;
  g->edge[1].v = 2;
  g->edge[1].w = 4;

  //edge 1 --> 3
  g->edge[2].u = 1;
  g->edge[2].v = 3;
  g->edge[2].w = 3;

  //edge 2 --> 1
  g->edge[3].u = 2;
  g->edge[3].v = 1;
  g->edge[3].w = 6;

  //edge 3 --> 2
  g->edge[4].u = 3;
  g->edge[4].v = 2;
  g->edge[4].w = 2;

  bellmanford(g, 0);  //0 is the source vertex

  return 0;
}

void bellmanford(struct Graph *g, int source) {
  //variables
  int i, j, u, v, w;

  //total vertex in the graph g
  int tV = g->V;

  //total edge in the graph g
  int tE = g->E;

  //distance array
  //size equal to the number of vertices of the graph g
  int d[tV];

  //predecessor array
  //size equal to the number of vertices of the graph g
  int p[tV];

  //step 1: fill the distance array and predecessor array
  for (i = 0; i < tV; i++) {
    d[i] = INFINITY;
    p[i] = 0;
  }

  //mark the source vertex
  d[source] = 0;

  //step 2: relax edges |V| - 1 times
  for (i = 1; i <= tV - 1; i++) {
    for (j = 0; j < tE; j++) {
      //get the edge data
      u = g->edge[j].u;
      v = g->edge[j].v;
      w = g->edge[j].w;

      if (d[u] != INFINITY && d[v] > d[u] + w) {
        d[v] = d[u] + w;
        p[v] = u;
      }
    }
  }

  //step 3: detect negative cycle
  //if value changes then we have a negative cycle in the graph
  //and we cannot find the shortest distances
  for (i = 0; i < tE; i++) {
    u = g->edge[i].u;
    v = g->edge[i].v;
    w = g->edge[i].w;
    if (d[u] != INFINITY && d[v] > d[u] + w) {
      printf(""Negative weight cycle detected!\n"");
      return;
    }
  }

  //No negative weight cycle found!
  //print the distance and predecessor array
  printf(""Distance array: "");
  display(d, tV);
  printf(""Predecessor array: "");
  display(p, tV);
}

void display(int arr[], int size) {
  int i;
  for (i = 0; i < size; i++) {
    printf(""%d "", arr[i]);
  }
  printf(""\n"");
}"
Bellman Ford's Algorithm,"It is similar to Dijkstra's algorithm but it can work with graphs in which edges can have negative weights. Negative weight edges might seem useless at first but they can explain a lot of phenomena like cashflow, the heat released/absorbed in a chemical reaction, etc. For instance, if there are different ways to reach from one chemical A to another chemical B, each method will have sub-reactions involving both heat dissipation and absorption. If we want to find the set of reactions where minimum energy is required, then we will need to be able to factor in the heat absorption as negative weights and heat dissipation as positive weights. Negative weight edges can create negative weight cycles i.e. a cycle that will reduce the total path distance by coming back to the same point. Shortest path algorithms like Dijkstra's Algorithm that aren't able to detect such a cycle can give an incorrect result because they can go through a negative weight cycle and reduce the path length. Bellman Ford algorithm works by overestimating the length of the path from the starting vertex to all other vertices. Then it iteratively relaxes those estimates by finding new paths that are shorter than the previously overestimated paths.  By doing this repeatedly for all vertices, we can guarantee that the result is optimized. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length. Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. Bellman Ford's algorithm and Dijkstra's algorithm are very similar in structure. While Dijkstra looks only to the immediate neighbors of a vertex, Bellman goes through each edge in every iteration. And, the space complexity is O(V). For calculating shortest paths in routing algorithms For finding the shortest path","// Bellman Ford Algorithm in C++

#include <bits/stdc++.h>

// Struct for the edges of the graph
struct Edge {
  int u;  //start vertex of the edge
  int v;  //end vertex of the edge
  int w;  //w of the edge (u,v)
};

// Graph - it consists of edges
struct Graph {
  int V;        // Total number of vertices in the graph
  int E;        // Total number of edges in the graph
  struct Edge* edge;  // Array of edges
};

// Creates a graph with V vertices and E edges
struct Graph* createGraph(int V, int E) {
  struct Graph* graph = new Graph;
  graph->V = V;  // Total Vertices
  graph->E = E;  // Total edges

  // Array of edges for graph
  graph->edge = new Edge[E];
  return graph;
}

// Printing the solution
void printArr(int arr[], int size) {
  int i;
  for (i = 0; i < size; i++) {
    printf(""%d "", arr[i]);
  }
  printf(""\n"");
}

void BellmanFord(struct Graph* graph, int u) {
  int V = graph->V;
  int E = graph->E;
  int dist[V];

  // Step 1: fill the distance array and predecessor array
  for (int i = 0; i < V; i++)
    dist[i] = INT_MAX;

  // Mark the source vertex
  dist[u] = 0;

  // Step 2: relax edges |V| - 1 times
  for (int i = 1; i <= V - 1; i++) {
    for (int j = 0; j < E; j++) {
      // Get the edge data
      int u = graph->edge[j].u;
      int v = graph->edge[j].v;
      int w = graph->edge[j].w;
      if (dist[u] != INT_MAX && dist[u] + w < dist[v])
        dist[v] = dist[u] + w;
    }
  }

  // Step 3: detect negative cycle
  // if value changes then we have a negative cycle in the graph
  // and we cannot find the shortest distances
  for (int i = 0; i < E; i++) {
    int u = graph->edge[i].u;
    int v = graph->edge[i].v;
    int w = graph->edge[i].w;
    if (dist[u] != INT_MAX && dist[u] + w < dist[v]) {
      printf(""Graph contains negative w cycle"");
      return;
    }
  }

  // No negative weight cycle found!
  // Print the distance and predecessor array
  printArr(dist, V);

  return;
}

int main() {
  // Create a graph
  int V = 5;  // Total vertices
  int E = 8;  // Total edges

  // Array of edges for graph
  struct Graph* graph = createGraph(V, E);

  //------- adding the edges of the graph
  /*
		edge(u, v)
		where 	u = start vertex of the edge (u,v)
				v = end vertex of the edge (u,v)
		
		w is the weight of the edge (u,v)
	*/

  //edge 0 --> 1
  graph->edge[0].u = 0;
  graph->edge[0].v = 1;
  graph->edge[0].w = 5;

  //edge 0 --> 2
  graph->edge[1].u = 0;
  graph->edge[1].v = 2;
  graph->edge[1].w = 4;

  //edge 1 --> 3
  graph->edge[2].u = 1;
  graph->edge[2].v = 3;
  graph->edge[2].w = 3;

  //edge 2 --> 1
  graph->edge[3].u = 2;
  graph->edge[3].v = 1;
  graph->edge[3].w = 6;

  //edge 3 --> 2
  graph->edge[4].u = 3;
  graph->edge[4].v = 2;
  graph->edge[4].w = 2;

  BellmanFord(graph, 0);  //0 is the source vertex

  return 0;
}"
Bubble Sort (With Code in Python/C++/Java/C),"Bubble sort is a sorting algorithm that compares two adjacent elements and swaps them until they are in the intended order. Just like the movement of air bubbles in the water that rise up to the surface, each element of the array move to the end in each iteration. Therefore, it is called a bubble sort. Suppose we are trying to sort the elements in ascending order. 1. First Iteration (Compare and Swap) Starting from the first index, compare the first and the second elements. If the first element is greater than the second element, they are swapped. Now, compare the second and the third elements. Swap them if they are not in order. The above process goes on until the last element.
		
			Compare the Adjacent Elements 2. Remaining Iteration The same process goes on for the remaining iterations. After each iteration, the largest element among the unsorted elements is placed at the end. In each iteration, the comparison takes place up to the last unsorted element.  The array is sorted when all the unsorted elements are placed at their correct positions. In the above algorithm, all the comparisons are made even if the array is already sorted. This increases the execution time. To solve this, we can introduce an extra variable swapped. The value of swapped is set true if there occurs swapping of elements. Otherwise, it is set false. After an iteration, if there is no swapping, the value of swapped will be false. This means elements are already sorted and there is no need to perform further iterations. This will reduce the execution time and helps to optimize the bubble sort. Algorithm for optimized bubble sort is Bubble Sort compares the adjacent elements. Hence, the number of comparisons is nearly equals to n2 Hence, Complexity: O(n2) Also, if we observe the code, bubble sort requires two loops. Hence, the complexity is n*n = n2 Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs.
	Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting.
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs. Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting. Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Space complexity is O(1) because an extra variable is used for swapping.
	In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Space complexity is O(1) because an extra variable is used for swapping. In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Bubble sort is used if complexity does not matter
	short and simple code is preferred complexity does not matter short and simple code is preferred Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Quicksort Insertion Sort Merge Sort Selection Sort","# Bubble sort in Python

def bubbleSort(array):
    
  # loop to access each array element
  for i in range(len(array)):

    # loop to compare array elements
    for j in range(0, len(array) - i - 1):

      # compare two adjacent elements
      # change > to < to sort in descending order
      if array[j] > array[j + 1]:

        # swapping elements if elements
        # are not in the intended order
        temp = array[j]
        array[j] = array[j+1]
        array[j+1] = temp


data = [-2, 45, 0, 11, -9]

bubbleSort(data)

print('Sorted Array in Ascending Order:')
print(data)"
Bubble Sort (With Code in Python/C++/Java/C),"Bubble sort is a sorting algorithm that compares two adjacent elements and swaps them until they are in the intended order. Just like the movement of air bubbles in the water that rise up to the surface, each element of the array move to the end in each iteration. Therefore, it is called a bubble sort. Suppose we are trying to sort the elements in ascending order. 1. First Iteration (Compare and Swap) Starting from the first index, compare the first and the second elements. If the first element is greater than the second element, they are swapped. Now, compare the second and the third elements. Swap them if they are not in order. The above process goes on until the last element.
		
			Compare the Adjacent Elements 2. Remaining Iteration The same process goes on for the remaining iterations. After each iteration, the largest element among the unsorted elements is placed at the end. In each iteration, the comparison takes place up to the last unsorted element.  The array is sorted when all the unsorted elements are placed at their correct positions. In the above algorithm, all the comparisons are made even if the array is already sorted. This increases the execution time. To solve this, we can introduce an extra variable swapped. The value of swapped is set true if there occurs swapping of elements. Otherwise, it is set false. After an iteration, if there is no swapping, the value of swapped will be false. This means elements are already sorted and there is no need to perform further iterations. This will reduce the execution time and helps to optimize the bubble sort. Algorithm for optimized bubble sort is Bubble Sort compares the adjacent elements. Hence, the number of comparisons is nearly equals to n2 Hence, Complexity: O(n2) Also, if we observe the code, bubble sort requires two loops. Hence, the complexity is n*n = n2 Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs.
	Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting.
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs. Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting. Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Space complexity is O(1) because an extra variable is used for swapping.
	In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Space complexity is O(1) because an extra variable is used for swapping. In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Bubble sort is used if complexity does not matter
	short and simple code is preferred complexity does not matter short and simple code is preferred Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Quicksort Insertion Sort Merge Sort Selection Sort","// Bubble sort in Java

import java.util.Arrays;

class Main {

  // perform the bubble sort
  static void bubbleSort(int array[]) {
    int size = array.length;
    
    // loop to access each array element
    for (int i = 0; i < size - 1; i++)
    
      // loop to compare array elements
      for (int j = 0; j < size - i - 1; j++)

        // compare two adjacent elements
        // change > to < to sort in descending order
        if (array[j] > array[j + 1]) {

          // swapping occurs if elements
          // are not in the intended order
          int temp = array[j];
          array[j] = array[j + 1];
          array[j + 1] = temp;
        }
  }

  public static void main(String args[]) {
      
    int[] data = { -2, 45, 0, 11, -9 };
    
    // call method using class name
    Main.bubbleSort(data);
    
    System.out.println(""Sorted Array in Ascending Order:"");
    System.out.println(Arrays.toString(data));
  }
}"
Bubble Sort (With Code in Python/C++/Java/C),"Bubble sort is a sorting algorithm that compares two adjacent elements and swaps them until they are in the intended order. Just like the movement of air bubbles in the water that rise up to the surface, each element of the array move to the end in each iteration. Therefore, it is called a bubble sort. Suppose we are trying to sort the elements in ascending order. 1. First Iteration (Compare and Swap) Starting from the first index, compare the first and the second elements. If the first element is greater than the second element, they are swapped. Now, compare the second and the third elements. Swap them if they are not in order. The above process goes on until the last element.
		
			Compare the Adjacent Elements 2. Remaining Iteration The same process goes on for the remaining iterations. After each iteration, the largest element among the unsorted elements is placed at the end. In each iteration, the comparison takes place up to the last unsorted element.  The array is sorted when all the unsorted elements are placed at their correct positions. In the above algorithm, all the comparisons are made even if the array is already sorted. This increases the execution time. To solve this, we can introduce an extra variable swapped. The value of swapped is set true if there occurs swapping of elements. Otherwise, it is set false. After an iteration, if there is no swapping, the value of swapped will be false. This means elements are already sorted and there is no need to perform further iterations. This will reduce the execution time and helps to optimize the bubble sort. Algorithm for optimized bubble sort is Bubble Sort compares the adjacent elements. Hence, the number of comparisons is nearly equals to n2 Hence, Complexity: O(n2) Also, if we observe the code, bubble sort requires two loops. Hence, the complexity is n*n = n2 Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs.
	Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting.
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs. Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting. Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Space complexity is O(1) because an extra variable is used for swapping.
	In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Space complexity is O(1) because an extra variable is used for swapping. In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Bubble sort is used if complexity does not matter
	short and simple code is preferred complexity does not matter short and simple code is preferred Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Quicksort Insertion Sort Merge Sort Selection Sort","// Bubble sort in C

#include <stdio.h>

// perform the bubble sort
void bubbleSort(int array[], int size) {

  // loop to access each array element
  for (int step = 0; step < size - 1; ++step) {
      
    // loop to compare array elements
    for (int i = 0; i < size - step - 1; ++i) {
      
      // compare two adjacent elements
      // change > to < to sort in descending order
      if (array[i] > array[i + 1]) {
        
        // swapping occurs if elements
        // are not in the intended order
        int temp = array[i];
        array[i] = array[i + 1];
        array[i + 1] = temp;
      }
    }
  }
}

// print array
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i) {
    printf(""%d  "", array[i]);
  }
  printf(""\n"");
}

int main() {
  int data[] = {-2, 45, 0, 11, -9};
  
  // find the array's length
  int size = sizeof(data) / sizeof(data[0]);

  bubbleSort(data, size);
  
  printf(""Sorted Array in Ascending Order:\n"");
  printArray(data, size);
}"
Bubble Sort (With Code in Python/C++/Java/C),"Bubble sort is a sorting algorithm that compares two adjacent elements and swaps them until they are in the intended order. Just like the movement of air bubbles in the water that rise up to the surface, each element of the array move to the end in each iteration. Therefore, it is called a bubble sort. Suppose we are trying to sort the elements in ascending order. 1. First Iteration (Compare and Swap) Starting from the first index, compare the first and the second elements. If the first element is greater than the second element, they are swapped. Now, compare the second and the third elements. Swap them if they are not in order. The above process goes on until the last element.
		
			Compare the Adjacent Elements 2. Remaining Iteration The same process goes on for the remaining iterations. After each iteration, the largest element among the unsorted elements is placed at the end. In each iteration, the comparison takes place up to the last unsorted element.  The array is sorted when all the unsorted elements are placed at their correct positions. In the above algorithm, all the comparisons are made even if the array is already sorted. This increases the execution time. To solve this, we can introduce an extra variable swapped. The value of swapped is set true if there occurs swapping of elements. Otherwise, it is set false. After an iteration, if there is no swapping, the value of swapped will be false. This means elements are already sorted and there is no need to perform further iterations. This will reduce the execution time and helps to optimize the bubble sort. Algorithm for optimized bubble sort is Bubble Sort compares the adjacent elements. Hence, the number of comparisons is nearly equals to n2 Hence, Complexity: O(n2) Also, if we observe the code, bubble sort requires two loops. Hence, the complexity is n*n = n2 Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs.
	Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting.
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then the worst case occurs. Best Case Complexity: O(n)
		If the array is already sorted, then there is no need for sorting. Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Space complexity is O(1) because an extra variable is used for swapping.
	In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Space complexity is O(1) because an extra variable is used for swapping. In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2). Bubble sort is used if complexity does not matter
	short and simple code is preferred complexity does not matter short and simple code is preferred Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Quicksort Insertion Sort Merge Sort Selection Sort","// Bubble sort in C++

#include <iostream>
using namespace std;

// perform bubble sort
void bubbleSort(int array[], int size) {

  // loop to access each array element
  for (int step = 0; step < size; ++step) {
      
    // loop to compare array elements
    for (int i = 0; i < size - step; ++i) {

      // compare two adjacent elements
      // change > to < to sort in descending order
      if (array[i] > array[i + 1]) {

        // swapping elements if elements
        // are not in the intended order
        int temp = array[i];
        array[i] = array[i + 1];
        array[i + 1] = temp;
      }
    }
  }
}

// print array
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i) {
    cout << ""  "" << array[i];
  }
  cout << ""\n"";
}

int main() {
  int data[] = {-2, 45, 0, 11, -9};
  
  // find array's length
  int size = sizeof(data) / sizeof(data[0]);
  
  bubbleSort(data, size);
  
  cout << ""Sorted Array in Ascending Order:\n"";  
  printArray(data, size);
}"
Selection Sort (With Code in Python/C++/Java/C),"Selection sort is a sorting algorithm that selects the smallest element from an unsorted list in each iteration and places that element at the beginning of the unsorted list. Set the first element as minimum.

		
			Select first element as minimum Compare minimum with the second element. If the second element is smaller than minimum, assign the second element as minimum.
		
		Compare minimum with the third element. Again, if the third element is smaller, then assign minimum to the third element otherwise do nothing. The process goes on until the last element.
		
			Compare minimum with the remaining elements After each iteration, minimum is placed in the front of the unsorted list.
		
			Swap the first with minimum For each iteration, indexing starts from the first unsorted element. Step 1 to 3 are repeated until all the elements are placed at their correct positions.
		
			The first iteration
		

		
			The second iteration
		

		
			The third iteration
		

		
			The fourth iteration Number of comparisons: (n - 1) + (n - 2) + (n - 3) + ..... + 1 = n(n - 1) / 2 nearly equals to n2. Complexity = O(n2) Also, we can analyze the complexity by simply observing the number of loops. There are 2 loops so the complexity is n*n = n2. Time Complexities: Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs.
	Best Case Complexity: O(n2)
		It occurs when the array is already sorted
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs. Best Case Complexity: O(n2)
		It occurs when the array is already sorted Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending).  The time complexity of the selection sort is the same in all cases. At every step, you have to find the minimum element and put it in the right place. The minimum element is not known until the end of the array is not reached. Space Complexity: Space complexity is O(1) because an extra variable temp is used. The selection sort is used when a small list is to be sorted
	cost of swapping does not matter
	checking of all the elements is compulsory
	cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) a small list is to be sorted cost of swapping does not matter checking of all the elements is compulsory cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) Bubble Sort Quicksort Insertion Sort Merge Sort","# Selection sort in Python


def selectionSort(array, size):
   
    for step in range(size):
        min_idx = step

        for i in range(step + 1, size):
         
            # to sort in descending order, change > to < in this line
            # select the minimum element in each loop
            if array[i] < array[min_idx]:
                min_idx = i
         
        # put min at the correct position
        (array[step], array[min_idx]) = (array[min_idx], array[step])


data = [-2, 45, 0, 11, -9]
size = len(data)
selectionSort(data, size)
print('Sorted Array in Ascending Order:')
print(data)"
Selection Sort (With Code in Python/C++/Java/C),"Selection sort is a sorting algorithm that selects the smallest element from an unsorted list in each iteration and places that element at the beginning of the unsorted list. Set the first element as minimum.

		
			Select first element as minimum Compare minimum with the second element. If the second element is smaller than minimum, assign the second element as minimum.
		
		Compare minimum with the third element. Again, if the third element is smaller, then assign minimum to the third element otherwise do nothing. The process goes on until the last element.
		
			Compare minimum with the remaining elements After each iteration, minimum is placed in the front of the unsorted list.
		
			Swap the first with minimum For each iteration, indexing starts from the first unsorted element. Step 1 to 3 are repeated until all the elements are placed at their correct positions.
		
			The first iteration
		

		
			The second iteration
		

		
			The third iteration
		

		
			The fourth iteration Number of comparisons: (n - 1) + (n - 2) + (n - 3) + ..... + 1 = n(n - 1) / 2 nearly equals to n2. Complexity = O(n2) Also, we can analyze the complexity by simply observing the number of loops. There are 2 loops so the complexity is n*n = n2. Time Complexities: Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs.
	Best Case Complexity: O(n2)
		It occurs when the array is already sorted
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs. Best Case Complexity: O(n2)
		It occurs when the array is already sorted Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending).  The time complexity of the selection sort is the same in all cases. At every step, you have to find the minimum element and put it in the right place. The minimum element is not known until the end of the array is not reached. Space Complexity: Space complexity is O(1) because an extra variable temp is used. The selection sort is used when a small list is to be sorted
	cost of swapping does not matter
	checking of all the elements is compulsory
	cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) a small list is to be sorted cost of swapping does not matter checking of all the elements is compulsory cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) Bubble Sort Quicksort Insertion Sort Merge Sort","// Selection sort in Java

import java.util.Arrays;

class SelectionSort {
  void selectionSort(int array[]) {
    int size = array.length;

    for (int step = 0; step < size - 1; step++) {
      int min_idx = step;

      for (int i = step + 1; i < size; i++) {

        // To sort in descending order, change > to < in this line.
        // Select the minimum element in each loop.
        if (array[i] < array[min_idx]) {
          min_idx = i;
        }
      }

      // put min at the correct position
      int temp = array[step];
      array[step] = array[min_idx];
      array[min_idx] = temp;
    }
  }

  // driver code
  public static void main(String args[]) {
    int[] data = { 20, 12, 10, 15, 2 };
    SelectionSort ss = new SelectionSort();
    ss.selectionSort(data);
    System.out.println(""Sorted Array in Ascending Order: "");
    System.out.println(Arrays.toString(data));
  }
}"
Selection Sort (With Code in Python/C++/Java/C),"Selection sort is a sorting algorithm that selects the smallest element from an unsorted list in each iteration and places that element at the beginning of the unsorted list. Set the first element as minimum.

		
			Select first element as minimum Compare minimum with the second element. If the second element is smaller than minimum, assign the second element as minimum.
		
		Compare minimum with the third element. Again, if the third element is smaller, then assign minimum to the third element otherwise do nothing. The process goes on until the last element.
		
			Compare minimum with the remaining elements After each iteration, minimum is placed in the front of the unsorted list.
		
			Swap the first with minimum For each iteration, indexing starts from the first unsorted element. Step 1 to 3 are repeated until all the elements are placed at their correct positions.
		
			The first iteration
		

		
			The second iteration
		

		
			The third iteration
		

		
			The fourth iteration Number of comparisons: (n - 1) + (n - 2) + (n - 3) + ..... + 1 = n(n - 1) / 2 nearly equals to n2. Complexity = O(n2) Also, we can analyze the complexity by simply observing the number of loops. There are 2 loops so the complexity is n*n = n2. Time Complexities: Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs.
	Best Case Complexity: O(n2)
		It occurs when the array is already sorted
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs. Best Case Complexity: O(n2)
		It occurs when the array is already sorted Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending).  The time complexity of the selection sort is the same in all cases. At every step, you have to find the minimum element and put it in the right place. The minimum element is not known until the end of the array is not reached. Space Complexity: Space complexity is O(1) because an extra variable temp is used. The selection sort is used when a small list is to be sorted
	cost of swapping does not matter
	checking of all the elements is compulsory
	cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) a small list is to be sorted cost of swapping does not matter checking of all the elements is compulsory cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) Bubble Sort Quicksort Insertion Sort Merge Sort","// Selection sort in C

#include <stdio.h>

// function to swap the the position of two elements
void swap(int *a, int *b) {
  int temp = *a;
  *a = *b;
  *b = temp;
}

void selectionSort(int array[], int size) {
  for (int step = 0; step < size - 1; step++) {
    int min_idx = step;
    for (int i = step + 1; i < size; i++) {

      // To sort in descending order, change > to < in this line.
      // Select the minimum element in each loop.
      if (array[i] < array[min_idx])
        min_idx = i;
    }

    // put min at the correct position
    swap(&array[min_idx], &array[step]);
  }
}

// function to print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i) {
    printf(""%d  "", array[i]);
  }
  printf(""\n"");
}

// driver code
int main() {
  int data[] = {20, 12, 10, 15, 2};
  int size = sizeof(data) / sizeof(data[0]);
  selectionSort(data, size);
  printf(""Sorted array in Acsending Order:\n"");
  printArray(data, size);
}"
Selection Sort (With Code in Python/C++/Java/C),"Selection sort is a sorting algorithm that selects the smallest element from an unsorted list in each iteration and places that element at the beginning of the unsorted list. Set the first element as minimum.

		
			Select first element as minimum Compare minimum with the second element. If the second element is smaller than minimum, assign the second element as minimum.
		
		Compare minimum with the third element. Again, if the third element is smaller, then assign minimum to the third element otherwise do nothing. The process goes on until the last element.
		
			Compare minimum with the remaining elements After each iteration, minimum is placed in the front of the unsorted list.
		
			Swap the first with minimum For each iteration, indexing starts from the first unsorted element. Step 1 to 3 are repeated until all the elements are placed at their correct positions.
		
			The first iteration
		

		
			The second iteration
		

		
			The third iteration
		

		
			The fourth iteration Number of comparisons: (n - 1) + (n - 2) + (n - 3) + ..... + 1 = n(n - 1) / 2 nearly equals to n2. Complexity = O(n2) Also, we can analyze the complexity by simply observing the number of loops. There are 2 loops so the complexity is n*n = n2. Time Complexities: Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs.
	Best Case Complexity: O(n2)
		It occurs when the array is already sorted
	Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		If we want to sort in ascending order and the array is in descending order then, the worst case occurs. Best Case Complexity: O(n2)
		It occurs when the array is already sorted Average Case Complexity: O(n2)
		It occurs when the elements of the array are in jumbled order (neither ascending nor descending).  The time complexity of the selection sort is the same in all cases. At every step, you have to find the minimum element and put it in the right place. The minimum element is not known until the end of the array is not reached. Space Complexity: Space complexity is O(1) because an extra variable temp is used. The selection sort is used when a small list is to be sorted
	cost of swapping does not matter
	checking of all the elements is compulsory
	cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) a small list is to be sorted cost of swapping does not matter checking of all the elements is compulsory cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort) Bubble Sort Quicksort Insertion Sort Merge Sort","// Selection sort in C++

#include <iostream>
using namespace std;

// function to swap the the position of two elements
void swap(int *a, int *b) {
  int temp = *a;
  *a = *b;
  *b = temp;
}

// function to print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; i++) {
    cout << array[i] << "" "";
  }
  cout << endl;
}

void selectionSort(int array[], int size) {
  for (int step = 0; step < size - 1; step++) {
    int min_idx = step;
    for (int i = step + 1; i < size; i++) {

      // To sort in descending order, change > to < in this line.
      // Select the minimum element in each loop.
      if (array[i] < array[min_idx])
        min_idx = i;
    }

    // put min at the correct position
    swap(&array[min_idx], &array[step]);
  }
}

// driver code
int main() {
  int data[] = {20, 12, 10, 15, 2};
  int size = sizeof(data) / sizeof(data[0]);
  selectionSort(data, size);
  cout << ""Sorted array in Acsending Order:\n"";
  printArray(data, size);
}"
Insertion Sort (With Code in Python/C++/Java/C),"Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. Insertion sort works similarly as we sort cards in our hand in a card game. We assume that the first card is already sorted then, we select an unsorted card. If the unsorted card is greater than the card in hand, it is placed on the right otherwise, to the left. In the same way, other unsorted cards are taken and put in their right place. A similar approach is used by insertion sort. Suppose we need to sort the following array. The first element in the array is assumed to be sorted. Take the second element and store it separately in key.
		
		Compare key with the first element. If the first element is greater than key, then key is placed in front of the first element.

		
			If the first element is greater than key, then key is placed in front of the first element. Now, the first two elements are sorted.
		
		Take the third element and compare it with the elements on the left of it. Placed it just behind the element smaller than it. If there is no element smaller than it, then place it at the beginning of the array.
		
			Place 1 at the beginning Similarly, place every unsorted element at its correct position.
		
			Place 4 behind 1
		

		
			Place 3 behind 1 and the array is sorted  Time Complexities Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2
	Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear.
	Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2 Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear. Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Space Complexity Space complexity is O(1) because an extra variable key is used. The insertion sort is used when: the array is has a small number of elements
	there are only a few elements left to be sorted the array is has a small number of elements there are only a few elements left to be sorted Bubble Sort Quicksort Merge Sort Selection Sort","# Insertion sort in Python


def insertionSort(array):

    for step in range(1, len(array)):
        key = array[step]
        j = step - 1
        
        # Compare key with each element on the left of it until an element smaller than it is found
        # For descending order, change key<array[j] to key>array[j].        
        while j >= 0 and key < array[j]:
            array[j + 1] = array[j]
            j = j - 1
        
        # Place key at after the element just smaller than it.
        array[j + 1] = key


data = [9, 5, 1, 4, 3]
insertionSort(data)
print('Sorted Array in Ascending Order:')
print(data)"
Insertion Sort (With Code in Python/C++/Java/C),"Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. Insertion sort works similarly as we sort cards in our hand in a card game. We assume that the first card is already sorted then, we select an unsorted card. If the unsorted card is greater than the card in hand, it is placed on the right otherwise, to the left. In the same way, other unsorted cards are taken and put in their right place. A similar approach is used by insertion sort. Suppose we need to sort the following array. The first element in the array is assumed to be sorted. Take the second element and store it separately in key.
		
		Compare key with the first element. If the first element is greater than key, then key is placed in front of the first element.

		
			If the first element is greater than key, then key is placed in front of the first element. Now, the first two elements are sorted.
		
		Take the third element and compare it with the elements on the left of it. Placed it just behind the element smaller than it. If there is no element smaller than it, then place it at the beginning of the array.
		
			Place 1 at the beginning Similarly, place every unsorted element at its correct position.
		
			Place 4 behind 1
		

		
			Place 3 behind 1 and the array is sorted  Time Complexities Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2
	Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear.
	Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2 Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear. Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Space Complexity Space complexity is O(1) because an extra variable key is used. The insertion sort is used when: the array is has a small number of elements
	there are only a few elements left to be sorted the array is has a small number of elements there are only a few elements left to be sorted Bubble Sort Quicksort Merge Sort Selection Sort","// Insertion sort in Java

import java.util.Arrays;

class InsertionSort {

  void insertionSort(int array[]) {
    int size = array.length;

    for (int step = 1; step < size; step++) {
      int key = array[step];
      int j = step - 1;

      // Compare key with each element on the left of it until an element smaller than
      // it is found.
      // For descending order, change key<array[j] to key>array[j].
      while (j >= 0 && key < array[j]) {
        array[j + 1] = array[j];
        --j;
      }

      // Place key at after the element just smaller than it.
      array[j + 1] = key;
    }
  }

  // Driver code
  public static void main(String args[]) {
    int[] data = { 9, 5, 1, 4, 3 };
    InsertionSort is = new InsertionSort();
    is.insertionSort(data);
    System.out.println(""Sorted Array in Ascending Order: "");
    System.out.println(Arrays.toString(data));
  }
}"
Insertion Sort (With Code in Python/C++/Java/C),"Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. Insertion sort works similarly as we sort cards in our hand in a card game. We assume that the first card is already sorted then, we select an unsorted card. If the unsorted card is greater than the card in hand, it is placed on the right otherwise, to the left. In the same way, other unsorted cards are taken and put in their right place. A similar approach is used by insertion sort. Suppose we need to sort the following array. The first element in the array is assumed to be sorted. Take the second element and store it separately in key.
		
		Compare key with the first element. If the first element is greater than key, then key is placed in front of the first element.

		
			If the first element is greater than key, then key is placed in front of the first element. Now, the first two elements are sorted.
		
		Take the third element and compare it with the elements on the left of it. Placed it just behind the element smaller than it. If there is no element smaller than it, then place it at the beginning of the array.
		
			Place 1 at the beginning Similarly, place every unsorted element at its correct position.
		
			Place 4 behind 1
		

		
			Place 3 behind 1 and the array is sorted  Time Complexities Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2
	Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear.
	Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2 Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear. Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Space Complexity Space complexity is O(1) because an extra variable key is used. The insertion sort is used when: the array is has a small number of elements
	there are only a few elements left to be sorted the array is has a small number of elements there are only a few elements left to be sorted Bubble Sort Quicksort Merge Sort Selection Sort","// Insertion sort in C

#include <stdio.h>

// Function to print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; i++) {
    printf(""%d "", array[i]);
  }
  printf(""\n"");
}

void insertionSort(int array[], int size) {
  for (int step = 1; step < size; step++) {
    int key = array[step];
    int j = step - 1;

    // Compare key with each element on the left of it until an element smaller than
    // it is found.
    // For descending order, change key<array[j] to key>array[j].
    while (key < array[j] && j >= 0) {
      array[j + 1] = array[j];
      --j;
    }
    array[j + 1] = key;
  }
}

// Driver code
int main() {
  int data[] = {9, 5, 1, 4, 3};
  int size = sizeof(data) / sizeof(data[0]);
  insertionSort(data, size);
  printf(""Sorted array in ascending order:\n"");
  printArray(data, size);
}"
Insertion Sort (With Code in Python/C++/Java/C),"Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. Insertion sort works similarly as we sort cards in our hand in a card game. We assume that the first card is already sorted then, we select an unsorted card. If the unsorted card is greater than the card in hand, it is placed on the right otherwise, to the left. In the same way, other unsorted cards are taken and put in their right place. A similar approach is used by insertion sort. Suppose we need to sort the following array. The first element in the array is assumed to be sorted. Take the second element and store it separately in key.
		
		Compare key with the first element. If the first element is greater than key, then key is placed in front of the first element.

		
			If the first element is greater than key, then key is placed in front of the first element. Now, the first two elements are sorted.
		
		Take the third element and compare it with the elements on the left of it. Placed it just behind the element smaller than it. If there is no element smaller than it, then place it at the beginning of the array.
		
			Place 1 at the beginning Similarly, place every unsorted element at its correct position.
		
			Place 4 behind 1
		

		
			Place 3 behind 1 and the array is sorted  Time Complexities Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2
	Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear.
	Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Worst Case Complexity: O(n2)
		Suppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.
		
		Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
		
		Thus, the total number of comparisons = n*(n-1) ~ n2 Best Case Complexity: O(n)
		When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear. Average Case Complexity: O(n2)
		It occurs when the elements of an array are in jumbled order (neither ascending nor descending). Space Complexity Space complexity is O(1) because an extra variable key is used. The insertion sort is used when: the array is has a small number of elements
	there are only a few elements left to be sorted the array is has a small number of elements there are only a few elements left to be sorted Bubble Sort Quicksort Merge Sort Selection Sort","// Insertion sort in C++

#include <iostream>
using namespace std;

// Function to print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; i++) {
    cout << array[i] << "" "";
  }
  cout << endl;
}

void insertionSort(int array[], int size) {
  for (int step = 1; step < size; step++) {
    int key = array[step];
    int j = step - 1;

    // Compare key with each element on the left of it until an element smaller than
    // it is found.
    // For descending order, change key<array[j] to key>array[j].
    while (key < array[j] && j >= 0) {
      array[j + 1] = array[j];
      --j;
    }
    array[j + 1] = key;
  }
}

// Driver code
int main() {
  int data[] = {9, 5, 1, 4, 3};
  int size = sizeof(data) / sizeof(data[0]);
  insertionSort(data, size);
  cout << ""Sorted array in ascending order:\n"";
  printArray(data, size);
}"
Merge Sort (With Code in Python/C++/Java/C),"Merge Sort is one of the most popular sorting algorithms that is based on the principle of Divide and Conquer Algorithm. Here, a problem is divided into multiple sub-problems. Each sub-problem is solved individually. Finally, sub-problems are combined to form the final solution. Using the Divide and Conquer technique, we divide a problem into subproblems. When the solution to each subproblem is ready, we 'combine' the results from the subproblems to solve the main problem. Suppose we had to sort an array A. A subproblem would be to sort a sub-section of this array starting at index p and ending at index r, denoted as A[p..r]. Divide If q is the half-way point between p and r, then we can split the subarray A[p..r] into two arrays A[p..q] and A[q+1, r]. Conquer In the conquer step, we try to sort both the subarrays A[p..q] and A[q+1, r]. If we haven't yet reached the base case, we again divide both these subarrays and try to sort them. Combine When the conquer step reaches the base step and we get two sorted subarrays A[p..q] and A[q+1, r] for array A[p..r], we combine the results by creating a sorted array A[p..r] from two sorted subarrays A[p..q] and A[q+1, r]. The MergeSort function repeatedly divides the array into two halves until we reach a stage where we try to perform MergeSort on a subarray of size 1 i.e. p == r. After that, the merge function comes into play and combines the sorted arrays into larger arrays until the whole array is merged. To sort an entire array, we need to call MergeSort(A, 0, length(A)-1).  As shown in the image below, the merge sort algorithm recursively divides the array into halves until we reach the base case of array with 1 element. After that, the merge function picks up the sorted sub-arrays and merges them to gradually sort the entire array. Every recursive algorithm is dependent on a base case and the ability to combine the results from base cases. Merge sort is no different. The most important part of the merge sort algorithm is, you guessed it, merge step. The merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array). The algorithm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final sorted array. A noticeable difference between the merging step we described above and the one we use for merge sort is that we only perform the merge function on consecutive sub-arrays. This is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray) and the last index of the second subarray. Our task is to merge two subarrays A[p..q] and A[q+1..r] to create a sorted array A[p..r]. So the inputs to the function are A, p, q and r The merge function works as follows: Create copies of the subarrays L <- A[p..q] and M <- A[q+1..r]. Create three pointers i, j and k
		
			i maintains current index of L, starting at 1
			j maintains current index of M, starting at 1
			k maintains the current index of A[p..q], starting at p. i maintains current index of L, starting at 1 j maintains current index of M, starting at 1 k maintains the current index of A[p..q], starting at p. Until we reach the end of either L or M, pick the larger among the elements from L and M and place them in the correct position at A[p..q] When we run out of elements in either L or M, pick up the remaining elements and put in A[p..q] In code, this would look like: A lot is happening in this function, so let's take an example to see how this would work. As usual, a picture speaks a thousand words. The array A[0..5] contains two sorted subarrays A[0..3] and A[4..5]. Let us see how the merge function will merge the two arrays. This step would have been needed if the size of M was greater than L. At the end of the merge function, the subarray A[p..r] is sorted. Best Case Complexity: O(n*log n) Worst Case Complexity: O(n*log n) Average Case Complexity: O(n*log n) The space complexity of merge sort is O(n). Inversion count problem
	External sorting
	E-commerce applications Inversion count problem External sorting E-commerce applications Quicksort Insertion Sort Selection Sort Bucket Sort","# MergeSort in Python


def mergeSort(array):
    if len(array) > 1:

        #  r is the point where the array is divided into two subarrays
        r = len(array)//2
        L = array[:r]
        M = array[r:]

        # Sort the two halves
        mergeSort(L)
        mergeSort(M)

        i = j = k = 0

        # Until we reach either end of either L or M, pick larger among
        # elements L and M and place them in the correct position at A[p..r]
        while i < len(L) and j < len(M):
            if L[i] < M[j]:
                array[k] = L[i]
                i += 1
            else:
                array[k] = M[j]
                j += 1
            k += 1

        # When we run out of elements in either L or M,
        # pick up the remaining elements and put in A[p..r]
        while i < len(L):
            array[k] = L[i]
            i += 1
            k += 1

        while j < len(M):
            array[k] = M[j]
            j += 1
            k += 1


# Print the array
def printList(array):
    for i in range(len(array)):
        print(array[i], end="" "")
    print()


# Driver program
if __name__ == '__main__':
    array = [6, 5, 12, 10, 9, 1]

    mergeSort(array)

    print(""Sorted array is: "")
    printList(array)"
Merge Sort (With Code in Python/C++/Java/C),"Merge Sort is one of the most popular sorting algorithms that is based on the principle of Divide and Conquer Algorithm. Here, a problem is divided into multiple sub-problems. Each sub-problem is solved individually. Finally, sub-problems are combined to form the final solution. Using the Divide and Conquer technique, we divide a problem into subproblems. When the solution to each subproblem is ready, we 'combine' the results from the subproblems to solve the main problem. Suppose we had to sort an array A. A subproblem would be to sort a sub-section of this array starting at index p and ending at index r, denoted as A[p..r]. Divide If q is the half-way point between p and r, then we can split the subarray A[p..r] into two arrays A[p..q] and A[q+1, r]. Conquer In the conquer step, we try to sort both the subarrays A[p..q] and A[q+1, r]. If we haven't yet reached the base case, we again divide both these subarrays and try to sort them. Combine When the conquer step reaches the base step and we get two sorted subarrays A[p..q] and A[q+1, r] for array A[p..r], we combine the results by creating a sorted array A[p..r] from two sorted subarrays A[p..q] and A[q+1, r]. The MergeSort function repeatedly divides the array into two halves until we reach a stage where we try to perform MergeSort on a subarray of size 1 i.e. p == r. After that, the merge function comes into play and combines the sorted arrays into larger arrays until the whole array is merged. To sort an entire array, we need to call MergeSort(A, 0, length(A)-1).  As shown in the image below, the merge sort algorithm recursively divides the array into halves until we reach the base case of array with 1 element. After that, the merge function picks up the sorted sub-arrays and merges them to gradually sort the entire array. Every recursive algorithm is dependent on a base case and the ability to combine the results from base cases. Merge sort is no different. The most important part of the merge sort algorithm is, you guessed it, merge step. The merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array). The algorithm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final sorted array. A noticeable difference between the merging step we described above and the one we use for merge sort is that we only perform the merge function on consecutive sub-arrays. This is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray) and the last index of the second subarray. Our task is to merge two subarrays A[p..q] and A[q+1..r] to create a sorted array A[p..r]. So the inputs to the function are A, p, q and r The merge function works as follows: Create copies of the subarrays L <- A[p..q] and M <- A[q+1..r]. Create three pointers i, j and k
		
			i maintains current index of L, starting at 1
			j maintains current index of M, starting at 1
			k maintains the current index of A[p..q], starting at p. i maintains current index of L, starting at 1 j maintains current index of M, starting at 1 k maintains the current index of A[p..q], starting at p. Until we reach the end of either L or M, pick the larger among the elements from L and M and place them in the correct position at A[p..q] When we run out of elements in either L or M, pick up the remaining elements and put in A[p..q] In code, this would look like: A lot is happening in this function, so let's take an example to see how this would work. As usual, a picture speaks a thousand words. The array A[0..5] contains two sorted subarrays A[0..3] and A[4..5]. Let us see how the merge function will merge the two arrays. This step would have been needed if the size of M was greater than L. At the end of the merge function, the subarray A[p..r] is sorted. Best Case Complexity: O(n*log n) Worst Case Complexity: O(n*log n) Average Case Complexity: O(n*log n) The space complexity of merge sort is O(n). Inversion count problem
	External sorting
	E-commerce applications Inversion count problem External sorting E-commerce applications Quicksort Insertion Sort Selection Sort Bucket Sort","// Merge sort in Java

class MergeSort {

  // Merge two subarrays L and M into arr
  void merge(int arr[], int p, int q, int r) {

    // Create L ← A[p..q] and M ← A[q+1..r]
    int n1 = q - p + 1;
    int n2 = r - q;

    int L[] = new int[n1];
    int M[] = new int[n2];

    for (int i = 0; i < n1; i++)
      L[i] = arr[p + i];
    for (int j = 0; j < n2; j++)
      M[j] = arr[q + 1 + j];

    // Maintain current index of sub-arrays and main array
    int i, j, k;
    i = 0;
    j = 0;
    k = p;

    // Until we reach either end of either L or M, pick larger among
    // elements L and M and place them in the correct position at A[p..r]
    while (i < n1 && j < n2) {
      if (L[i] <= M[j]) {
        arr[k] = L[i];
        i++;
      } else {
        arr[k] = M[j];
        j++;
      }
      k++;
    }

    // When we run out of elements in either L or M,
    // pick up the remaining elements and put in A[p..r]
    while (i < n1) {
      arr[k] = L[i];
      i++;
      k++;
    }

    while (j < n2) {
      arr[k] = M[j];
      j++;
      k++;
    }
  }

  // Divide the array into two subarrays, sort them and merge them
  void mergeSort(int arr[], int l, int r) {
    if (l < r) {

      // m is the point where the array is divided into two subarrays
      int m = (l + r) / 2;

      mergeSort(arr, l, m);
      mergeSort(arr, m + 1, r);

      // Merge the sorted subarrays
      merge(arr, l, m, r);
    }
  }

  // Print the array
  static void printArray(int arr[]) {
    int n = arr.length;
    for (int i = 0; i < n; ++i)
      System.out.print(arr[i] + "" "");
    System.out.println();
  }

  // Driver program
  public static void main(String args[]) {
    int arr[] = { 6, 5, 12, 10, 9, 1 };

    MergeSort ob = new MergeSort();
    ob.mergeSort(arr, 0, arr.length - 1);

    System.out.println(""Sorted array:"");
    printArray(arr);
  }
}"
Merge Sort (With Code in Python/C++/Java/C),"Merge Sort is one of the most popular sorting algorithms that is based on the principle of Divide and Conquer Algorithm. Here, a problem is divided into multiple sub-problems. Each sub-problem is solved individually. Finally, sub-problems are combined to form the final solution. Using the Divide and Conquer technique, we divide a problem into subproblems. When the solution to each subproblem is ready, we 'combine' the results from the subproblems to solve the main problem. Suppose we had to sort an array A. A subproblem would be to sort a sub-section of this array starting at index p and ending at index r, denoted as A[p..r]. Divide If q is the half-way point between p and r, then we can split the subarray A[p..r] into two arrays A[p..q] and A[q+1, r]. Conquer In the conquer step, we try to sort both the subarrays A[p..q] and A[q+1, r]. If we haven't yet reached the base case, we again divide both these subarrays and try to sort them. Combine When the conquer step reaches the base step and we get two sorted subarrays A[p..q] and A[q+1, r] for array A[p..r], we combine the results by creating a sorted array A[p..r] from two sorted subarrays A[p..q] and A[q+1, r]. The MergeSort function repeatedly divides the array into two halves until we reach a stage where we try to perform MergeSort on a subarray of size 1 i.e. p == r. After that, the merge function comes into play and combines the sorted arrays into larger arrays until the whole array is merged. To sort an entire array, we need to call MergeSort(A, 0, length(A)-1).  As shown in the image below, the merge sort algorithm recursively divides the array into halves until we reach the base case of array with 1 element. After that, the merge function picks up the sorted sub-arrays and merges them to gradually sort the entire array. Every recursive algorithm is dependent on a base case and the ability to combine the results from base cases. Merge sort is no different. The most important part of the merge sort algorithm is, you guessed it, merge step. The merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array). The algorithm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final sorted array. A noticeable difference between the merging step we described above and the one we use for merge sort is that we only perform the merge function on consecutive sub-arrays. This is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray) and the last index of the second subarray. Our task is to merge two subarrays A[p..q] and A[q+1..r] to create a sorted array A[p..r]. So the inputs to the function are A, p, q and r The merge function works as follows: Create copies of the subarrays L <- A[p..q] and M <- A[q+1..r]. Create three pointers i, j and k
		
			i maintains current index of L, starting at 1
			j maintains current index of M, starting at 1
			k maintains the current index of A[p..q], starting at p. i maintains current index of L, starting at 1 j maintains current index of M, starting at 1 k maintains the current index of A[p..q], starting at p. Until we reach the end of either L or M, pick the larger among the elements from L and M and place them in the correct position at A[p..q] When we run out of elements in either L or M, pick up the remaining elements and put in A[p..q] In code, this would look like: A lot is happening in this function, so let's take an example to see how this would work. As usual, a picture speaks a thousand words. The array A[0..5] contains two sorted subarrays A[0..3] and A[4..5]. Let us see how the merge function will merge the two arrays. This step would have been needed if the size of M was greater than L. At the end of the merge function, the subarray A[p..r] is sorted. Best Case Complexity: O(n*log n) Worst Case Complexity: O(n*log n) Average Case Complexity: O(n*log n) The space complexity of merge sort is O(n). Inversion count problem
	External sorting
	E-commerce applications Inversion count problem External sorting E-commerce applications Quicksort Insertion Sort Selection Sort Bucket Sort","// Merge sort in C

#include <stdio.h>

// Merge two subarrays L and M into arr
void merge(int arr[], int p, int q, int r) {

  // Create L ← A[p..q] and M ← A[q+1..r]
  int n1 = q - p + 1;
  int n2 = r - q;

  int L[n1], M[n2];

  for (int i = 0; i < n1; i++)
    L[i] = arr[p + i];
  for (int j = 0; j < n2; j++)
    M[j] = arr[q + 1 + j];

  // Maintain current index of sub-arrays and main array
  int i, j, k;
  i = 0;
  j = 0;
  k = p;

  // Until we reach either end of either L or M, pick larger among
  // elements L and M and place them in the correct position at A[p..r]
  while (i < n1 && j < n2) {
    if (L[i] <= M[j]) {
      arr[k] = L[i];
      i++;
    } else {
      arr[k] = M[j];
      j++;
    }
    k++;
  }

  // When we run out of elements in either L or M,
  // pick up the remaining elements and put in A[p..r]
  while (i < n1) {
    arr[k] = L[i];
    i++;
    k++;
  }

  while (j < n2) {
    arr[k] = M[j];
    j++;
    k++;
  }
}

// Divide the array into two subarrays, sort them and merge them
void mergeSort(int arr[], int l, int r) {
  if (l < r) {

    // m is the point where the array is divided into two subarrays
    int m = l + (r - l) / 2;

    mergeSort(arr, l, m);
    mergeSort(arr, m + 1, r);

    // Merge the sorted subarrays
    merge(arr, l, m, r);
  }
}

// Print the array
void printArray(int arr[], int size) {
  for (int i = 0; i < size; i++)
    printf(""%d "", arr[i]);
  printf(""\n"");
}

// Driver program
int main() {
  int arr[] = {6, 5, 12, 10, 9, 1};
  int size = sizeof(arr) / sizeof(arr[0]);

  mergeSort(arr, 0, size - 1);

  printf(""Sorted array: \n"");
  printArray(arr, size);
}"
Merge Sort (With Code in Python/C++/Java/C),"Merge Sort is one of the most popular sorting algorithms that is based on the principle of Divide and Conquer Algorithm. Here, a problem is divided into multiple sub-problems. Each sub-problem is solved individually. Finally, sub-problems are combined to form the final solution. Using the Divide and Conquer technique, we divide a problem into subproblems. When the solution to each subproblem is ready, we 'combine' the results from the subproblems to solve the main problem. Suppose we had to sort an array A. A subproblem would be to sort a sub-section of this array starting at index p and ending at index r, denoted as A[p..r]. Divide If q is the half-way point between p and r, then we can split the subarray A[p..r] into two arrays A[p..q] and A[q+1, r]. Conquer In the conquer step, we try to sort both the subarrays A[p..q] and A[q+1, r]. If we haven't yet reached the base case, we again divide both these subarrays and try to sort them. Combine When the conquer step reaches the base step and we get two sorted subarrays A[p..q] and A[q+1, r] for array A[p..r], we combine the results by creating a sorted array A[p..r] from two sorted subarrays A[p..q] and A[q+1, r]. The MergeSort function repeatedly divides the array into two halves until we reach a stage where we try to perform MergeSort on a subarray of size 1 i.e. p == r. After that, the merge function comes into play and combines the sorted arrays into larger arrays until the whole array is merged. To sort an entire array, we need to call MergeSort(A, 0, length(A)-1).  As shown in the image below, the merge sort algorithm recursively divides the array into halves until we reach the base case of array with 1 element. After that, the merge function picks up the sorted sub-arrays and merges them to gradually sort the entire array. Every recursive algorithm is dependent on a base case and the ability to combine the results from base cases. Merge sort is no different. The most important part of the merge sort algorithm is, you guessed it, merge step. The merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array). The algorithm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final sorted array. A noticeable difference between the merging step we described above and the one we use for merge sort is that we only perform the merge function on consecutive sub-arrays. This is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray) and the last index of the second subarray. Our task is to merge two subarrays A[p..q] and A[q+1..r] to create a sorted array A[p..r]. So the inputs to the function are A, p, q and r The merge function works as follows: Create copies of the subarrays L <- A[p..q] and M <- A[q+1..r]. Create three pointers i, j and k
		
			i maintains current index of L, starting at 1
			j maintains current index of M, starting at 1
			k maintains the current index of A[p..q], starting at p. i maintains current index of L, starting at 1 j maintains current index of M, starting at 1 k maintains the current index of A[p..q], starting at p. Until we reach the end of either L or M, pick the larger among the elements from L and M and place them in the correct position at A[p..q] When we run out of elements in either L or M, pick up the remaining elements and put in A[p..q] In code, this would look like: A lot is happening in this function, so let's take an example to see how this would work. As usual, a picture speaks a thousand words. The array A[0..5] contains two sorted subarrays A[0..3] and A[4..5]. Let us see how the merge function will merge the two arrays. This step would have been needed if the size of M was greater than L. At the end of the merge function, the subarray A[p..r] is sorted. Best Case Complexity: O(n*log n) Worst Case Complexity: O(n*log n) Average Case Complexity: O(n*log n) The space complexity of merge sort is O(n). Inversion count problem
	External sorting
	E-commerce applications Inversion count problem External sorting E-commerce applications Quicksort Insertion Sort Selection Sort Bucket Sort","// Merge sort in C++

#include <iostream>
using namespace std;

// Merge two subarrays L and M into arr
void merge(int arr[], int p, int q, int r) {
  
  // Create L ← A[p..q] and M ← A[q+1..r]
  int n1 = q - p + 1;
  int n2 = r - q;

  int L[n1], M[n2];

  for (int i = 0; i < n1; i++)
    L[i] = arr[p + i];
  for (int j = 0; j < n2; j++)
    M[j] = arr[q + 1 + j];

  // Maintain current index of sub-arrays and main array
  int i, j, k;
  i = 0;
  j = 0;
  k = p;

  // Until we reach either end of either L or M, pick larger among
  // elements L and M and place them in the correct position at A[p..r]
  while (i < n1 && j < n2) {
    if (L[i] <= M[j]) {
      arr[k] = L[i];
      i++;
    } else {
      arr[k] = M[j];
      j++;
    }
    k++;
  }

  // When we run out of elements in either L or M,
  // pick up the remaining elements and put in A[p..r]
  while (i < n1) {
    arr[k] = L[i];
    i++;
    k++;
  }

  while (j < n2) {
    arr[k] = M[j];
    j++;
    k++;
  }
}

// Divide the array into two subarrays, sort them and merge them
void mergeSort(int arr[], int l, int r) {
  if (l < r) {
    // m is the point where the array is divided into two subarrays
    int m = l + (r - l) / 2;

    mergeSort(arr, l, m);
    mergeSort(arr, m + 1, r);

    // Merge the sorted subarrays
    merge(arr, l, m, r);
  }
}

// Print the array
void printArray(int arr[], int size) {
  for (int i = 0; i < size; i++)
    cout << arr[i] << "" "";
  cout << endl;
}

// Driver program
int main() {
  int arr[] = {6, 5, 12, 10, 9, 1};
  int size = sizeof(arr) / sizeof(arr[0]);

  mergeSort(arr, 0, size - 1);

  cout << ""Sorted array: \n"";
  printArray(arr, size);
  return 0;
}"
QuickSort (With Code in Python/C++/Java/C),"Quicksort is a sorting algorithm based on the divide and conquer approach where An array is divided into subarrays by selecting a pivot element (element selected from the array).
		
		While dividing the array, the pivot element should be positioned in such a way that elements less than pivot are kept on the left side and elements greater than pivot are on the right side of the pivot. The left and right subarrays are also divided using the same approach. This process continues until each subarray contains a single element. At this point, elements are already sorted. Finally, elements are combined to form a sorted array. 1. Select the Pivot Element There are different variations of quicksort where the pivot element is selected from different positions. Here, we will be selecting the rightmost element of the array as the pivot element. 2. Rearrange the Array Now the elements of the array are rearranged so that elements that are smaller than the pivot are put on the left and the elements greater than the pivot are put on the right. Here's how we rearrange the array: A pointer is fixed at the pivot element. The pivot element is compared with the elements beginning from the first index.
		
			Comparison of pivot element with element beginning from the first index If the element is greater than the pivot element, a second pointer is set for that element.
		
			If the element is greater than the pivot element, a second pointer is set for that element. Now, pivot is compared with other elements. If an element smaller than the pivot element is reached, the smaller element is swapped with the greater element found earlier.
		
			Pivot is compared with other elements. Again, the process is repeated to set the next greater element as the second pointer. And, swap it with another smaller element.
		
			The process is repeated to set the next greater element as the second pointer. The process goes on until the second last element is reached.
		
			The process goes on until the second last element is reached. Finally, the pivot element is swapped with the second pointer.
		
			Finally, the pivot element is swapped with the second pointer. 3. Divide Subarrays Pivot elements are again chosen for the left and the right sub-parts separately. And, step 2 is repeated.   The subarrays are divided until each subarray is formed of a single element. At this point, the array is already sorted. You can understand the working of quicksort algorithm with the help of the illustrations below. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element.
	Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element. Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. The space complexity for quicksort is O(log n). Quicksort algorithm is used when the programming language is good for recursion
	time complexity matters
	space complexity matters the programming language is good for recursion time complexity matters space complexity matters Insertion Sort
	Merge Sort
	Selection Sort
	Bucket Sort Insertion Sort Merge Sort Selection Sort Bucket Sort","# Quick sort in Python

# function to find the partition position
def partition(array, low, high):

  # choose the rightmost element as pivot
  pivot = array[high]

  # pointer for greater element
  i = low - 1

  # traverse through all elements
  # compare each element with pivot
  for j in range(low, high):
    if array[j] <= pivot:
      # if element smaller than pivot is found
      # swap it with the greater element pointed by i
      i = i + 1

      # swapping element at i with element at j
      (array[i], array[j]) = (array[j], array[i])

  # swap the pivot element with the greater element specified by i
  (array[i + 1], array[high]) = (array[high], array[i + 1])

  # return the position from where partition is done
  return i + 1

# function to perform quicksort
def quickSort(array, low, high):
  if low < high:

    # find pivot element such that
    # element smaller than pivot are on the left
    # element greater than pivot are on the right
    pi = partition(array, low, high)

    # recursive call on the left of pivot
    quickSort(array, low, pi - 1)

    # recursive call on the right of pivot
    quickSort(array, pi + 1, high)


data = [8, 7, 2, 1, 0, 9, 6]
print(""Unsorted Array"")
print(data)

size = len(data)

quickSort(data, 0, size - 1)

print('Sorted Array in Ascending Order:')
print(data)"
QuickSort (With Code in Python/C++/Java/C),"Quicksort is a sorting algorithm based on the divide and conquer approach where An array is divided into subarrays by selecting a pivot element (element selected from the array).
		
		While dividing the array, the pivot element should be positioned in such a way that elements less than pivot are kept on the left side and elements greater than pivot are on the right side of the pivot. The left and right subarrays are also divided using the same approach. This process continues until each subarray contains a single element. At this point, elements are already sorted. Finally, elements are combined to form a sorted array. 1. Select the Pivot Element There are different variations of quicksort where the pivot element is selected from different positions. Here, we will be selecting the rightmost element of the array as the pivot element. 2. Rearrange the Array Now the elements of the array are rearranged so that elements that are smaller than the pivot are put on the left and the elements greater than the pivot are put on the right. Here's how we rearrange the array: A pointer is fixed at the pivot element. The pivot element is compared with the elements beginning from the first index.
		
			Comparison of pivot element with element beginning from the first index If the element is greater than the pivot element, a second pointer is set for that element.
		
			If the element is greater than the pivot element, a second pointer is set for that element. Now, pivot is compared with other elements. If an element smaller than the pivot element is reached, the smaller element is swapped with the greater element found earlier.
		
			Pivot is compared with other elements. Again, the process is repeated to set the next greater element as the second pointer. And, swap it with another smaller element.
		
			The process is repeated to set the next greater element as the second pointer. The process goes on until the second last element is reached.
		
			The process goes on until the second last element is reached. Finally, the pivot element is swapped with the second pointer.
		
			Finally, the pivot element is swapped with the second pointer. 3. Divide Subarrays Pivot elements are again chosen for the left and the right sub-parts separately. And, step 2 is repeated.   The subarrays are divided until each subarray is formed of a single element. At this point, the array is already sorted. You can understand the working of quicksort algorithm with the help of the illustrations below. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element.
	Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element. Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. The space complexity for quicksort is O(log n). Quicksort algorithm is used when the programming language is good for recursion
	time complexity matters
	space complexity matters the programming language is good for recursion time complexity matters space complexity matters Insertion Sort
	Merge Sort
	Selection Sort
	Bucket Sort Insertion Sort Merge Sort Selection Sort Bucket Sort","// Quick sort in Java

import java.util.Arrays;

class Quicksort {

  // method to find the partition position
  static int partition(int array[], int low, int high) {
    
    // choose the rightmost element as pivot
    int pivot = array[high];
    
    // pointer for greater element
    int i = (low - 1);

    // traverse through all elements
    // compare each element with pivot
    for (int j = low; j < high; j++) {
      if (array[j] <= pivot) {

        // if element smaller than pivot is found
        // swap it with the greatr element pointed by i
        i++;

        // swapping element at i with element at j
        int temp = array[i];
        array[i] = array[j];
        array[j] = temp;
      }

    }

    // swapt the pivot element with the greater element specified by i
    int temp = array[i + 1];
    array[i + 1] = array[high];
    array[high] = temp;

    // return the position from where partition is done
    return (i + 1);
  }

  static void quickSort(int array[], int low, int high) {
    if (low < high) {

      // find pivot element such that
      // elements smaller than pivot are on the left
      // elements greater than pivot are on the right
      int pi = partition(array, low, high);
      
      // recursive call on the left of pivot
      quickSort(array, low, pi - 1);

      // recursive call on the right of pivot
      quickSort(array, pi + 1, high);
    }
  }
}

// Main class
class Main {
  public static void main(String args[]) {

    int[] data = { 8, 7, 2, 1, 0, 9, 6 };
    System.out.println(""Unsorted Array"");
    System.out.println(Arrays.toString(data));

    int size = data.length;

    // call quicksort() on array data
    Quicksort.quickSort(data, 0, size - 1);

    System.out.println(""Sorted Array in Ascending Order: "");
    System.out.println(Arrays.toString(data));
  }
}"
QuickSort (With Code in Python/C++/Java/C),"Quicksort is a sorting algorithm based on the divide and conquer approach where An array is divided into subarrays by selecting a pivot element (element selected from the array).
		
		While dividing the array, the pivot element should be positioned in such a way that elements less than pivot are kept on the left side and elements greater than pivot are on the right side of the pivot. The left and right subarrays are also divided using the same approach. This process continues until each subarray contains a single element. At this point, elements are already sorted. Finally, elements are combined to form a sorted array. 1. Select the Pivot Element There are different variations of quicksort where the pivot element is selected from different positions. Here, we will be selecting the rightmost element of the array as the pivot element. 2. Rearrange the Array Now the elements of the array are rearranged so that elements that are smaller than the pivot are put on the left and the elements greater than the pivot are put on the right. Here's how we rearrange the array: A pointer is fixed at the pivot element. The pivot element is compared with the elements beginning from the first index.
		
			Comparison of pivot element with element beginning from the first index If the element is greater than the pivot element, a second pointer is set for that element.
		
			If the element is greater than the pivot element, a second pointer is set for that element. Now, pivot is compared with other elements. If an element smaller than the pivot element is reached, the smaller element is swapped with the greater element found earlier.
		
			Pivot is compared with other elements. Again, the process is repeated to set the next greater element as the second pointer. And, swap it with another smaller element.
		
			The process is repeated to set the next greater element as the second pointer. The process goes on until the second last element is reached.
		
			The process goes on until the second last element is reached. Finally, the pivot element is swapped with the second pointer.
		
			Finally, the pivot element is swapped with the second pointer. 3. Divide Subarrays Pivot elements are again chosen for the left and the right sub-parts separately. And, step 2 is repeated.   The subarrays are divided until each subarray is formed of a single element. At this point, the array is already sorted. You can understand the working of quicksort algorithm with the help of the illustrations below. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element.
	Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element. Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. The space complexity for quicksort is O(log n). Quicksort algorithm is used when the programming language is good for recursion
	time complexity matters
	space complexity matters the programming language is good for recursion time complexity matters space complexity matters Insertion Sort
	Merge Sort
	Selection Sort
	Bucket Sort Insertion Sort Merge Sort Selection Sort Bucket Sort","// Quick sort in C

#include <stdio.h>

// function to swap elements
void swap(int *a, int *b) {
  int t = *a;
  *a = *b;
  *b = t;
}

// function to find the partition position
int partition(int array[], int low, int high) {
  
  // select the rightmost element as pivot
  int pivot = array[high];
  
  // pointer for greater element
  int i = (low - 1);

  // traverse each element of the array
  // compare them with the pivot
  for (int j = low; j < high; j++) {
    if (array[j] <= pivot) {
        
      // if element smaller than pivot is found
      // swap it with the greater element pointed by i
      i++;
      
      // swap element at i with element at j
      swap(&array[i], &array[j]);
    }
  }

  // swap the pivot element with the greater element at i
  swap(&array[i + 1], &array[high]);
  
  // return the partition point
  return (i + 1);
}

void quickSort(int array[], int low, int high) {
  if (low < high) {
    
    // find the pivot element such that
    // elements smaller than pivot are on left of pivot
    // elements greater than pivot are on right of pivot
    int pi = partition(array, low, high);
    
    // recursive call on the left of pivot
    quickSort(array, low, pi - 1);
    
    // recursive call on the right of pivot
    quickSort(array, pi + 1, high);
  }
}

// function to print array elements
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i) {
    printf(""%d  "", array[i]);
  }
  printf(""\n"");
}

// main function
int main() {
  int data[] = {8, 7, 2, 1, 0, 9, 6};
  
  int n = sizeof(data) / sizeof(data[0]);
  
  printf(""Unsorted Array\n"");
  printArray(data, n);
  
  // perform quicksort on data
  quickSort(data, 0, n - 1);
  
  printf(""Sorted array in ascending order: \n"");
  printArray(data, n);
}"
QuickSort (With Code in Python/C++/Java/C),"Quicksort is a sorting algorithm based on the divide and conquer approach where An array is divided into subarrays by selecting a pivot element (element selected from the array).
		
		While dividing the array, the pivot element should be positioned in such a way that elements less than pivot are kept on the left side and elements greater than pivot are on the right side of the pivot. The left and right subarrays are also divided using the same approach. This process continues until each subarray contains a single element. At this point, elements are already sorted. Finally, elements are combined to form a sorted array. 1. Select the Pivot Element There are different variations of quicksort where the pivot element is selected from different positions. Here, we will be selecting the rightmost element of the array as the pivot element. 2. Rearrange the Array Now the elements of the array are rearranged so that elements that are smaller than the pivot are put on the left and the elements greater than the pivot are put on the right. Here's how we rearrange the array: A pointer is fixed at the pivot element. The pivot element is compared with the elements beginning from the first index.
		
			Comparison of pivot element with element beginning from the first index If the element is greater than the pivot element, a second pointer is set for that element.
		
			If the element is greater than the pivot element, a second pointer is set for that element. Now, pivot is compared with other elements. If an element smaller than the pivot element is reached, the smaller element is swapped with the greater element found earlier.
		
			Pivot is compared with other elements. Again, the process is repeated to set the next greater element as the second pointer. And, swap it with another smaller element.
		
			The process is repeated to set the next greater element as the second pointer. The process goes on until the second last element is reached.
		
			The process goes on until the second last element is reached. Finally, the pivot element is swapped with the second pointer.
		
			Finally, the pivot element is swapped with the second pointer. 3. Divide Subarrays Pivot elements are again chosen for the left and the right sub-parts separately. And, step 2 is repeated.   The subarrays are divided until each subarray is formed of a single element. At this point, the array is already sorted. You can understand the working of quicksort algorithm with the help of the illustrations below. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Worst Case Complexity [Big-O]: O(n2)
		It occurs when the pivot element picked is either the greatest or the smallest element.
		
		This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains n - 1 elements. Thus, quicksort is called only on this sub-array.
		
		However, the quicksort algorithm has better performance for scattered pivots. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element.
	Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. Best Case Complexity [Big-omega]: O(n*log n)
		It occurs when the pivot element is always the middle element or near to the middle element. Average Case Complexity [Big-theta]: O(n*log n)
		It occurs when the above conditions do not occur. The space complexity for quicksort is O(log n). Quicksort algorithm is used when the programming language is good for recursion
	time complexity matters
	space complexity matters the programming language is good for recursion time complexity matters space complexity matters Insertion Sort
	Merge Sort
	Selection Sort
	Bucket Sort Insertion Sort Merge Sort Selection Sort Bucket Sort","// Quick sort in C++

#include <iostream>
using namespace std;

// function to swap elements
void swap(int *a, int *b) {
  int t = *a;
  *a = *b;
  *b = t;
}

// function to print the array
void printArray(int array[], int size) {
  int i;
  for (i = 0; i < size; i++)
    cout << array[i] << "" "";
  cout << endl;
}

// function to rearrange array (find the partition point)
int partition(int array[], int low, int high) {
    
  // select the rightmost element as pivot
  int pivot = array[high];
  
  // pointer for greater element
  int i = (low - 1);

  // traverse each element of the array
  // compare them with the pivot
  for (int j = low; j < high; j++) {
    if (array[j] <= pivot) {
        
      // if element smaller than pivot is found
      // swap it with the greater element pointed by i
      i++;
      
      // swap element at i with element at j
      swap(&array[i], &array[j]);
    }
  }
  
  // swap pivot with the greater element at i
  swap(&array[i + 1], &array[high]);
  
  // return the partition point
  return (i + 1);
}

void quickSort(int array[], int low, int high) {
  if (low < high) {
      
    // find the pivot element such that
    // elements smaller than pivot are on left of pivot
    // elements greater than pivot are on righ of pivot
    int pi = partition(array, low, high);

    // recursive call on the left of pivot
    quickSort(array, low, pi - 1);

    // recursive call on the right of pivot
    quickSort(array, pi + 1, high);
  }
}

// Driver code
int main() {
  int data[] = {8, 7, 6, 1, 0, 9, 2};
  int n = sizeof(data) / sizeof(data[0]);
  
  cout << ""Unsorted Array: \n"";
  printArray(data, n);
  
  // perform quicksort on data
  quickSort(data, 0, n - 1);
  
  cout << ""Sorted array in ascending order: \n"";
  printArray(data, n);
}"
Counting Sort (With Code in Python/C++/Java/C),"Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each unique element in the array. The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array. Find out the maximum element (let it be max) from the given array.

		
			Given array Initialize an array of length max+1 with all elements 0. This array is used for storing the count of the elements in the array.
		
			Count array Store the count of each element at their respective index in count array
		
		For example: if the count of element 3 is 2 then, 2 is stored in the 3rd position of count array. If element ""5"" is not present in the array, then 0 is stored in 5th position.
		
			Count of each element stored Store cumulative sum of the elements of the count array. It helps in placing the elements into the correct index of the sorted array.
		
			Cumulative count Find the index of each element of the original array in the count array. This gives the cumulative count. Place the element at the index calculated as shown in figure below.
		
			Counting sort After placing each element at its correct position, decrease its count by one. Time Complexities There are mainly four main loops. (Finding the greatest value can be done outside the function.) Overall complexity = O(max)+O(size)+O(max)+O(size) = O(max+size) Worst Case Complexity: O(n+k)
	Best Case Complexity: O(n+k)
	Average Case Complexity: O(n+k) Worst Case Complexity: O(n+k) Best Case Complexity: O(n+k) Average Case Complexity: O(n+k) In all the above cases, the complexity is the same because no matter how the elements are placed in the array, the algorithm goes through n+k times.  There is no comparison between any elements, so it is better than comparison based sorting techniques. But, it is bad if the integers are very large because the array of that size should be made. Space Complexity The space complexity of Counting Sort is O(max). Larger the range of elements, larger is the space complexity. Counting sort is used when: there are smaller integers with multiple counts.
	linear complexity is the need. there are smaller integers with multiple counts. linear complexity is the need. Quicksort Merge Sort Bucket Sort Radix Sort","# Counting sort in Python programming


def countingSort(array):
    size = len(array)
    output = [0] * size

    # Initialize count array
    count = [0] * 10

    # Store the count of each elements in count array
    for i in range(0, size):
        count[array[i]] += 1

    # Store the cummulative count
    for i in range(1, 10):
        count[i] += count[i - 1]

    # Find the index of each element of the original array in count array
    # place the elements in output array
    i = size - 1
    while i >= 0:
        output[count[array[i]] - 1] = array[i]
        count[array[i]] -= 1
        i -= 1

    # Copy the sorted elements into original array
    for i in range(0, size):
        array[i] = output[i]


data = [4, 2, 2, 8, 3, 3, 1]
countingSort(data)
print(""Sorted Array in Ascending Order: "")
print(data)"
Counting Sort (With Code in Python/C++/Java/C),"Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each unique element in the array. The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array. Find out the maximum element (let it be max) from the given array.

		
			Given array Initialize an array of length max+1 with all elements 0. This array is used for storing the count of the elements in the array.
		
			Count array Store the count of each element at their respective index in count array
		
		For example: if the count of element 3 is 2 then, 2 is stored in the 3rd position of count array. If element ""5"" is not present in the array, then 0 is stored in 5th position.
		
			Count of each element stored Store cumulative sum of the elements of the count array. It helps in placing the elements into the correct index of the sorted array.
		
			Cumulative count Find the index of each element of the original array in the count array. This gives the cumulative count. Place the element at the index calculated as shown in figure below.
		
			Counting sort After placing each element at its correct position, decrease its count by one. Time Complexities There are mainly four main loops. (Finding the greatest value can be done outside the function.) Overall complexity = O(max)+O(size)+O(max)+O(size) = O(max+size) Worst Case Complexity: O(n+k)
	Best Case Complexity: O(n+k)
	Average Case Complexity: O(n+k) Worst Case Complexity: O(n+k) Best Case Complexity: O(n+k) Average Case Complexity: O(n+k) In all the above cases, the complexity is the same because no matter how the elements are placed in the array, the algorithm goes through n+k times.  There is no comparison between any elements, so it is better than comparison based sorting techniques. But, it is bad if the integers are very large because the array of that size should be made. Space Complexity The space complexity of Counting Sort is O(max). Larger the range of elements, larger is the space complexity. Counting sort is used when: there are smaller integers with multiple counts.
	linear complexity is the need. there are smaller integers with multiple counts. linear complexity is the need. Quicksort Merge Sort Bucket Sort Radix Sort","// Counting sort in Java programming

import java.util.Arrays;

class CountingSort {
  void countSort(int array[], int size) {
    int[] output = new int[size + 1];

    // Find the largest element of the array
    int max = array[0];
    for (int i = 1; i < size; i++) {
      if (array[i] > max)
        max = array[i];
    }
    int[] count = new int[max + 1];

    // Initialize count array with all zeros.
    for (int i = 0; i < max; ++i) {
      count[i] = 0;
    }

    // Store the count of each element
    for (int i = 0; i < size; i++) {
      count[array[i]]++;
    }

    // Store the cummulative count of each array
    for (int i = 1; i <= max; i++) {
      count[i] += count[i - 1];
    }

    // Find the index of each element of the original array in count array, and
    // place the elements in output array
    for (int i = size - 1; i >= 0; i--) {
      output[count[array[i]] - 1] = array[i];
      count[array[i]]--;
    }

    // Copy the sorted elements into original array
    for (int i = 0; i < size; i++) {
      array[i] = output[i];
    }
  }

  // Driver code
  public static void main(String args[]) {
    int[] data = { 4, 2, 2, 8, 3, 3, 1 };
    int size = data.length;
    CountingSort cs = new CountingSort();
    cs.countSort(data, size);
    System.out.println(""Sorted Array in Ascending Order: "");
    System.out.println(Arrays.toString(data));
  }
}"
Counting Sort (With Code in Python/C++/Java/C),"Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each unique element in the array. The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array. Find out the maximum element (let it be max) from the given array.

		
			Given array Initialize an array of length max+1 with all elements 0. This array is used for storing the count of the elements in the array.
		
			Count array Store the count of each element at their respective index in count array
		
		For example: if the count of element 3 is 2 then, 2 is stored in the 3rd position of count array. If element ""5"" is not present in the array, then 0 is stored in 5th position.
		
			Count of each element stored Store cumulative sum of the elements of the count array. It helps in placing the elements into the correct index of the sorted array.
		
			Cumulative count Find the index of each element of the original array in the count array. This gives the cumulative count. Place the element at the index calculated as shown in figure below.
		
			Counting sort After placing each element at its correct position, decrease its count by one. Time Complexities There are mainly four main loops. (Finding the greatest value can be done outside the function.) Overall complexity = O(max)+O(size)+O(max)+O(size) = O(max+size) Worst Case Complexity: O(n+k)
	Best Case Complexity: O(n+k)
	Average Case Complexity: O(n+k) Worst Case Complexity: O(n+k) Best Case Complexity: O(n+k) Average Case Complexity: O(n+k) In all the above cases, the complexity is the same because no matter how the elements are placed in the array, the algorithm goes through n+k times.  There is no comparison between any elements, so it is better than comparison based sorting techniques. But, it is bad if the integers are very large because the array of that size should be made. Space Complexity The space complexity of Counting Sort is O(max). Larger the range of elements, larger is the space complexity. Counting sort is used when: there are smaller integers with multiple counts.
	linear complexity is the need. there are smaller integers with multiple counts. linear complexity is the need. Quicksort Merge Sort Bucket Sort Radix Sort","// Counting sort in C programming

#include <stdio.h>

void countingSort(int array[], int size) {
  int output[10];

  // Find the largest element of the array
  int max = array[0];
  for (int i = 1; i < size; i++) {
    if (array[i] > max)
      max = array[i];
  }

  // The size of count must be at least (max+1) but
  // we cannot declare it as int count(max+1) in C as
  // it does not support dynamic memory allocation.
  // So, its size is provided statically.
  int count[10];

  // Initialize count array with all zeros.
  for (int i = 0; i <= max; ++i) {
    count[i] = 0;
  }

  // Store the count of each element
  for (int i = 0; i < size; i++) {
    count[array[i]]++;
  }

  // Store the cummulative count of each array
  for (int i = 1; i <= max; i++) {
    count[i] += count[i - 1];
  }

  // Find the index of each element of the original array in count array, and
  // place the elements in output array
  for (int i = size - 1; i >= 0; i--) {
    output[count[array[i]] - 1] = array[i];
    count[array[i]]--;
  }

  // Copy the sorted elements into original array
  for (int i = 0; i < size; i++) {
    array[i] = output[i];
  }
}

// Function to print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i) {
    printf(""%d  "", array[i]);
  }
  printf(""\n"");
}

// Driver code
int main() {
  int array[] = {4, 2, 2, 8, 3, 3, 1};
  int n = sizeof(array) / sizeof(array[0]);
  countingSort(array, n);
  printArray(array, n);
}"
Counting Sort (With Code in Python/C++/Java/C),"Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each unique element in the array. The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array. Find out the maximum element (let it be max) from the given array.

		
			Given array Initialize an array of length max+1 with all elements 0. This array is used for storing the count of the elements in the array.
		
			Count array Store the count of each element at their respective index in count array
		
		For example: if the count of element 3 is 2 then, 2 is stored in the 3rd position of count array. If element ""5"" is not present in the array, then 0 is stored in 5th position.
		
			Count of each element stored Store cumulative sum of the elements of the count array. It helps in placing the elements into the correct index of the sorted array.
		
			Cumulative count Find the index of each element of the original array in the count array. This gives the cumulative count. Place the element at the index calculated as shown in figure below.
		
			Counting sort After placing each element at its correct position, decrease its count by one. Time Complexities There are mainly four main loops. (Finding the greatest value can be done outside the function.) Overall complexity = O(max)+O(size)+O(max)+O(size) = O(max+size) Worst Case Complexity: O(n+k)
	Best Case Complexity: O(n+k)
	Average Case Complexity: O(n+k) Worst Case Complexity: O(n+k) Best Case Complexity: O(n+k) Average Case Complexity: O(n+k) In all the above cases, the complexity is the same because no matter how the elements are placed in the array, the algorithm goes through n+k times.  There is no comparison between any elements, so it is better than comparison based sorting techniques. But, it is bad if the integers are very large because the array of that size should be made. Space Complexity The space complexity of Counting Sort is O(max). Larger the range of elements, larger is the space complexity. Counting sort is used when: there are smaller integers with multiple counts.
	linear complexity is the need. there are smaller integers with multiple counts. linear complexity is the need. Quicksort Merge Sort Bucket Sort Radix Sort","// Counting sort in C++ programming

#include <iostream>
using namespace std;

void countSort(int array[], int size) {
  // The size of count must be at least the (max+1) but
  // we cannot assign declare it as int count(max+1) in C++ as
  // it does not support dynamic memory allocation.
  // So, its size is provided statically.
  int output[10];
  int count[10];
  int max = array[0];

  // Find the largest element of the array
  for (int i = 1; i < size; i++) {
    if (array[i] > max)
      max = array[i];
  }

  // Initialize count array with all zeros.
  for (int i = 0; i <= max; ++i) {
    count[i] = 0;
  }

  // Store the count of each element
  for (int i = 0; i < size; i++) {
    count[array[i]]++;
  }

  // Store the cummulative count of each array
  for (int i = 1; i <= max; i++) {
    count[i] += count[i - 1];
  }

  // Find the index of each element of the original array in count array, and
  // place the elements in output array
  for (int i = size - 1; i >= 0; i--) {
    output[count[array[i]] - 1] = array[i];
    count[array[i]]--;
  }

  // Copy the sorted elements into original array
  for (int i = 0; i < size; i++) {
    array[i] = output[i];
  }
}

// Function to print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; i++)
    cout << array[i] << "" "";
  cout << endl;
}

// Driver code
int main() {
  int array[] = {4, 2, 2, 8, 3, 3, 1};
  int n = sizeof(array) / sizeof(array[0]);
  countSort(array, n);
  printArray(array, n);
}"
"Radix Sort (With Code in Python, C++, Java and C)","Radix sort is a sorting algorithm that sorts the elements by first grouping the individual digits of the same place value. Then, sort the elements according to their increasing/decreasing order. Suppose, we have an array of 8 elements. First, we will sort elements based on the value of the unit place. Then, we will sort elements based on the value of the tenth place. This process goes on until the last significant place. Let the initial array be [121, 432, 564, 23, 1, 45, 788]. It is sorted according to radix sort as shown in the figure below. Please go through the counting sort before reading this article because counting sort is used as an intermediate sort in radix sort. Find the largest element in the array, i.e. max. Let X be the number of digits in max. X is calculated because we have to go through all the significant places of all elements.
		
		In this array [121, 432, 564, 23, 1, 45, 788], we have the largest number 788. It has 3 digits. Therefore, the loop should go up to hundreds place (3 times). Now, go through each significant place one by one.
		
		Use any stable sorting technique to sort the digits at each significant place. We have used counting sort for this.
		
		Sort the elements based on the unit place digits (X=0).
		
			Using counting sort to sort elements based on unit place Now, sort the elements based on digits at tens place.
		
			Sort elements based on tens place Finally, sort the elements based on the digits at hundreds place.
		
			Sort elements based on hundreds place Since radix sort is a non-comparative algorithm, it has advantages over comparative sorting algorithms. For the radix sort that uses counting sort as an intermediate stable sort, the time complexity is O(d(n+k)).  Here, d is the number cycle and O(n+k) is the time complexity of counting sort. Thus, radix sort has linear time complexity which is better than O(nlog n) of comparative sorting algorithms. If we take very large digit numbers or the number of other bases like 32-bit and 64-bit numbers then it can perform in linear time however the intermediate sort takes large space. This makes radix sort space inefficient. This is the reason why this sort is not used in software libraries. Radix sort is implemented in DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array.
	places where there are numbers in large ranges. DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array. places where there are numbers in large ranges. Quicksort Merge Sort Bucket Sort Counting Sort","# Radix sort in Python


# Using counting sort to sort the elements in the basis of significant places
def countingSort(array, place):
    size = len(array)
    output = [0] * size
    count = [0] * 10

    # Calculate count of elements
    for i in range(0, size):
        index = array[i] // place
        count[index % 10] += 1

    # Calculate cumulative count
    for i in range(1, 10):
        count[i] += count[i - 1]

    # Place the elements in sorted order
    i = size - 1
    while i >= 0:
        index = array[i] // place
        output[count[index % 10] - 1] = array[i]
        count[index % 10] -= 1
        i -= 1

    for i in range(0, size):
        array[i] = output[i]


# Main function to implement radix sort
def radixSort(array):
    # Get maximum element
    max_element = max(array)

    # Apply counting sort to sort elements based on place value.
    place = 1
    while max_element // place > 0:
        countingSort(array, place)
        place *= 10


data = [121, 432, 564, 23, 1, 45, 788]
radixSort(data)
print(data)"
"Radix Sort (With Code in Python, C++, Java and C)","Radix sort is a sorting algorithm that sorts the elements by first grouping the individual digits of the same place value. Then, sort the elements according to their increasing/decreasing order. Suppose, we have an array of 8 elements. First, we will sort elements based on the value of the unit place. Then, we will sort elements based on the value of the tenth place. This process goes on until the last significant place. Let the initial array be [121, 432, 564, 23, 1, 45, 788]. It is sorted according to radix sort as shown in the figure below. Please go through the counting sort before reading this article because counting sort is used as an intermediate sort in radix sort. Find the largest element in the array, i.e. max. Let X be the number of digits in max. X is calculated because we have to go through all the significant places of all elements.
		
		In this array [121, 432, 564, 23, 1, 45, 788], we have the largest number 788. It has 3 digits. Therefore, the loop should go up to hundreds place (3 times). Now, go through each significant place one by one.
		
		Use any stable sorting technique to sort the digits at each significant place. We have used counting sort for this.
		
		Sort the elements based on the unit place digits (X=0).
		
			Using counting sort to sort elements based on unit place Now, sort the elements based on digits at tens place.
		
			Sort elements based on tens place Finally, sort the elements based on the digits at hundreds place.
		
			Sort elements based on hundreds place Since radix sort is a non-comparative algorithm, it has advantages over comparative sorting algorithms. For the radix sort that uses counting sort as an intermediate stable sort, the time complexity is O(d(n+k)).  Here, d is the number cycle and O(n+k) is the time complexity of counting sort. Thus, radix sort has linear time complexity which is better than O(nlog n) of comparative sorting algorithms. If we take very large digit numbers or the number of other bases like 32-bit and 64-bit numbers then it can perform in linear time however the intermediate sort takes large space. This makes radix sort space inefficient. This is the reason why this sort is not used in software libraries. Radix sort is implemented in DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array.
	places where there are numbers in large ranges. DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array. places where there are numbers in large ranges. Quicksort Merge Sort Bucket Sort Counting Sort","// Radix Sort in Java Programming

import java.util.Arrays;

class RadixSort {

  // Using counting sort to sort the elements in the basis of significant places
  void countingSort(int array[], int size, int place) {
    int[] output = new int[size + 1];
    int max = array[0];
    for (int i = 1; i < size; i++) {
      if (array[i] > max)
        max = array[i];
    }
    int[] count = new int[max + 1];

    for (int i = 0; i < max; ++i)
      count[i] = 0;

    // Calculate count of elements
    for (int i = 0; i < size; i++)
      count[(array[i] / place) % 10]++;

    // Calculate cumulative count
    for (int i = 1; i < 10; i++)
      count[i] += count[i - 1];

    // Place the elements in sorted order
    for (int i = size - 1; i >= 0; i--) {
      output[count[(array[i] / place) % 10] - 1] = array[i];
      count[(array[i] / place) % 10]--;
    }

    for (int i = 0; i < size; i++)
      array[i] = output[i];
  }

  // Function to get the largest element from an array
  int getMax(int array[], int n) {
    int max = array[0];
    for (int i = 1; i < n; i++)
      if (array[i] > max)
        max = array[i];
    return max;
  }

  // Main function to implement radix sort
  void radixSort(int array[], int size) {
    // Get maximum element
    int max = getMax(array, size);

    // Apply counting sort to sort elements based on place value.
    for (int place = 1; max / place > 0; place *= 10)
      countingSort(array, size, place);
  }

  // Driver code
  public static void main(String args[]) {
    int[] data = { 121, 432, 564, 23, 1, 45, 788 };
    int size = data.length;
    RadixSort rs = new RadixSort();
    rs.radixSort(data, size);
    System.out.println(""Sorted Array in Ascending Order: "");
    System.out.println(Arrays.toString(data));
  }
}"
"Radix Sort (With Code in Python, C++, Java and C)","Radix sort is a sorting algorithm that sorts the elements by first grouping the individual digits of the same place value. Then, sort the elements according to their increasing/decreasing order. Suppose, we have an array of 8 elements. First, we will sort elements based on the value of the unit place. Then, we will sort elements based on the value of the tenth place. This process goes on until the last significant place. Let the initial array be [121, 432, 564, 23, 1, 45, 788]. It is sorted according to radix sort as shown in the figure below. Please go through the counting sort before reading this article because counting sort is used as an intermediate sort in radix sort. Find the largest element in the array, i.e. max. Let X be the number of digits in max. X is calculated because we have to go through all the significant places of all elements.
		
		In this array [121, 432, 564, 23, 1, 45, 788], we have the largest number 788. It has 3 digits. Therefore, the loop should go up to hundreds place (3 times). Now, go through each significant place one by one.
		
		Use any stable sorting technique to sort the digits at each significant place. We have used counting sort for this.
		
		Sort the elements based on the unit place digits (X=0).
		
			Using counting sort to sort elements based on unit place Now, sort the elements based on digits at tens place.
		
			Sort elements based on tens place Finally, sort the elements based on the digits at hundreds place.
		
			Sort elements based on hundreds place Since radix sort is a non-comparative algorithm, it has advantages over comparative sorting algorithms. For the radix sort that uses counting sort as an intermediate stable sort, the time complexity is O(d(n+k)).  Here, d is the number cycle and O(n+k) is the time complexity of counting sort. Thus, radix sort has linear time complexity which is better than O(nlog n) of comparative sorting algorithms. If we take very large digit numbers or the number of other bases like 32-bit and 64-bit numbers then it can perform in linear time however the intermediate sort takes large space. This makes radix sort space inefficient. This is the reason why this sort is not used in software libraries. Radix sort is implemented in DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array.
	places where there are numbers in large ranges. DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array. places where there are numbers in large ranges. Quicksort Merge Sort Bucket Sort Counting Sort","// Radix Sort in C Programming

#include <stdio.h>

// Function to get the largest element from an array
int getMax(int array[], int n) {
  int max = array[0];
  for (int i = 1; i < n; i++)
    if (array[i] > max)
      max = array[i];
  return max;
}

// Using counting sort to sort the elements in the basis of significant places
void countingSort(int array[], int size, int place) {
  int output[size + 1];
  int max = (array[0] / place) % 10;

  for (int i = 1; i < size; i++) {
    if (((array[i] / place) % 10) > max)
      max = array[i];
  }
  int count[max + 1];

  for (int i = 0; i < max; ++i)
    count[i] = 0;

  // Calculate count of elements
  for (int i = 0; i < size; i++)
    count[(array[i] / place) % 10]++;
    
  // Calculate cumulative count
  for (int i = 1; i < 10; i++)
    count[i] += count[i - 1];

  // Place the elements in sorted order
  for (int i = size - 1; i >= 0; i--) {
    output[count[(array[i] / place) % 10] - 1] = array[i];
    count[(array[i] / place) % 10]--;
  }

  for (int i = 0; i < size; i++)
    array[i] = output[i];
}

// Main function to implement radix sort
void radixsort(int array[], int size) {
  // Get maximum element
  int max = getMax(array, size);

  // Apply counting sort to sort elements based on place value.
  for (int place = 1; max / place > 0; place *= 10)
    countingSort(array, size, place);
}

// Print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i) {
    printf(""%d  "", array[i]);
  }
  printf(""\n"");
}

// Driver code
int main() {
  int array[] = {121, 432, 564, 23, 1, 45, 788};
  int n = sizeof(array) / sizeof(array[0]);
  radixsort(array, n);
  printArray(array, n);
}"
"Radix Sort (With Code in Python, C++, Java and C)","Radix sort is a sorting algorithm that sorts the elements by first grouping the individual digits of the same place value. Then, sort the elements according to their increasing/decreasing order. Suppose, we have an array of 8 elements. First, we will sort elements based on the value of the unit place. Then, we will sort elements based on the value of the tenth place. This process goes on until the last significant place. Let the initial array be [121, 432, 564, 23, 1, 45, 788]. It is sorted according to radix sort as shown in the figure below. Please go through the counting sort before reading this article because counting sort is used as an intermediate sort in radix sort. Find the largest element in the array, i.e. max. Let X be the number of digits in max. X is calculated because we have to go through all the significant places of all elements.
		
		In this array [121, 432, 564, 23, 1, 45, 788], we have the largest number 788. It has 3 digits. Therefore, the loop should go up to hundreds place (3 times). Now, go through each significant place one by one.
		
		Use any stable sorting technique to sort the digits at each significant place. We have used counting sort for this.
		
		Sort the elements based on the unit place digits (X=0).
		
			Using counting sort to sort elements based on unit place Now, sort the elements based on digits at tens place.
		
			Sort elements based on tens place Finally, sort the elements based on the digits at hundreds place.
		
			Sort elements based on hundreds place Since radix sort is a non-comparative algorithm, it has advantages over comparative sorting algorithms. For the radix sort that uses counting sort as an intermediate stable sort, the time complexity is O(d(n+k)).  Here, d is the number cycle and O(n+k) is the time complexity of counting sort. Thus, radix sort has linear time complexity which is better than O(nlog n) of comparative sorting algorithms. If we take very large digit numbers or the number of other bases like 32-bit and 64-bit numbers then it can perform in linear time however the intermediate sort takes large space. This makes radix sort space inefficient. This is the reason why this sort is not used in software libraries. Radix sort is implemented in DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array.
	places where there are numbers in large ranges. DC3 algorithm (Kärkkäinen-Sanders-Burkhardt) while making a suffix array. places where there are numbers in large ranges. Quicksort Merge Sort Bucket Sort Counting Sort","// Radix Sort in C++ Programming

#include <iostream>
using namespace std;

// Function to get the largest element from an array
int getMax(int array[], int n) {
  int max = array[0];
  for (int i = 1; i < n; i++)
    if (array[i] > max)
      max = array[i];
  return max;
}

// Using counting sort to sort the elements in the basis of significant places
void countingSort(int array[], int size, int place) {
  const int max = 10;
  int output[size];
  int count[max];

  for (int i = 0; i < max; ++i)
    count[i] = 0;

  // Calculate count of elements
  for (int i = 0; i < size; i++)
    count[(array[i] / place) % 10]++;

  // Calculate cumulative count
  for (int i = 1; i < max; i++)
    count[i] += count[i - 1];

  // Place the elements in sorted order
  for (int i = size - 1; i >= 0; i--) {
    output[count[(array[i] / place) % 10] - 1] = array[i];
    count[(array[i] / place) % 10]--;
  }

  for (int i = 0; i < size; i++)
    array[i] = output[i];
}

// Main function to implement radix sort
void radixsort(int array[], int size) {
  // Get maximum element
  int max = getMax(array, size);

  // Apply counting sort to sort elements based on place value.
  for (int place = 1; max / place > 0; place *= 10)
    countingSort(array, size, place);
}

// Print an array
void printArray(int array[], int size) {
  int i;
  for (i = 0; i < size; i++)
    cout << array[i] << "" "";
  cout << endl;
}

// Driver code
int main() {
  int array[] = {121, 432, 564, 23, 1, 45, 788};
  int n = sizeof(array) / sizeof(array[0]);
  radixsort(array, n);
  printArray(array, n);
}"
"Bucket Sort (With Code in Python, C++, Java and C)","Bucket Sort is a sorting algorithm that divides the unsorted array elements into several groups called buckets. Each bucket is then sorted by using any of the suitable sorting algorithms or recursively applying the same bucket algorithm. Finally, the sorted buckets are combined to form a final sorted array. Scatter Gather Approach The process of bucket sort can be understood as a scatter-gather approach. Here, elements are first scattered into buckets then the elements in each bucket are sorted. Finally, the elements are gathered in order. Suppose, the input array is:
		
			Input array
		
		
		Create an array of size 10. Each slot of this array is used as a bucket for storing elements.
		
			Array in which each position is a bucket Insert elements into the buckets from the array. The elements are inserted according to the range of the bucket.
		
		In our example code, we have buckets each of ranges from 0 to 1, 1 to 2, 2 to 3,...... (n-1) to n.
		Suppose, an input element is .23 is taken. It is multiplied by size = 10 (ie. .23*10=2.3). Then, it is converted into an integer (ie. 2.3≈2). Finally, .23 is inserted into bucket-2.
		
			Insert elements into the buckets from the array
		
		
		Similarly, .25 is also inserted into the same bucket. Everytime, the floor value of the floating point number is taken.
		
		If we take integer numbers as input, we have to divide it by the interval (10 here) to get the floor value.
		
		Similarly, other elements are inserted into their respective buckets.
		
			Insert all the elements into the buckets from the array The elements of each bucket are sorted using any of the stable sorting algorithms. Here, we have used quicksort (inbuilt function).
		
			Sort the elements in each bucket The elements from each bucket are gathered.
		
		It is done by iterating through the bucket and inserting an individual element into the original array in each cycle. The element from the bucket is erased once it is copied into the original array.
		
			Gather elements from each bucket Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2).
	Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case.
	Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2). Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case. Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Bucket sort is used when: input is uniformly distributed over a range.
	there are floating point values input is uniformly distributed over a range. there are floating point values Bubble Sort
	Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Bubble Sort Quicksort Insertion Sort Merge Sort Selection Sort","# Bucket Sort in Python


def bucketSort(array):
    bucket = []

    # Create empty buckets
    for i in range(len(array)):
        bucket.append([])

    # Insert elements into their respective buckets
    for j in array:
        index_b = int(10 * j)
        bucket[index_b].append(j)

    # Sort the elements of each bucket
    for i in range(len(array)):
        bucket[i] = sorted(bucket[i])

    # Get the sorted elements
    k = 0
    for i in range(len(array)):
        for j in range(len(bucket[i])):
            array[k] = bucket[i][j]
            k += 1
    return array


array = [.42, .32, .33, .52, .37, .47, .51]
print(""Sorted Array in descending order is"")
print(bucketSort(array))"
"Bucket Sort (With Code in Python, C++, Java and C)","Bucket Sort is a sorting algorithm that divides the unsorted array elements into several groups called buckets. Each bucket is then sorted by using any of the suitable sorting algorithms or recursively applying the same bucket algorithm. Finally, the sorted buckets are combined to form a final sorted array. Scatter Gather Approach The process of bucket sort can be understood as a scatter-gather approach. Here, elements are first scattered into buckets then the elements in each bucket are sorted. Finally, the elements are gathered in order. Suppose, the input array is:
		
			Input array
		
		
		Create an array of size 10. Each slot of this array is used as a bucket for storing elements.
		
			Array in which each position is a bucket Insert elements into the buckets from the array. The elements are inserted according to the range of the bucket.
		
		In our example code, we have buckets each of ranges from 0 to 1, 1 to 2, 2 to 3,...... (n-1) to n.
		Suppose, an input element is .23 is taken. It is multiplied by size = 10 (ie. .23*10=2.3). Then, it is converted into an integer (ie. 2.3≈2). Finally, .23 is inserted into bucket-2.
		
			Insert elements into the buckets from the array
		
		
		Similarly, .25 is also inserted into the same bucket. Everytime, the floor value of the floating point number is taken.
		
		If we take integer numbers as input, we have to divide it by the interval (10 here) to get the floor value.
		
		Similarly, other elements are inserted into their respective buckets.
		
			Insert all the elements into the buckets from the array The elements of each bucket are sorted using any of the stable sorting algorithms. Here, we have used quicksort (inbuilt function).
		
			Sort the elements in each bucket The elements from each bucket are gathered.
		
		It is done by iterating through the bucket and inserting an individual element into the original array in each cycle. The element from the bucket is erased once it is copied into the original array.
		
			Gather elements from each bucket Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2).
	Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case.
	Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2). Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case. Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Bucket sort is used when: input is uniformly distributed over a range.
	there are floating point values input is uniformly distributed over a range. there are floating point values Bubble Sort
	Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Bubble Sort Quicksort Insertion Sort Merge Sort Selection Sort","// Bucket sort in Java

import java.util.ArrayList;
import java.util.Collections;

public class BucketSort {
  public void bucketSort(float[] arr, int n) {
    if (n <= 0)
      return;
    @SuppressWarnings(""unchecked"")
    ArrayList<Float>[] bucket = new ArrayList[n];

    // Create empty buckets
    for (int i = 0; i < n; i++)
      bucket[i] = new ArrayList<Float>();

    // Add elements into the buckets
    for (int i = 0; i < n; i++) {
      int bucketIndex = (int) arr[i] * n;
      bucket[bucketIndex].add(arr[i]);
    }

    // Sort the elements of each bucket
    for (int i = 0; i < n; i++) {
      Collections.sort((bucket[i]));
    }

    // Get the sorted array
    int index = 0;
    for (int i = 0; i < n; i++) {
      for (int j = 0, size = bucket[i].size(); j < size; j++) {
        arr[index++] = bucket[i].get(j);
      }
    }
  }

  // Driver code
  public static void main(String[] args) {
    BucketSort b = new BucketSort();
    float[] arr = { (float) 0.42, (float) 0.32, (float) 0.33, (float) 0.52, (float) 0.37, (float) 0.47,
        (float) 0.51 };
    b.bucketSort(arr, 7);

    for (float i : arr)
      System.out.print(i + ""  "");
  }
}"
"Bucket Sort (With Code in Python, C++, Java and C)","Bucket Sort is a sorting algorithm that divides the unsorted array elements into several groups called buckets. Each bucket is then sorted by using any of the suitable sorting algorithms or recursively applying the same bucket algorithm. Finally, the sorted buckets are combined to form a final sorted array. Scatter Gather Approach The process of bucket sort can be understood as a scatter-gather approach. Here, elements are first scattered into buckets then the elements in each bucket are sorted. Finally, the elements are gathered in order. Suppose, the input array is:
		
			Input array
		
		
		Create an array of size 10. Each slot of this array is used as a bucket for storing elements.
		
			Array in which each position is a bucket Insert elements into the buckets from the array. The elements are inserted according to the range of the bucket.
		
		In our example code, we have buckets each of ranges from 0 to 1, 1 to 2, 2 to 3,...... (n-1) to n.
		Suppose, an input element is .23 is taken. It is multiplied by size = 10 (ie. .23*10=2.3). Then, it is converted into an integer (ie. 2.3≈2). Finally, .23 is inserted into bucket-2.
		
			Insert elements into the buckets from the array
		
		
		Similarly, .25 is also inserted into the same bucket. Everytime, the floor value of the floating point number is taken.
		
		If we take integer numbers as input, we have to divide it by the interval (10 here) to get the floor value.
		
		Similarly, other elements are inserted into their respective buckets.
		
			Insert all the elements into the buckets from the array The elements of each bucket are sorted using any of the stable sorting algorithms. Here, we have used quicksort (inbuilt function).
		
			Sort the elements in each bucket The elements from each bucket are gathered.
		
		It is done by iterating through the bucket and inserting an individual element into the original array in each cycle. The element from the bucket is erased once it is copied into the original array.
		
			Gather elements from each bucket Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2).
	Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case.
	Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2). Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case. Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Bucket sort is used when: input is uniformly distributed over a range.
	there are floating point values input is uniformly distributed over a range. there are floating point values Bubble Sort
	Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Bubble Sort Quicksort Insertion Sort Merge Sort Selection Sort","// Bucket sort in C

#include <stdio.h>
#include <stdlib.h>

#define NARRAY 7   // Array size
#define NBUCKET 6  // Number of buckets
#define INTERVAL 10  // Each bucket capacity

struct Node {
  int data;
  struct Node *next;
};

void BucketSort(int arr[]);
struct Node *InsertionSort(struct Node *list);
void print(int arr[]);
void printBuckets(struct Node *list);
int getBucketIndex(int value);

// Sorting function
void BucketSort(int arr[]) {
  int i, j;
  struct Node **buckets;

  // Create buckets and allocate memory size
  buckets = (struct Node **)malloc(sizeof(struct Node *) * NBUCKET);

  // Initialize empty buckets
  for (i = 0; i < NBUCKET; ++i) {
    buckets[i] = NULL;
  }

  // Fill the buckets with respective elements
  for (i = 0; i < NARRAY; ++i) {
    struct Node *current;
    int pos = getBucketIndex(arr[i]);
    current = (struct Node *)malloc(sizeof(struct Node));
    current->data = arr[i];
    current->next = buckets[pos];
    buckets[pos] = current;
  }

  // Print the buckets along with their elements
  for (i = 0; i < NBUCKET; i++) {
    printf(""Bucket[%d]: "", i);
    printBuckets(buckets[i]);
    printf(""\n"");
  }

  // Sort the elements of each bucket
  for (i = 0; i < NBUCKET; ++i) {
    buckets[i] = InsertionSort(buckets[i]);
  }

  printf(""-------------\n"");
  printf(""Bucktets after sorting\n"");
  for (i = 0; i < NBUCKET; i++) {
    printf(""Bucket[%d]: "", i);
    printBuckets(buckets[i]);
    printf(""\n"");
  }

  // Put sorted elements on arr
  for (j = 0, i = 0; i < NBUCKET; ++i) {
    struct Node *node;
    node = buckets[i];
    while (node) {
      arr[j++] = node->data;
      node = node->next;
    }
  }

  return;
}

// Function to sort the elements of each bucket
struct Node *InsertionSort(struct Node *list) {
  struct Node *k, *nodeList;
  if (list == 0 || list->next == 0) {
    return list;
  }

  nodeList = list;
  k = list->next;
  nodeList->next = 0;
  while (k != 0) {
    struct Node *ptr;
    if (nodeList->data > k->data) {
      struct Node *tmp;
      tmp = k;
      k = k->next;
      tmp->next = nodeList;
      nodeList = tmp;
      continue;
    }

    for (ptr = nodeList; ptr->next != 0; ptr = ptr->next) {
      if (ptr->next->data > k->data)
        break;
    }

    if (ptr->next != 0) {
      struct Node *tmp;
      tmp = k;
      k = k->next;
      tmp->next = ptr->next;
      ptr->next = tmp;
      continue;
    } else {
      ptr->next = k;
      k = k->next;
      ptr->next->next = 0;
      continue;
    }
  }
  return nodeList;
}

int getBucketIndex(int value) {
  return value / INTERVAL;
}

void print(int ar[]) {
  int i;
  for (i = 0; i < NARRAY; ++i) {
    printf(""%d "", ar[i]);
  }
  printf(""\n"");
}

// Print buckets
void printBuckets(struct Node *list) {
  struct Node *cur = list;
  while (cur) {
    printf(""%d "", cur->data);
    cur = cur->next;
  }
}

// Driver code
int main(void) {
  int array[NARRAY] = {42, 32, 33, 52, 37, 47, 51};

  printf(""Initial array: "");
  print(array);
  printf(""-------------\n"");

  BucketSort(array);
  printf(""-------------\n"");
  printf(""Sorted array: "");
  print(array);
  return 0;
}"
"Bucket Sort (With Code in Python, C++, Java and C)","Bucket Sort is a sorting algorithm that divides the unsorted array elements into several groups called buckets. Each bucket is then sorted by using any of the suitable sorting algorithms or recursively applying the same bucket algorithm. Finally, the sorted buckets are combined to form a final sorted array. Scatter Gather Approach The process of bucket sort can be understood as a scatter-gather approach. Here, elements are first scattered into buckets then the elements in each bucket are sorted. Finally, the elements are gathered in order. Suppose, the input array is:
		
			Input array
		
		
		Create an array of size 10. Each slot of this array is used as a bucket for storing elements.
		
			Array in which each position is a bucket Insert elements into the buckets from the array. The elements are inserted according to the range of the bucket.
		
		In our example code, we have buckets each of ranges from 0 to 1, 1 to 2, 2 to 3,...... (n-1) to n.
		Suppose, an input element is .23 is taken. It is multiplied by size = 10 (ie. .23*10=2.3). Then, it is converted into an integer (ie. 2.3≈2). Finally, .23 is inserted into bucket-2.
		
			Insert elements into the buckets from the array
		
		
		Similarly, .25 is also inserted into the same bucket. Everytime, the floor value of the floating point number is taken.
		
		If we take integer numbers as input, we have to divide it by the interval (10 here) to get the floor value.
		
		Similarly, other elements are inserted into their respective buckets.
		
			Insert all the elements into the buckets from the array The elements of each bucket are sorted using any of the stable sorting algorithms. Here, we have used quicksort (inbuilt function).
		
			Sort the elements in each bucket The elements from each bucket are gathered.
		
		It is done by iterating through the bucket and inserting an individual element into the original array in each cycle. The element from the bucket is erased once it is copied into the original array.
		
			Gather elements from each bucket Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2).
	Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case.
	Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Worst Case Complexity: O(n2)
		When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others.
		It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket.
		The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2). Best Case Complexity: O(n+k)
		It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket.
		The complexity becomes even better if the elements inside the buckets are already sorted.
		If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case. Average Case Complexity: O(n)
		It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements. Bucket sort is used when: input is uniformly distributed over a range.
	there are floating point values input is uniformly distributed over a range. there are floating point values Bubble Sort
	Quicksort
	Insertion Sort
	Merge Sort
	Selection Sort Bubble Sort Quicksort Insertion Sort Merge Sort Selection Sort","// Bucket sort in C++

#include <iomanip>
#include <iostream>
using namespace std;

#define NARRAY 7   // Array size
#define NBUCKET 6  // Number of buckets
#define INTERVAL 10  // Each bucket capacity

struct Node {
  int data;
  struct Node *next;
};

void BucketSort(int arr[]);
struct Node *InsertionSort(struct Node *list);
void print(int arr[]);
void printBuckets(struct Node *list);
int getBucketIndex(int value);

// Sorting function
void BucketSort(int arr[]) {
  int i, j;
  struct Node **buckets;

  // Create buckets and allocate memory size
  buckets = (struct Node **)malloc(sizeof(struct Node *) * NBUCKET);

  // Initialize empty buckets
  for (i = 0; i < NBUCKET; ++i) {
    buckets[i] = NULL;
  }

  // Fill the buckets with respective elements
  for (i = 0; i < NARRAY; ++i) {
    struct Node *current;
    int pos = getBucketIndex(arr[i]);
    current = (struct Node *)malloc(sizeof(struct Node));
    current->data = arr[i];
    current->next = buckets[pos];
    buckets[pos] = current;
  }

  // Print the buckets along with their elements
  for (i = 0; i < NBUCKET; i++) {
    cout << ""Bucket["" << i << ""] : "";
    printBuckets(buckets[i]);
    cout << endl;
  }

  // Sort the elements of each bucket
  for (i = 0; i < NBUCKET; ++i) {
    buckets[i] = InsertionSort(buckets[i]);
  }

  cout << ""-------------"" << endl;
  cout << ""Bucktets after sorted"" << endl;
  for (i = 0; i < NBUCKET; i++) {
    cout << ""Bucket["" << i << ""] : "";
    printBuckets(buckets[i]);
    cout << endl;
  }

  // Put sorted elements on arr
  for (j = 0, i = 0; i < NBUCKET; ++i) {
    struct Node *node;
    node = buckets[i];
    while (node) {
      arr[j++] = node->data;
      node = node->next;
    }
  }

  for (i = 0; i < NBUCKET; ++i) {
    struct Node *node;
    node = buckets[i];
    while (node) {
      struct Node *tmp;
      tmp = node;
      node = node->next;
      free(tmp);
    }
  }
  free(buckets);
  return;
}

// Function to sort the elements of each bucket
struct Node *InsertionSort(struct Node *list) {
  struct Node *k, *nodeList;
  if (list == 0 || list->next == 0) {
    return list;
  }

  nodeList = list;
  k = list->next;
  nodeList->next = 0;
  while (k != 0) {
    struct Node *ptr;
    if (nodeList->data > k->data) {
      struct Node *tmp;
      tmp = k;
      k = k->next;
      tmp->next = nodeList;
      nodeList = tmp;
      continue;
    }

    for (ptr = nodeList; ptr->next != 0; ptr = ptr->next) {
      if (ptr->next->data > k->data)
        break;
    }

    if (ptr->next != 0) {
      struct Node *tmp;
      tmp = k;
      k = k->next;
      tmp->next = ptr->next;
      ptr->next = tmp;
      continue;
    } else {
      ptr->next = k;
      k = k->next;
      ptr->next->next = 0;
      continue;
    }
  }
  return nodeList;
}

int getBucketIndex(int value) {
  return value / INTERVAL;
}

// Print buckets
void print(int ar[]) {
  int i;
  for (i = 0; i < NARRAY; ++i) {
    cout << setw(3) << ar[i];
  }
  cout << endl;
}

void printBuckets(struct Node *list) {
  struct Node *cur = list;
  while (cur) {
    cout << setw(3) << cur->data;
    cur = cur->next;
  }
}

// Driver code
int main(void) {
  int array[NARRAY] = {42, 32, 33, 52, 37, 47, 51};

  cout << ""Initial array: "" << endl;
  print(array);
  cout << ""-------------"" << endl;

  BucketSort(array);
  cout << ""-------------"" << endl;
  cout << ""Sorted array: "" << endl;
  print(array);
}"
"Heap Sort (With Code in Python, C++, Java and C)","Heap Sort is a popular and efficient sorting algorithm in computer programming. Learning how to write the heap sort algorithm requires knowledge of two types of data structures - arrays and trees. The initial set of numbers that we want to sort is stored in an array e.g. [10, 3, 76, 34, 23, 32] and after sorting, we get a sorted array [3,10,23,32,34,76]. Heap sort works by visualizing the elements of the array as a special kind of complete binary tree called a heap. Note: As a prerequisite, you must know about a complete binary tree and heap data structure. A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2. Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap is a special tree-based data structure. A binary tree is said to follow a heap data structure if it is a complete binary tree
	All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap it is a complete binary tree All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap The following example diagram shows Max-Heap and Min-Heap. To learn more about it, please visit Heap Data Structure. Starting from a complete binary tree, we can modify it to become a Max-Heap by running a function called heapify on all the non-leaf elements of the heap. Since heapify uses recursion, it can be difficult to grasp. So let's first think about how you would heapify a tree with just three elements. The example above shows two scenarios - one in which the root is the largest element and we don't need to do anything. And another in which the root had a larger element as a child and we needed to swap to maintain max-heap property. If you're worked with recursive algorithms before, you've probably identified that this must be the base case. Now let's think of another scenario in which there is more than one level.  The top element isn't a max-heap but all the sub-trees are max-heaps. To maintain the max-heap property for the entire tree, we will have to keep pushing 2 downwards until it reaches its correct position. Thus, to maintain the max-heap property in a tree where both sub-trees are max-heaps, we need to run heapify on the root element repeatedly until it is larger than its children or it becomes a leaf node. We can combine both these conditions in one heapify function as This function works for both the base case and for a tree of any size. We can thus move the root element to the correct position to maintain the max-heap status for any tree size as long as the sub-trees are max-heaps. To build a max-heap from any tree, we can thus start heapifying each sub-tree from the bottom up and end up with a max-heap after the function is applied to all the elements including the root element. In the case of a complete tree, the first index of a non-leaf node is given by n/2 - 1. All other nodes after that are leaf-nodes and thus don't need to be heapified. So, we can build a maximum heap as As shown in the above diagram, we start by heapifying the lowest smallest trees and gradually move up until we reach the root element. If you've understood everything till here, congratulations, you are on your way to mastering the Heap sort. Since the tree satisfies Max-Heap property, then the largest item is stored at the root node. Swap: Remove the root element and put at the end of the array (nth position) Put the last item of the tree (heap) at the vacant place. Remove: Reduce the size of the heap by 1. Heapify: Heapify the root element again so that we have the highest element at root. The process is repeated until all the items of the list are sorted. The code below shows the operation. Heap Sort has O(nlog n) time complexities for all the cases ( best case, average case, and worst case). Let us understand the reason why. The height of a complete binary tree containing n elements is log n As we have seen earlier, to fully heapify an element whose subtrees are already max-heaps, we need to keep comparing the element with its left and right children and pushing it downwards until it reaches a point where both its children are smaller than it. In the worst case scenario, we will need to move an element from the root to the leaf node making a multiple of log(n) comparisons and swaps. During the build_max_heap stage, we do that for n/2 elements so the worst case complexity of the build_heap step is n/2*log n ~ nlog n. During the sorting step, we exchange the root element with the last element and heapify the root element. For each element, this again takes log n worst time because we might have to bring the element all the way from the root to the leaf. Since we repeat this n times, the heap_sort step is also nlog n. Also since the build_max_heap and heap_sort steps are executed one after another, the algorithmic complexity is not multiplied and it remains in the order of nlog n. Also it performs sorting in O(1) space complexity. Compared with Quick Sort, it has a better worst case ( O(nlog n) ). Quick Sort has complexity O(n^2) for worst case. But in other cases, Quick Sort is fast. Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort. Systems concerned with security and embedded systems such as Linux Kernel use Heap Sort because of the O(n log n) upper bound on Heapsort's running time and constant O(1) upper bound on its auxiliary storage. Although Heap Sort has O(n log n) time complexity even for the worst case, it doesn't have more applications ( compared to other sorting algorithms like Quick Sort, Merge Sort ). However, its underlying data structure, heap, can be efficiently used if we want to extract the smallest (or largest) from the list of items without the overhead of keeping the remaining items in the sorted order. For e.g Priority Queues. Quicksort Merge Sort","# Heap Sort in python


  def heapify(arr, n, i):
      # Find largest among root and children
      largest = i
      l = 2 * i + 1
      r = 2 * i + 2
  
      if l < n and arr[i] < arr[l]:
          largest = l
  
      if r < n and arr[largest] < arr[r]:
          largest = r
  
      # If root is not largest, swap with largest and continue heapifying
      if largest != i:
          arr[i], arr[largest] = arr[largest], arr[i]
          heapify(arr, n, largest)
  
  
  def heapSort(arr):
      n = len(arr)
  
      # Build max heap
      for i in range(n//2, -1, -1):
          heapify(arr, n, i)
  
      for i in range(n-1, 0, -1):
          # Swap
          arr[i], arr[0] = arr[0], arr[i]
  
          # Heapify root element
          heapify(arr, i, 0)
  
  
  arr = [1, 12, 9, 5, 6, 10]
  heapSort(arr)
  n = len(arr)
  print(""Sorted array is"")
  for i in range(n):
      print(""%d "" % arr[i], end='')
  "
"Heap Sort (With Code in Python, C++, Java and C)","Heap Sort is a popular and efficient sorting algorithm in computer programming. Learning how to write the heap sort algorithm requires knowledge of two types of data structures - arrays and trees. The initial set of numbers that we want to sort is stored in an array e.g. [10, 3, 76, 34, 23, 32] and after sorting, we get a sorted array [3,10,23,32,34,76]. Heap sort works by visualizing the elements of the array as a special kind of complete binary tree called a heap. Note: As a prerequisite, you must know about a complete binary tree and heap data structure. A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2. Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap is a special tree-based data structure. A binary tree is said to follow a heap data structure if it is a complete binary tree
	All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap it is a complete binary tree All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap The following example diagram shows Max-Heap and Min-Heap. To learn more about it, please visit Heap Data Structure. Starting from a complete binary tree, we can modify it to become a Max-Heap by running a function called heapify on all the non-leaf elements of the heap. Since heapify uses recursion, it can be difficult to grasp. So let's first think about how you would heapify a tree with just three elements. The example above shows two scenarios - one in which the root is the largest element and we don't need to do anything. And another in which the root had a larger element as a child and we needed to swap to maintain max-heap property. If you're worked with recursive algorithms before, you've probably identified that this must be the base case. Now let's think of another scenario in which there is more than one level.  The top element isn't a max-heap but all the sub-trees are max-heaps. To maintain the max-heap property for the entire tree, we will have to keep pushing 2 downwards until it reaches its correct position. Thus, to maintain the max-heap property in a tree where both sub-trees are max-heaps, we need to run heapify on the root element repeatedly until it is larger than its children or it becomes a leaf node. We can combine both these conditions in one heapify function as This function works for both the base case and for a tree of any size. We can thus move the root element to the correct position to maintain the max-heap status for any tree size as long as the sub-trees are max-heaps. To build a max-heap from any tree, we can thus start heapifying each sub-tree from the bottom up and end up with a max-heap after the function is applied to all the elements including the root element. In the case of a complete tree, the first index of a non-leaf node is given by n/2 - 1. All other nodes after that are leaf-nodes and thus don't need to be heapified. So, we can build a maximum heap as As shown in the above diagram, we start by heapifying the lowest smallest trees and gradually move up until we reach the root element. If you've understood everything till here, congratulations, you are on your way to mastering the Heap sort. Since the tree satisfies Max-Heap property, then the largest item is stored at the root node. Swap: Remove the root element and put at the end of the array (nth position) Put the last item of the tree (heap) at the vacant place. Remove: Reduce the size of the heap by 1. Heapify: Heapify the root element again so that we have the highest element at root. The process is repeated until all the items of the list are sorted. The code below shows the operation. Heap Sort has O(nlog n) time complexities for all the cases ( best case, average case, and worst case). Let us understand the reason why. The height of a complete binary tree containing n elements is log n As we have seen earlier, to fully heapify an element whose subtrees are already max-heaps, we need to keep comparing the element with its left and right children and pushing it downwards until it reaches a point where both its children are smaller than it. In the worst case scenario, we will need to move an element from the root to the leaf node making a multiple of log(n) comparisons and swaps. During the build_max_heap stage, we do that for n/2 elements so the worst case complexity of the build_heap step is n/2*log n ~ nlog n. During the sorting step, we exchange the root element with the last element and heapify the root element. For each element, this again takes log n worst time because we might have to bring the element all the way from the root to the leaf. Since we repeat this n times, the heap_sort step is also nlog n. Also since the build_max_heap and heap_sort steps are executed one after another, the algorithmic complexity is not multiplied and it remains in the order of nlog n. Also it performs sorting in O(1) space complexity. Compared with Quick Sort, it has a better worst case ( O(nlog n) ). Quick Sort has complexity O(n^2) for worst case. But in other cases, Quick Sort is fast. Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort. Systems concerned with security and embedded systems such as Linux Kernel use Heap Sort because of the O(n log n) upper bound on Heapsort's running time and constant O(1) upper bound on its auxiliary storage. Although Heap Sort has O(n log n) time complexity even for the worst case, it doesn't have more applications ( compared to other sorting algorithms like Quick Sort, Merge Sort ). However, its underlying data structure, heap, can be efficiently used if we want to extract the smallest (or largest) from the list of items without the overhead of keeping the remaining items in the sorted order. For e.g Priority Queues. Quicksort Merge Sort","// Heap Sort in Java
  
  public class HeapSort {
  
    public void sort(int arr[]) {
      int n = arr.length;
  
      // Build max heap
      for (int i = n / 2 - 1; i >= 0; i--) {
        heapify(arr, n, i);
      }
  
      // Heap sort
      for (int i = n - 1; i >= 0; i--) {
        int temp = arr[0];
        arr[0] = arr[i];
        arr[i] = temp;
  
        // Heapify root element
        heapify(arr, i, 0);
      }
    }
  
    void heapify(int arr[], int n, int i) {
      // Find largest among root, left child and right child
      int largest = i;
      int l = 2 * i + 1;
      int r = 2 * i + 2;
  
      if (l < n && arr[l] > arr[largest])
        largest = l;
  
      if (r < n && arr[r] > arr[largest])
        largest = r;
  
      // Swap and continue heapifying if root is not largest
      if (largest != i) {
        int swap = arr[i];
        arr[i] = arr[largest];
        arr[largest] = swap;
  
        heapify(arr, n, largest);
      }
    }
  
    // Function to print an array
    static void printArray(int arr[]) {
      int n = arr.length;
      for (int i = 0; i < n; ++i)
        System.out.print(arr[i] + "" "");
      System.out.println();
    }
  
    // Driver code
    public static void main(String args[]) {
      int arr[] = { 1, 12, 9, 5, 6, 10 };
  
      HeapSort hs = new HeapSort();
      hs.sort(arr);
  
      System.out.println(""Sorted array is"");
      printArray(arr);
    }
  }"
"Heap Sort (With Code in Python, C++, Java and C)","Heap Sort is a popular and efficient sorting algorithm in computer programming. Learning how to write the heap sort algorithm requires knowledge of two types of data structures - arrays and trees. The initial set of numbers that we want to sort is stored in an array e.g. [10, 3, 76, 34, 23, 32] and after sorting, we get a sorted array [3,10,23,32,34,76]. Heap sort works by visualizing the elements of the array as a special kind of complete binary tree called a heap. Note: As a prerequisite, you must know about a complete binary tree and heap data structure. A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2. Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap is a special tree-based data structure. A binary tree is said to follow a heap data structure if it is a complete binary tree
	All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap it is a complete binary tree All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap The following example diagram shows Max-Heap and Min-Heap. To learn more about it, please visit Heap Data Structure. Starting from a complete binary tree, we can modify it to become a Max-Heap by running a function called heapify on all the non-leaf elements of the heap. Since heapify uses recursion, it can be difficult to grasp. So let's first think about how you would heapify a tree with just three elements. The example above shows two scenarios - one in which the root is the largest element and we don't need to do anything. And another in which the root had a larger element as a child and we needed to swap to maintain max-heap property. If you're worked with recursive algorithms before, you've probably identified that this must be the base case. Now let's think of another scenario in which there is more than one level.  The top element isn't a max-heap but all the sub-trees are max-heaps. To maintain the max-heap property for the entire tree, we will have to keep pushing 2 downwards until it reaches its correct position. Thus, to maintain the max-heap property in a tree where both sub-trees are max-heaps, we need to run heapify on the root element repeatedly until it is larger than its children or it becomes a leaf node. We can combine both these conditions in one heapify function as This function works for both the base case and for a tree of any size. We can thus move the root element to the correct position to maintain the max-heap status for any tree size as long as the sub-trees are max-heaps. To build a max-heap from any tree, we can thus start heapifying each sub-tree from the bottom up and end up with a max-heap after the function is applied to all the elements including the root element. In the case of a complete tree, the first index of a non-leaf node is given by n/2 - 1. All other nodes after that are leaf-nodes and thus don't need to be heapified. So, we can build a maximum heap as As shown in the above diagram, we start by heapifying the lowest smallest trees and gradually move up until we reach the root element. If you've understood everything till here, congratulations, you are on your way to mastering the Heap sort. Since the tree satisfies Max-Heap property, then the largest item is stored at the root node. Swap: Remove the root element and put at the end of the array (nth position) Put the last item of the tree (heap) at the vacant place. Remove: Reduce the size of the heap by 1. Heapify: Heapify the root element again so that we have the highest element at root. The process is repeated until all the items of the list are sorted. The code below shows the operation. Heap Sort has O(nlog n) time complexities for all the cases ( best case, average case, and worst case). Let us understand the reason why. The height of a complete binary tree containing n elements is log n As we have seen earlier, to fully heapify an element whose subtrees are already max-heaps, we need to keep comparing the element with its left and right children and pushing it downwards until it reaches a point where both its children are smaller than it. In the worst case scenario, we will need to move an element from the root to the leaf node making a multiple of log(n) comparisons and swaps. During the build_max_heap stage, we do that for n/2 elements so the worst case complexity of the build_heap step is n/2*log n ~ nlog n. During the sorting step, we exchange the root element with the last element and heapify the root element. For each element, this again takes log n worst time because we might have to bring the element all the way from the root to the leaf. Since we repeat this n times, the heap_sort step is also nlog n. Also since the build_max_heap and heap_sort steps are executed one after another, the algorithmic complexity is not multiplied and it remains in the order of nlog n. Also it performs sorting in O(1) space complexity. Compared with Quick Sort, it has a better worst case ( O(nlog n) ). Quick Sort has complexity O(n^2) for worst case. But in other cases, Quick Sort is fast. Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort. Systems concerned with security and embedded systems such as Linux Kernel use Heap Sort because of the O(n log n) upper bound on Heapsort's running time and constant O(1) upper bound on its auxiliary storage. Although Heap Sort has O(n log n) time complexity even for the worst case, it doesn't have more applications ( compared to other sorting algorithms like Quick Sort, Merge Sort ). However, its underlying data structure, heap, can be efficiently used if we want to extract the smallest (or largest) from the list of items without the overhead of keeping the remaining items in the sorted order. For e.g Priority Queues. Quicksort Merge Sort","// Heap Sort in C
  
  #include <stdio.h>
  
  // Function to swap the the position of two elements
  void swap(int *a, int *b) {
    int temp = *a;
    *a = *b;
    *b = temp;
  }
  
  void heapify(int arr[], int n, int i) {
    // Find largest among root, left child and right child
    int largest = i;
    int left = 2 * i + 1;
    int right = 2 * i + 2;
  
    if (left < n && arr[left] > arr[largest])
      largest = left;
  
    if (right < n && arr[right] > arr[largest])
      largest = right;
  
    // Swap and continue heapifying if root is not largest
    if (largest != i) {
      swap(&arr[i], &arr[largest]);
      heapify(arr, n, largest);
    }
  }
  
  // Main function to do heap sort
  void heapSort(int arr[], int n) {
    // Build max heap
    for (int i = n / 2 - 1; i >= 0; i--)
      heapify(arr, n, i);
  
    // Heap sort
    for (int i = n - 1; i >= 0; i--) {
      swap(&arr[0], &arr[i]);
  
      // Heapify root element to get highest element at root again
      heapify(arr, i, 0);
    }
  }
  
  // Print an array
  void printArray(int arr[], int n) {
    for (int i = 0; i < n; ++i)
      printf(""%d "", arr[i]);
    printf(""\n"");
  }
  
  // Driver code
  int main() {
    int arr[] = {1, 12, 9, 5, 6, 10};
    int n = sizeof(arr) / sizeof(arr[0]);
  
    heapSort(arr, n);
  
    printf(""Sorted array is \n"");
    printArray(arr, n);
  }"
"Heap Sort (With Code in Python, C++, Java and C)","Heap Sort is a popular and efficient sorting algorithm in computer programming. Learning how to write the heap sort algorithm requires knowledge of two types of data structures - arrays and trees. The initial set of numbers that we want to sort is stored in an array e.g. [10, 3, 76, 34, 23, 32] and after sorting, we get a sorted array [3,10,23,32,34,76]. Heap sort works by visualizing the elements of the array as a special kind of complete binary tree called a heap. Note: As a prerequisite, you must know about a complete binary tree and heap data structure. A complete binary tree has an interesting property that we can use to find the children and parents of any node. If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. Also, the parent of any element at index i is given by the lower bound of (i-1)/2. Let's test it out, Let us also confirm that the rules hold for finding parent of any node Understanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort. Heap is a special tree-based data structure. A binary tree is said to follow a heap data structure if it is a complete binary tree
	All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap it is a complete binary tree All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap The following example diagram shows Max-Heap and Min-Heap. To learn more about it, please visit Heap Data Structure. Starting from a complete binary tree, we can modify it to become a Max-Heap by running a function called heapify on all the non-leaf elements of the heap. Since heapify uses recursion, it can be difficult to grasp. So let's first think about how you would heapify a tree with just three elements. The example above shows two scenarios - one in which the root is the largest element and we don't need to do anything. And another in which the root had a larger element as a child and we needed to swap to maintain max-heap property. If you're worked with recursive algorithms before, you've probably identified that this must be the base case. Now let's think of another scenario in which there is more than one level.  The top element isn't a max-heap but all the sub-trees are max-heaps. To maintain the max-heap property for the entire tree, we will have to keep pushing 2 downwards until it reaches its correct position. Thus, to maintain the max-heap property in a tree where both sub-trees are max-heaps, we need to run heapify on the root element repeatedly until it is larger than its children or it becomes a leaf node. We can combine both these conditions in one heapify function as This function works for both the base case and for a tree of any size. We can thus move the root element to the correct position to maintain the max-heap status for any tree size as long as the sub-trees are max-heaps. To build a max-heap from any tree, we can thus start heapifying each sub-tree from the bottom up and end up with a max-heap after the function is applied to all the elements including the root element. In the case of a complete tree, the first index of a non-leaf node is given by n/2 - 1. All other nodes after that are leaf-nodes and thus don't need to be heapified. So, we can build a maximum heap as As shown in the above diagram, we start by heapifying the lowest smallest trees and gradually move up until we reach the root element. If you've understood everything till here, congratulations, you are on your way to mastering the Heap sort. Since the tree satisfies Max-Heap property, then the largest item is stored at the root node. Swap: Remove the root element and put at the end of the array (nth position) Put the last item of the tree (heap) at the vacant place. Remove: Reduce the size of the heap by 1. Heapify: Heapify the root element again so that we have the highest element at root. The process is repeated until all the items of the list are sorted. The code below shows the operation. Heap Sort has O(nlog n) time complexities for all the cases ( best case, average case, and worst case). Let us understand the reason why. The height of a complete binary tree containing n elements is log n As we have seen earlier, to fully heapify an element whose subtrees are already max-heaps, we need to keep comparing the element with its left and right children and pushing it downwards until it reaches a point where both its children are smaller than it. In the worst case scenario, we will need to move an element from the root to the leaf node making a multiple of log(n) comparisons and swaps. During the build_max_heap stage, we do that for n/2 elements so the worst case complexity of the build_heap step is n/2*log n ~ nlog n. During the sorting step, we exchange the root element with the last element and heapify the root element. For each element, this again takes log n worst time because we might have to bring the element all the way from the root to the leaf. Since we repeat this n times, the heap_sort step is also nlog n. Also since the build_max_heap and heap_sort steps are executed one after another, the algorithmic complexity is not multiplied and it remains in the order of nlog n. Also it performs sorting in O(1) space complexity. Compared with Quick Sort, it has a better worst case ( O(nlog n) ). Quick Sort has complexity O(n^2) for worst case. But in other cases, Quick Sort is fast. Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort. Systems concerned with security and embedded systems such as Linux Kernel use Heap Sort because of the O(n log n) upper bound on Heapsort's running time and constant O(1) upper bound on its auxiliary storage. Although Heap Sort has O(n log n) time complexity even for the worst case, it doesn't have more applications ( compared to other sorting algorithms like Quick Sort, Merge Sort ). However, its underlying data structure, heap, can be efficiently used if we want to extract the smallest (or largest) from the list of items without the overhead of keeping the remaining items in the sorted order. For e.g Priority Queues. Quicksort Merge Sort","// Heap Sort in C++
  
  #include <iostream>
  using namespace std;
  
  void heapify(int arr[], int n, int i) {
    // Find largest among root, left child and right child
    int largest = i;
    int left = 2 * i + 1;
    int right = 2 * i + 2;
  
    if (left < n && arr[left] > arr[largest])
      largest = left;
  
    if (right < n && arr[right] > arr[largest])
      largest = right;
  
    // Swap and continue heapifying if root is not largest
    if (largest != i) {
      swap(arr[i], arr[largest]);
      heapify(arr, n, largest);
    }
  }
  
  // main function to do heap sort
  void heapSort(int arr[], int n) {
    // Build max heap
    for (int i = n / 2 - 1; i >= 0; i--)
      heapify(arr, n, i);
  
    // Heap sort
    for (int i = n - 1; i >= 0; i--) {
      swap(arr[0], arr[i]);
  
      // Heapify root element to get highest element at root again
      heapify(arr, i, 0);
    }
  }
  
  // Print an array
  void printArray(int arr[], int n) {
    for (int i = 0; i < n; ++i)
      cout << arr[i] << "" "";
    cout << ""\n"";
  }
  
  // Driver code
  int main() {
    int arr[] = {1, 12, 9, 5, 6, 10};
    int n = sizeof(arr) / sizeof(arr[0]);
    heapSort(arr, n);
  
    cout << ""Sorted array is \n"";
    printArray(arr, n);
  }"
"Shell Sort (With Code in Python, C++, Java and C)","Shell sort is a generalized version of the insertion sort algorithm. It first sorts elements that are far apart from each other and successively reduces the interval between the elements to be sorted. The interval between the elements is reduced based on the sequence used. Some of the optimal sequences that can be used in the shell sort algorithm are: Shell's original sequence: N/2 , N/4 , …, 1
	Knuth's increments: 1, 4, 13, …, (3k – 1) / 2
	Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1
	Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511…
	Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,...
	Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Shell's original sequence: N/2 , N/4 , …, 1 Knuth's increments: 1, 4, 13, …, (3k – 1) / 2 Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1 Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511… Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,... Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Note: The performance of the shell sort depends on the type of sequence used for a given input array. Suppose, we need to sort the following array.
		
			Initial array We are using the shell's original sequence (N/2, N/4, ...1) as intervals in our algorithm.
		
		In the first loop, if the array size is N = 8 then, the elements lying at the interval of N/2 = 4 are compared and swapped if they are not in order.
		
			The 0th element is compared with the 4th element.
			If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval The 0th element is compared with the 4th element. If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval In the second loop, an interval of N/4 = 8/4 = 2 is taken and again the elements lying at these intervals are sorted.
		
			Rearrange the elements at n/4 interval
		
		
		You might get confused at this point.
		
			All the elements in the array lying at the current interval are compared.
		
		
		The elements at 4th and 2nd position are compared. The elements at 2nd and 0th position are also compared. All the elements in the array lying at the current interval are compared. The same process goes on for remaining elements.
		
			Rearrange all the elements at n/4 interval Finally, when the interval is N/8 = 8/8 =1 then the array elements lying at the interval of 1 are sorted. The array is now completely sorted.
		
			Rearrange the elements at n/8 interval Shell sort is an unstable sorting algorithm because this algorithm does not examine the elements lying in between the intervals. Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between.
	Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array.
	Average Case Complexity: O(n*log n)
		It is around O(n1.25). Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between. Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array. Average Case Complexity: O(n*log n)
		It is around O(n1.25). The complexity depends on the interval chosen. The above complexities differ for different increment sequences chosen. Best increment sequence is unknown.  The space complexity for shell sort is O(1). Shell sort is used when: calling a stack is overhead. uClibc library uses this sort.
	recursion exceeds a limit. bzip2 compressor uses it.
	Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. calling a stack is overhead. uClibc library uses this sort. recursion exceeds a limit. bzip2 compressor uses it. Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. Bubble Sort Quicksort Insertion Sort Selection Sort","# Shell sort in python


def shellSort(array, n):

    # Rearrange elements at each n/2, n/4, n/8, ... intervals
    interval = n // 2
    while interval > 0:
        for i in range(interval, n):
            temp = array[i]
            j = i
            while j >= interval and array[j - interval] > temp:
                array[j] = array[j - interval]
                j -= interval

            array[j] = temp
        interval //= 2


data = [9, 8, 3, 7, 5, 6, 4, 1]
size = len(data)
shellSort(data, size)
print('Sorted Array in Ascending Order:')
print(data)"
"Shell Sort (With Code in Python, C++, Java and C)","Shell sort is a generalized version of the insertion sort algorithm. It first sorts elements that are far apart from each other and successively reduces the interval between the elements to be sorted. The interval between the elements is reduced based on the sequence used. Some of the optimal sequences that can be used in the shell sort algorithm are: Shell's original sequence: N/2 , N/4 , …, 1
	Knuth's increments: 1, 4, 13, …, (3k – 1) / 2
	Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1
	Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511…
	Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,...
	Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Shell's original sequence: N/2 , N/4 , …, 1 Knuth's increments: 1, 4, 13, …, (3k – 1) / 2 Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1 Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511… Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,... Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Note: The performance of the shell sort depends on the type of sequence used for a given input array. Suppose, we need to sort the following array.
		
			Initial array We are using the shell's original sequence (N/2, N/4, ...1) as intervals in our algorithm.
		
		In the first loop, if the array size is N = 8 then, the elements lying at the interval of N/2 = 4 are compared and swapped if they are not in order.
		
			The 0th element is compared with the 4th element.
			If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval The 0th element is compared with the 4th element. If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval In the second loop, an interval of N/4 = 8/4 = 2 is taken and again the elements lying at these intervals are sorted.
		
			Rearrange the elements at n/4 interval
		
		
		You might get confused at this point.
		
			All the elements in the array lying at the current interval are compared.
		
		
		The elements at 4th and 2nd position are compared. The elements at 2nd and 0th position are also compared. All the elements in the array lying at the current interval are compared. The same process goes on for remaining elements.
		
			Rearrange all the elements at n/4 interval Finally, when the interval is N/8 = 8/8 =1 then the array elements lying at the interval of 1 are sorted. The array is now completely sorted.
		
			Rearrange the elements at n/8 interval Shell sort is an unstable sorting algorithm because this algorithm does not examine the elements lying in between the intervals. Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between.
	Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array.
	Average Case Complexity: O(n*log n)
		It is around O(n1.25). Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between. Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array. Average Case Complexity: O(n*log n)
		It is around O(n1.25). The complexity depends on the interval chosen. The above complexities differ for different increment sequences chosen. Best increment sequence is unknown.  The space complexity for shell sort is O(1). Shell sort is used when: calling a stack is overhead. uClibc library uses this sort.
	recursion exceeds a limit. bzip2 compressor uses it.
	Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. calling a stack is overhead. uClibc library uses this sort. recursion exceeds a limit. bzip2 compressor uses it. Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. Bubble Sort Quicksort Insertion Sort Selection Sort","// Shell sort in Java programming

import java.util.Arrays;

// Shell sort
class ShellSort {

  // Rearrange elements at each n/2, n/4, n/8, ... intervals
  void shellSort(int array[], int n) {
  for (int interval = n / 2; interval > 0; interval /= 2) {
    for (int i = interval; i < n; i += 1) {
    int temp = array[i];
    int j;
    for (j = i; j >= interval && array[j - interval] > temp; j -= interval) {
      array[j] = array[j - interval];
    }
    array[j] = temp;
    }
  }
  }

  // Driver code
  public static void main(String args[]) {
  int[] data = { 9, 8, 3, 7, 5, 6, 4, 1 };
  int size = data.length;
  ShellSort ss = new ShellSort();
  ss.shellSort(data, size);
  System.out.println(""Sorted Array in Ascending Order: "");
  System.out.println(Arrays.toString(data));
  }
}"
"Shell Sort (With Code in Python, C++, Java and C)","Shell sort is a generalized version of the insertion sort algorithm. It first sorts elements that are far apart from each other and successively reduces the interval between the elements to be sorted. The interval between the elements is reduced based on the sequence used. Some of the optimal sequences that can be used in the shell sort algorithm are: Shell's original sequence: N/2 , N/4 , …, 1
	Knuth's increments: 1, 4, 13, …, (3k – 1) / 2
	Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1
	Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511…
	Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,...
	Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Shell's original sequence: N/2 , N/4 , …, 1 Knuth's increments: 1, 4, 13, …, (3k – 1) / 2 Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1 Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511… Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,... Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Note: The performance of the shell sort depends on the type of sequence used for a given input array. Suppose, we need to sort the following array.
		
			Initial array We are using the shell's original sequence (N/2, N/4, ...1) as intervals in our algorithm.
		
		In the first loop, if the array size is N = 8 then, the elements lying at the interval of N/2 = 4 are compared and swapped if they are not in order.
		
			The 0th element is compared with the 4th element.
			If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval The 0th element is compared with the 4th element. If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval In the second loop, an interval of N/4 = 8/4 = 2 is taken and again the elements lying at these intervals are sorted.
		
			Rearrange the elements at n/4 interval
		
		
		You might get confused at this point.
		
			All the elements in the array lying at the current interval are compared.
		
		
		The elements at 4th and 2nd position are compared. The elements at 2nd and 0th position are also compared. All the elements in the array lying at the current interval are compared. The same process goes on for remaining elements.
		
			Rearrange all the elements at n/4 interval Finally, when the interval is N/8 = 8/8 =1 then the array elements lying at the interval of 1 are sorted. The array is now completely sorted.
		
			Rearrange the elements at n/8 interval Shell sort is an unstable sorting algorithm because this algorithm does not examine the elements lying in between the intervals. Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between.
	Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array.
	Average Case Complexity: O(n*log n)
		It is around O(n1.25). Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between. Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array. Average Case Complexity: O(n*log n)
		It is around O(n1.25). The complexity depends on the interval chosen. The above complexities differ for different increment sequences chosen. Best increment sequence is unknown.  The space complexity for shell sort is O(1). Shell sort is used when: calling a stack is overhead. uClibc library uses this sort.
	recursion exceeds a limit. bzip2 compressor uses it.
	Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. calling a stack is overhead. uClibc library uses this sort. recursion exceeds a limit. bzip2 compressor uses it. Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. Bubble Sort Quicksort Insertion Sort Selection Sort","// Shell Sort in C programming

#include <stdio.h>

// Shell sort
void shellSort(int array[], int n) {
  // Rearrange elements at each n/2, n/4, n/8, ... intervals
  for (int interval = n / 2; interval > 0; interval /= 2) {
    for (int i = interval; i < n; i += 1) {
      int temp = array[i];
      int j;
      for (j = i; j >= interval && array[j - interval] > temp; j -= interval) {
        array[j] = array[j - interval];
      }
      array[j] = temp;
    }
  }
}

// Print an array
void printArray(int array[], int size) {
  for (int i = 0; i < size; ++i) {
    printf(""%d  "", array[i]);
  }
  printf(""\n"");
}

// Driver code
int main() {
  int data[] = {9, 8, 3, 7, 5, 6, 4, 1};
  int size = sizeof(data) / sizeof(data[0]);
  shellSort(data, size);
  printf(""Sorted array: \n"");
  printArray(data, size);
}"
"Shell Sort (With Code in Python, C++, Java and C)","Shell sort is a generalized version of the insertion sort algorithm. It first sorts elements that are far apart from each other and successively reduces the interval between the elements to be sorted. The interval between the elements is reduced based on the sequence used. Some of the optimal sequences that can be used in the shell sort algorithm are: Shell's original sequence: N/2 , N/4 , …, 1
	Knuth's increments: 1, 4, 13, …, (3k – 1) / 2
	Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1
	Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511…
	Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,...
	Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Shell's original sequence: N/2 , N/4 , …, 1 Knuth's increments: 1, 4, 13, …, (3k – 1) / 2 Sedgewick's increments: 1, 8, 23, 77, 281, 1073, 4193, 16577...4j+1+ 3·2j+ 1 Hibbard's increments: 1, 3, 7, 15, 31, 63, 127, 255, 511… Papernov & Stasevich increment: 1, 3, 5, 9, 17, 33, 65,... Pratt: 1, 2, 3, 4, 6, 9, 8, 12, 18, 27, 16, 24, 36, 54, 81.... Note: The performance of the shell sort depends on the type of sequence used for a given input array. Suppose, we need to sort the following array.
		
			Initial array We are using the shell's original sequence (N/2, N/4, ...1) as intervals in our algorithm.
		
		In the first loop, if the array size is N = 8 then, the elements lying at the interval of N/2 = 4 are compared and swapped if they are not in order.
		
			The 0th element is compared with the 4th element.
			If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval The 0th element is compared with the 4th element. If the 0th element is greater than the 4th one then, the 4th element is first stored in temp variable and the 0th element (ie. greater element) is stored in the 4th position and the element stored in temp is stored in the 0th position.
				
					Rearrange the elements at n/2 interval
				
				
				This process goes on for all the remaining elements.
				
					Rearrange all the elements at n/2 interval In the second loop, an interval of N/4 = 8/4 = 2 is taken and again the elements lying at these intervals are sorted.
		
			Rearrange the elements at n/4 interval
		
		
		You might get confused at this point.
		
			All the elements in the array lying at the current interval are compared.
		
		
		The elements at 4th and 2nd position are compared. The elements at 2nd and 0th position are also compared. All the elements in the array lying at the current interval are compared. The same process goes on for remaining elements.
		
			Rearrange all the elements at n/4 interval Finally, when the interval is N/8 = 8/8 =1 then the array elements lying at the interval of 1 are sorted. The array is now completely sorted.
		
			Rearrange the elements at n/8 interval Shell sort is an unstable sorting algorithm because this algorithm does not examine the elements lying in between the intervals. Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between.
	Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array.
	Average Case Complexity: O(n*log n)
		It is around O(n1.25). Worst Case Complexity: less than or equal to O(n2)
		Worst case complexity for shell sort is always less than or equal to O(n2).
		
		According to Poonen Theorem, worst case complexity for shell sort is Θ(Nlog N)2/(log log N)2) or Θ(Nlog N)2/log log N) or Θ(N(log N)2) or something in between. Best Case Complexity: O(n*log n)
		When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array. Average Case Complexity: O(n*log n)
		It is around O(n1.25). The complexity depends on the interval chosen. The above complexities differ for different increment sequences chosen. Best increment sequence is unknown.  The space complexity for shell sort is O(1). Shell sort is used when: calling a stack is overhead. uClibc library uses this sort.
	recursion exceeds a limit. bzip2 compressor uses it.
	Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. calling a stack is overhead. uClibc library uses this sort. recursion exceeds a limit. bzip2 compressor uses it. Insertion sort does not perform well when the close elements are far apart. Shell sort helps in reducing the distance between the close elements. Thus, there will be less number of swappings to be performed. Bubble Sort Quicksort Insertion Sort Selection Sort","// Shell Sort in C++ programming

#include <iostream>
using namespace std;

// Shell sort
void shellSort(int array[], int n) {
  // Rearrange elements at each n/2, n/4, n/8, ... intervals
  for (int interval = n / 2; interval > 0; interval /= 2) {
    for (int i = interval; i < n; i += 1) {
      int temp = array[i];
      int j;
      for (j = i; j >= interval && array[j - interval] > temp; j -= interval) {
        array[j] = array[j - interval];
      }
      array[j] = temp;
    }
  }
}

// Print an array
void printArray(int array[], int size) {
  int i;
  for (i = 0; i < size; i++)
    cout << array[i] << "" "";
  cout << endl;
}

// Driver code
int main() {
  int data[] = {9, 8, 3, 7, 5, 6, 4, 1};
  int size = sizeof(data) / sizeof(data[0]);
  shellSort(data, size);
  cout << ""Sorted array: \n"";
  printArray(data, size);
}"
Linear Search (With Code),"Linear search is a sequential searching algorithm where we start from one end and check every element of the list until the desired element is found. It is the simplest searching algorithm. The following steps are followed to search for an element k = 1 in the list below. Start from the first element, compare k with each element x.

		
			Compare with each element If x == k, return the index.
		
			Element found Else, return not found. Time Complexity: O(n)  Space Complexity: O(1) For searching operations in smaller arrays (<100 items).","# Linear Search in Python


def linearSearch(array, n, x):

    # Going through array sequencially
    for i in range(0, n):
        if (array[i] == x):
            return i
    return -1


array = [2, 4, 0, 1, 9]
x = 1
n = len(array)
result = linearSearch(array, n, x)
if(result == -1):
    print(""Element not found"")
else:
    print(""Element found at index: "", result)"
Linear Search (With Code),"Linear search is a sequential searching algorithm where we start from one end and check every element of the list until the desired element is found. It is the simplest searching algorithm. The following steps are followed to search for an element k = 1 in the list below. Start from the first element, compare k with each element x.

		
			Compare with each element If x == k, return the index.
		
			Element found Else, return not found. Time Complexity: O(n)  Space Complexity: O(1) For searching operations in smaller arrays (<100 items).","// Linear Search in Java

class LinearSearch {
  public static int linearSearch(int array[], int x) {
  int n = array.length;

  // Going through array sequencially
  for (int i = 0; i < n; i++) {
    if (array[i] == x)
    return i;
  }
  return -1;
  }

  public static void main(String args[]) {
  int array[] = { 2, 4, 0, 1, 9 };
  int x = 1;

  int result = linearSearch(array, x);

  if (result == -1)
    System.out.print(""Element not found"");
  else
    System.out.print(""Element found at index: "" + result);
  }
}"
Linear Search (With Code),"Linear search is a sequential searching algorithm where we start from one end and check every element of the list until the desired element is found. It is the simplest searching algorithm. The following steps are followed to search for an element k = 1 in the list below. Start from the first element, compare k with each element x.

		
			Compare with each element If x == k, return the index.
		
			Element found Else, return not found. Time Complexity: O(n)  Space Complexity: O(1) For searching operations in smaller arrays (<100 items).","// Linear Search in C

#include <stdio.h>

int search(int array[], int n, int x) {
  
  // Going through array sequencially
  for (int i = 0; i < n; i++)
    if (array[i] == x)
      return i;
  return -1;
}

int main() {
  int array[] = {2, 4, 0, 1, 9};
  int x = 1;
  int n = sizeof(array) / sizeof(array[0]);

  int result = search(array, n, x);

  (result == -1) ? printf(""Element not found"") : printf(""Element found at index: %d"", result);
}"
Linear Search (With Code),"Linear search is a sequential searching algorithm where we start from one end and check every element of the list until the desired element is found. It is the simplest searching algorithm. The following steps are followed to search for an element k = 1 in the list below. Start from the first element, compare k with each element x.

		
			Compare with each element If x == k, return the index.
		
			Element found Else, return not found. Time Complexity: O(n)  Space Complexity: O(1) For searching operations in smaller arrays (<100 items).","// Linear Search in C++

#include <iostream>
using namespace std;

int search(int array[], int n, int x) {

  // Going through array sequencially
  for (int i = 0; i < n; i++)
    if (array[i] == x)
      return i;
  return -1;
}

int main() {
  int array[] = {2, 4, 0, 1, 9};
  int x = 1;
  int n = sizeof(array) / sizeof(array[0]);

  int result = search(array, n, x);

  (result == -1) ? cout << ""Element not found"" : cout << ""Element found at index: "" << result;
}"
Binary Search (With Code),"Binary Search is a searching algorithm for finding an element's position in a sorted array. In this approach, the element is always searched in the middle of a portion of an array. Binary search can be implemented only on a sorted list of items. If the elements are not sorted already, we need to sort them first. Binary Search Algorithm can be implemented in two ways which are discussed below. Iterative Method Recursive Method The recursive method follows the divide and conquer approach. The general steps for both methods are discussed below. The array in which searching is to be performed is:
		
			Initial array
		
		
		Let x = 4 be the element to be searched. Set two pointers low and high at the lowest and the highest positions respectively.
		
			Setting pointers Find the middle element mid of the array ie. arr[(low + high)/2] = 6.
		
			Mid element If x == mid, then return mid.Else, compare the element to be searched with m. If x > mid, compare x with the middle element of the elements on the right side of mid. This is done by setting low to low = mid + 1. Else, compare x with the middle element of the elements on the left side of mid. This is done by setting high to high = mid - 1.
		
			Finding mid element Repeat steps 3 to 6 until low meets high.
		
			Mid element x = 4 is found.
		
			Found  Time Complexities Best case complexity: O(1)
	Average case complexity: O(log n)
	Worst case complexity: O(log n) Best case complexity: O(1) Average case complexity: O(log n) Worst case complexity: O(log n) Space Complexity The space complexity of the binary search is O(1). In libraries of Java, .Net, C++ STL
	While debugging, the binary search is used to pinpoint the place where the error happens. In libraries of Java, .Net, C++ STL While debugging, the binary search is used to pinpoint the place where the error happens.","# Binary Search in python


def binarySearch(array, x, low, high):

    # Repeat until the pointers low and high meet each other
    while low <= high:

        mid = low + (high - low)//2

        if array[mid] == x:
            return mid

        elif array[mid] < x:
            low = mid + 1

        else:
            high = mid - 1

    return -1


array = [3, 4, 5, 6, 7, 8, 9]
x = 4

result = binarySearch(array, x, 0, len(array)-1)

if result != -1:
    print(""Element is present at index "" + str(result))
else:
    print(""Not found"")"
Binary Search (With Code),"Binary Search is a searching algorithm for finding an element's position in a sorted array. In this approach, the element is always searched in the middle of a portion of an array. Binary search can be implemented only on a sorted list of items. If the elements are not sorted already, we need to sort them first. Binary Search Algorithm can be implemented in two ways which are discussed below. Iterative Method Recursive Method The recursive method follows the divide and conquer approach. The general steps for both methods are discussed below. The array in which searching is to be performed is:
		
			Initial array
		
		
		Let x = 4 be the element to be searched. Set two pointers low and high at the lowest and the highest positions respectively.
		
			Setting pointers Find the middle element mid of the array ie. arr[(low + high)/2] = 6.
		
			Mid element If x == mid, then return mid.Else, compare the element to be searched with m. If x > mid, compare x with the middle element of the elements on the right side of mid. This is done by setting low to low = mid + 1. Else, compare x with the middle element of the elements on the left side of mid. This is done by setting high to high = mid - 1.
		
			Finding mid element Repeat steps 3 to 6 until low meets high.
		
			Mid element x = 4 is found.
		
			Found  Time Complexities Best case complexity: O(1)
	Average case complexity: O(log n)
	Worst case complexity: O(log n) Best case complexity: O(1) Average case complexity: O(log n) Worst case complexity: O(log n) Space Complexity The space complexity of the binary search is O(1). In libraries of Java, .Net, C++ STL
	While debugging, the binary search is used to pinpoint the place where the error happens. In libraries of Java, .Net, C++ STL While debugging, the binary search is used to pinpoint the place where the error happens.","// Binary Search in Java

class BinarySearch {
  int binarySearch(int array[], int x, int low, int high) {

    // Repeat until the pointers low and high meet each other
    while (low <= high) {
      int mid = low + (high - low) / 2;

      if (array[mid] == x)
        return mid;

      if (array[mid] < x)
        low = mid + 1;

      else
        high = mid - 1;
    }

    return -1;
  }

  public static void main(String args[]) {
    BinarySearch ob = new BinarySearch();
    int array[] = { 3, 4, 5, 6, 7, 8, 9 };
    int n = array.length;
    int x = 4;
    int result = ob.binarySearch(array, x, 0, n - 1);
    if (result == -1)
      System.out.println(""Not found"");
    else
      System.out.println(""Element found at index "" + result);
  }
}"
Binary Search (With Code),"Binary Search is a searching algorithm for finding an element's position in a sorted array. In this approach, the element is always searched in the middle of a portion of an array. Binary search can be implemented only on a sorted list of items. If the elements are not sorted already, we need to sort them first. Binary Search Algorithm can be implemented in two ways which are discussed below. Iterative Method Recursive Method The recursive method follows the divide and conquer approach. The general steps for both methods are discussed below. The array in which searching is to be performed is:
		
			Initial array
		
		
		Let x = 4 be the element to be searched. Set two pointers low and high at the lowest and the highest positions respectively.
		
			Setting pointers Find the middle element mid of the array ie. arr[(low + high)/2] = 6.
		
			Mid element If x == mid, then return mid.Else, compare the element to be searched with m. If x > mid, compare x with the middle element of the elements on the right side of mid. This is done by setting low to low = mid + 1. Else, compare x with the middle element of the elements on the left side of mid. This is done by setting high to high = mid - 1.
		
			Finding mid element Repeat steps 3 to 6 until low meets high.
		
			Mid element x = 4 is found.
		
			Found  Time Complexities Best case complexity: O(1)
	Average case complexity: O(log n)
	Worst case complexity: O(log n) Best case complexity: O(1) Average case complexity: O(log n) Worst case complexity: O(log n) Space Complexity The space complexity of the binary search is O(1). In libraries of Java, .Net, C++ STL
	While debugging, the binary search is used to pinpoint the place where the error happens. In libraries of Java, .Net, C++ STL While debugging, the binary search is used to pinpoint the place where the error happens.","// Binary Search in C

#include <stdio.h>

int binarySearch(int array[], int x, int low, int high) {
  // Repeat until the pointers low and high meet each other
  while (low <= high) {
    int mid = low + (high - low) / 2;

    if (array[mid] == x)
      return mid;

    if (array[mid] < x)
      low = mid + 1;

    else
      high = mid - 1;
  }

  return -1;
}

int main(void) {
  int array[] = {3, 4, 5, 6, 7, 8, 9};
  int n = sizeof(array) / sizeof(array[0]);
  int x = 4;
  int result = binarySearch(array, x, 0, n - 1);
  if (result == -1)
    printf(""Not found"");
  else
    printf(""Element is found at index %d"", result);
  return 0;
}"
Binary Search (With Code),"Binary Search is a searching algorithm for finding an element's position in a sorted array. In this approach, the element is always searched in the middle of a portion of an array. Binary search can be implemented only on a sorted list of items. If the elements are not sorted already, we need to sort them first. Binary Search Algorithm can be implemented in two ways which are discussed below. Iterative Method Recursive Method The recursive method follows the divide and conquer approach. The general steps for both methods are discussed below. The array in which searching is to be performed is:
		
			Initial array
		
		
		Let x = 4 be the element to be searched. Set two pointers low and high at the lowest and the highest positions respectively.
		
			Setting pointers Find the middle element mid of the array ie. arr[(low + high)/2] = 6.
		
			Mid element If x == mid, then return mid.Else, compare the element to be searched with m. If x > mid, compare x with the middle element of the elements on the right side of mid. This is done by setting low to low = mid + 1. Else, compare x with the middle element of the elements on the left side of mid. This is done by setting high to high = mid - 1.
		
			Finding mid element Repeat steps 3 to 6 until low meets high.
		
			Mid element x = 4 is found.
		
			Found  Time Complexities Best case complexity: O(1)
	Average case complexity: O(log n)
	Worst case complexity: O(log n) Best case complexity: O(1) Average case complexity: O(log n) Worst case complexity: O(log n) Space Complexity The space complexity of the binary search is O(1). In libraries of Java, .Net, C++ STL
	While debugging, the binary search is used to pinpoint the place where the error happens. In libraries of Java, .Net, C++ STL While debugging, the binary search is used to pinpoint the place where the error happens.","// Binary Search in C++

#include <iostream>
using namespace std;

int binarySearch(int array[], int x, int low, int high) {
  
	// Repeat until the pointers low and high meet each other
  while (low <= high) {
    int mid = low + (high - low) / 2;

    if (array[mid] == x)
      return mid;

    if (array[mid] < x)
      low = mid + 1;

    else
      high = mid - 1;
  }

  return -1;
}

int main(void) {
  int array[] = {3, 4, 5, 6, 7, 8, 9};
  int x = 4;
  int n = sizeof(array) / sizeof(array[0]);
  int result = binarySearch(array, x, 0, n - 1);
  if (result == -1)
    printf(""Not found"");
  else
    printf(""Element is found at index %d"", result);
}"
Ford-Fulkerson algorithm,"Ford-Fulkerson algorithm is a greedy approach for calculating the maximum possible flow in a network or a graph. A term, flow network, is used to describe a network of vertices and edges with a source (S) and a sink (T). Each vertex, except S and T, can receive and send an equal amount of stuff through it. S can only send and T can only receive stuff. We can visualize the understanding of the algorithm using a flow of liquid inside a network of pipes of different capacities. Each pipe has a certain capacity of liquid it can transfer at an instance. For this algorithm, we are going to find how much liquid can be flowed from the source to the sink at an instance using the network. It is the path available in a flow network. It represents the flow network that has additional possible flow. It is the capacity of the edge after subtracting the flow from the maximum capacity.  The algorithm follows: Initialize the flow in all the edges to 0. While there is an augmenting path between the source and the sink, add this path to the flow. Update the residual graph. We can also consider reverse-path if required because if we do not consider them, we may never find a maximum flow. The above concepts can be understood with the example below. The flow of all the edges is 0 at the beginning. Select any arbitrary path from S to T. In this step, we have selected path S-A-B-T.

		
			Find a path
		
		
		The minimum capacity among the three edges is 2 (B-T). Based on this, update the flow/capacity for each path.

		
			Update the capacities Select another path S-D-C-T. The minimum capacity among these edges is 3 (S-D).
		
			Find next path
		
		
		Update the capacities according to this.
		
			Update the capacities Now, let us consider the reverse-path B-D as well. Selecting path S-A-B-D-C-T. The minimum residual capacity among the edges is 1 (D-C).
		
			Find next path
		
		
		Updating the capacities.
		
			Update the capacities
		
		
		The capacity for forward and reverse paths are considered separately. Adding all the flows = 2 + 3 + 1 = 6, which is the maximum possible flow on the flow network. Note that if the capacity for any edge is full, then that path cannot be used. Water distribution pipeline
	Bipartite matching problem
	Circulation with demands Water distribution pipeline Bipartite matching problem Circulation with demands","# Ford-Fulkerson algorith in Python

from collections import defaultdict


class Graph:

    def __init__(self, graph):
        self.graph = graph
        self. ROW = len(graph)


    # Using BFS as a searching algorithm 
    def searching_algo_BFS(self, s, t, parent):

        visited = [False] * (self.ROW)
        queue = []

        queue.append(s)
        visited[s] = True

        while queue:

            u = queue.pop(0)

            for ind, val in enumerate(self.graph[u]):
                if visited[ind] == False and val > 0:
                    queue.append(ind)
                    visited[ind] = True
                    parent[ind] = u

        return True if visited[t] else False

    # Applying fordfulkerson algorithm
    def ford_fulkerson(self, source, sink):
        parent = [-1] * (self.ROW)
        max_flow = 0

        while self.searching_algo_BFS(source, sink, parent):

            path_flow = float(""Inf"")
            s = sink
            while(s != source):
                path_flow = min(path_flow, self.graph[parent[s]][s])
                s = parent[s]

            # Adding the path flows
            max_flow += path_flow

            # Updating the residual values of edges
            v = sink
            while(v != source):
                u = parent[v]
                self.graph[u][v] -= path_flow
                self.graph[v][u] += path_flow
                v = parent[v]

        return max_flow


graph = [[0, 8, 0, 0, 3, 0],
         [0, 0, 9, 0, 0, 0],
         [0, 0, 0, 0, 7, 2],
         [0, 0, 0, 0, 0, 5],
         [0, 0, 7, 4, 0, 0],
         [0, 0, 0, 0, 0, 0]]

g = Graph(graph)

source = 0
sink = 5

print(""Max Flow: %d "" % g.ford_fulkerson(source, sink))"
Ford-Fulkerson algorithm,"Ford-Fulkerson algorithm is a greedy approach for calculating the maximum possible flow in a network or a graph. A term, flow network, is used to describe a network of vertices and edges with a source (S) and a sink (T). Each vertex, except S and T, can receive and send an equal amount of stuff through it. S can only send and T can only receive stuff. We can visualize the understanding of the algorithm using a flow of liquid inside a network of pipes of different capacities. Each pipe has a certain capacity of liquid it can transfer at an instance. For this algorithm, we are going to find how much liquid can be flowed from the source to the sink at an instance using the network. It is the path available in a flow network. It represents the flow network that has additional possible flow. It is the capacity of the edge after subtracting the flow from the maximum capacity.  The algorithm follows: Initialize the flow in all the edges to 0. While there is an augmenting path between the source and the sink, add this path to the flow. Update the residual graph. We can also consider reverse-path if required because if we do not consider them, we may never find a maximum flow. The above concepts can be understood with the example below. The flow of all the edges is 0 at the beginning. Select any arbitrary path from S to T. In this step, we have selected path S-A-B-T.

		
			Find a path
		
		
		The minimum capacity among the three edges is 2 (B-T). Based on this, update the flow/capacity for each path.

		
			Update the capacities Select another path S-D-C-T. The minimum capacity among these edges is 3 (S-D).
		
			Find next path
		
		
		Update the capacities according to this.
		
			Update the capacities Now, let us consider the reverse-path B-D as well. Selecting path S-A-B-D-C-T. The minimum residual capacity among the edges is 1 (D-C).
		
			Find next path
		
		
		Updating the capacities.
		
			Update the capacities
		
		
		The capacity for forward and reverse paths are considered separately. Adding all the flows = 2 + 3 + 1 = 6, which is the maximum possible flow on the flow network. Note that if the capacity for any edge is full, then that path cannot be used. Water distribution pipeline
	Bipartite matching problem
	Circulation with demands Water distribution pipeline Bipartite matching problem Circulation with demands","// Ford-Fulkerson algorith in Java

import java.util.LinkedList;

class FordFulkerson {
  static final int V = 6;

  // Using BFS as a searching algorithm 
  boolean bfs(int Graph[][], int s, int t, int p[]) {
    boolean visited[] = new boolean[V];
    for (int i = 0; i < V; ++i)
      visited[i] = false;

    LinkedList<Integer> queue = new LinkedList<Integer>();
    queue.add(s);
    visited[s] = true;
    p[s] = -1;

    while (queue.size() != 0) {
      int u = queue.poll();

      for (int v = 0; v < V; v++) {
        if (visited[v] == false && Graph[u][v] > 0) {
          queue.add(v);
          p[v] = u;
          visited[v] = true;
        }
      }
    }

    return (visited[t] == true);
  }

  // Applying fordfulkerson algorithm
  int fordFulkerson(int graph[][], int s, int t) {
    int u, v;
    int Graph[][] = new int[V][V];

    for (u = 0; u < V; u++)
      for (v = 0; v < V; v++)
        Graph[u][v] = graph[u][v];

    int p[] = new int[V];

    int max_flow = 0;

    # Updating the residual calues of edges
    while (bfs(Graph, s, t, p)) {
      int path_flow = Integer.MAX_VALUE;
      for (v = t; v != s; v = p[v]) {
        u = p[v];
        path_flow = Math.min(path_flow, Graph[u][v]);
      }

      for (v = t; v != s; v = p[v]) {
        u = p[v];
        Graph[u][v] -= path_flow;
        Graph[v][u] += path_flow;
      }

      // Adding the path flows
      max_flow += path_flow;
    }

    return max_flow;
  }

  public static void main(String[] args) throws java.lang.Exception {
    int graph[][] = new int[][] { { 0, 8, 0, 0, 3, 0 }, { 0, 0, 9, 0, 0, 0 }, { 0, 0, 0, 0, 7, 2 },
        { 0, 0, 0, 0, 0, 5 }, { 0, 0, 7, 4, 0, 0 }, { 0, 0, 0, 0, 0, 0 } };
    FordFulkerson m = new FordFulkerson();

    System.out.println(""Max Flow: "" + m.fordFulkerson(graph, 0, 5));

  }
}"
Ford-Fulkerson algorithm,"Ford-Fulkerson algorithm is a greedy approach for calculating the maximum possible flow in a network or a graph. A term, flow network, is used to describe a network of vertices and edges with a source (S) and a sink (T). Each vertex, except S and T, can receive and send an equal amount of stuff through it. S can only send and T can only receive stuff. We can visualize the understanding of the algorithm using a flow of liquid inside a network of pipes of different capacities. Each pipe has a certain capacity of liquid it can transfer at an instance. For this algorithm, we are going to find how much liquid can be flowed from the source to the sink at an instance using the network. It is the path available in a flow network. It represents the flow network that has additional possible flow. It is the capacity of the edge after subtracting the flow from the maximum capacity.  The algorithm follows: Initialize the flow in all the edges to 0. While there is an augmenting path between the source and the sink, add this path to the flow. Update the residual graph. We can also consider reverse-path if required because if we do not consider them, we may never find a maximum flow. The above concepts can be understood with the example below. The flow of all the edges is 0 at the beginning. Select any arbitrary path from S to T. In this step, we have selected path S-A-B-T.

		
			Find a path
		
		
		The minimum capacity among the three edges is 2 (B-T). Based on this, update the flow/capacity for each path.

		
			Update the capacities Select another path S-D-C-T. The minimum capacity among these edges is 3 (S-D).
		
			Find next path
		
		
		Update the capacities according to this.
		
			Update the capacities Now, let us consider the reverse-path B-D as well. Selecting path S-A-B-D-C-T. The minimum residual capacity among the edges is 1 (D-C).
		
			Find next path
		
		
		Updating the capacities.
		
			Update the capacities
		
		
		The capacity for forward and reverse paths are considered separately. Adding all the flows = 2 + 3 + 1 = 6, which is the maximum possible flow on the flow network. Note that if the capacity for any edge is full, then that path cannot be used. Water distribution pipeline
	Bipartite matching problem
	Circulation with demands Water distribution pipeline Bipartite matching problem Circulation with demands","/ Ford - Fulkerson algorith in C

#include <stdio.h>

#define A 0
#define B 1
#define C 2
#define MAX_NODES 1000
#define O 1000000000

int n;
int e;
int capacity[MAX_NODES][MAX_NODES];
int flow[MAX_NODES][MAX_NODES];
int color[MAX_NODES];
int pred[MAX_NODES];

int min(int x, int y) {
  return x < y ? x : y;
}

int head, tail;
int q[MAX_NODES + 2];

void enqueue(int x) {
  q[tail] = x;
  tail++;
  color[x] = B;
}

int dequeue() {
  int x = q[head];
  head++;
  color[x] = C;
  return x;
}

// Using BFS as a searching algorithm
int bfs(int start, int target) {
  int u, v;
  for (u = 0; u < n; u++) {
    color[u] = A;
  }
  head = tail = 0;
  enqueue(start);
  pred[start] = -1;
  while (head != tail) {
    u = dequeue();
    for (v = 0; v < n; v++) {
      if (color[v] == A && capacity[u][v] - flow[u][v] > 0) {
        enqueue(v);
        pred[v] = u;
      }
    }
  }
  return color[target] == C;
}

// Applying fordfulkerson algorithm
int fordFulkerson(int source, int sink) {
  int i, j, u;
  int max_flow = 0;
  for (i = 0; i < n; i++) {
    for (j = 0; j < n; j++) {
      flow[i][j] = 0;
    }
  }

  // Updating the residual values of edges
  while (bfs(source, sink)) {
    int increment = O;
    for (u = n - 1; pred[u] >= 0; u = pred[u]) {
      increment = min(increment, capacity[pred[u]][u] - flow[pred[u]][u]);
    }
    for (u = n - 1; pred[u] >= 0; u = pred[u]) {
      flow[pred[u]][u] += increment;
      flow[u][pred[u]] -= increment;
    }
    // Adding the path flows
    max_flow += increment;
  }
  return max_flow;
}

int main() {
  for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
      capacity[i][j] = 0;
    }
  }
  n = 6;
  e = 7;

  capacity[0][1] = 8;
  capacity[0][4] = 3;
  capacity[1][2] = 9;
  capacity[2][4] = 7;
  capacity[2][5] = 2;
  capacity[3][5] = 5;
  capacity[4][2] = 7;
  capacity[4][3] = 4;

  int s = 0, t = 5;
  printf(""Max Flow: %d\n"", fordFulkerson(s, t));
}"
Ford-Fulkerson algorithm,"Ford-Fulkerson algorithm is a greedy approach for calculating the maximum possible flow in a network or a graph. A term, flow network, is used to describe a network of vertices and edges with a source (S) and a sink (T). Each vertex, except S and T, can receive and send an equal amount of stuff through it. S can only send and T can only receive stuff. We can visualize the understanding of the algorithm using a flow of liquid inside a network of pipes of different capacities. Each pipe has a certain capacity of liquid it can transfer at an instance. For this algorithm, we are going to find how much liquid can be flowed from the source to the sink at an instance using the network. It is the path available in a flow network. It represents the flow network that has additional possible flow. It is the capacity of the edge after subtracting the flow from the maximum capacity.  The algorithm follows: Initialize the flow in all the edges to 0. While there is an augmenting path between the source and the sink, add this path to the flow. Update the residual graph. We can also consider reverse-path if required because if we do not consider them, we may never find a maximum flow. The above concepts can be understood with the example below. The flow of all the edges is 0 at the beginning. Select any arbitrary path from S to T. In this step, we have selected path S-A-B-T.

		
			Find a path
		
		
		The minimum capacity among the three edges is 2 (B-T). Based on this, update the flow/capacity for each path.

		
			Update the capacities Select another path S-D-C-T. The minimum capacity among these edges is 3 (S-D).
		
			Find next path
		
		
		Update the capacities according to this.
		
			Update the capacities Now, let us consider the reverse-path B-D as well. Selecting path S-A-B-D-C-T. The minimum residual capacity among the edges is 1 (D-C).
		
			Find next path
		
		
		Updating the capacities.
		
			Update the capacities
		
		
		The capacity for forward and reverse paths are considered separately. Adding all the flows = 2 + 3 + 1 = 6, which is the maximum possible flow on the flow network. Note that if the capacity for any edge is full, then that path cannot be used. Water distribution pipeline
	Bipartite matching problem
	Circulation with demands Water distribution pipeline Bipartite matching problem Circulation with demands","// Ford-Fulkerson algorith in C++

#include <limits.h>
#include <string.h>

#include <iostream>
#include <queue>
using namespace std;

#define V 6

// Using BFS as a searching algorithm
bool bfs(int rGraph[V][V], int s, int t, int parent[]) {
  bool visited[V];
  memset(visited, 0, sizeof(visited));

  queue<int> q;
  q.push(s);
  visited[s] = true;
  parent[s] = -1;

  while (!q.empty()) {
    int u = q.front();
    q.pop();

    for (int v = 0; v < V; v++) {
      if (visited[v] == false && rGraph[u][v] > 0) {
        q.push(v);
        parent[v] = u;
        visited[v] = true;
      }
    }
  }

  return (visited[t] == true);
}

// Applying fordfulkerson algorithm
int fordFulkerson(int graph[V][V], int s, int t) {
  int u, v;

  int rGraph[V][V];
  for (u = 0; u < V; u++)
    for (v = 0; v < V; v++)
      rGraph[u][v] = graph[u][v];

  int parent[V];
  int max_flow = 0;

  // Updating the residual values of edges
  while (bfs(rGraph, s, t, parent)) {
    int path_flow = INT_MAX;
    for (v = t; v != s; v = parent[v]) {
      u = parent[v];
      path_flow = min(path_flow, rGraph[u][v]);
    }

    for (v = t; v != s; v = parent[v]) {
      u = parent[v];
      rGraph[u][v] -= path_flow;
      rGraph[v][u] += path_flow;
    }

    // Adding the path flows
    max_flow += path_flow;
  }

  return max_flow;
}

int main() {
  int graph[V][V] = {{0, 8, 0, 0, 3, 0},
             {0, 0, 9, 0, 0, 0},
             {0, 0, 0, 0, 7, 2},
             {0, 0, 0, 0, 0, 5},
             {0, 0, 7, 4, 0, 0},
             {0, 0, 0, 0, 0, 0}};

  cout << ""Max Flow: "" << fordFulkerson(graph, 0, 5) << endl;
}"
Dijkstra's Algorithm,"It differs from the minimum spanning tree because the shortest distance between two vertices might not include all the vertices of the graph. Dijkstra's Algorithm works on the basis that any subpath B -> D of the shortest path A -> D between vertices A and D is also the shortest path between vertices B and D. Djikstra used this property in the opposite direction i.e we overestimate the distance of each vertex from the starting vertex. Then we visit each node and its neighbors to find the shortest subpath to those neighbors. The algorithm uses a greedy approach in the sense that we find the next best solution hoping that the end result is the best solution for the whole problem. It is easier to start with an example and then think about the algorithm. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length.  Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. A minimum priority queue can be used to efficiently receive the vertex with least path distance. The implementation of Dijkstra's Algorithm in C++ is given below. The complexity of the code can be improved, but the abstractions are convenient to relate the code with the algorithm. Time Complexity: O(E Log V) where, E is the number of edges and V is the number of vertices. Space Complexity: O(V) To find the shortest path
	In social networking applications
	In a telephone network
	To find the locations in the map To find the shortest path In social networking applications In a telephone network To find the locations in the map","# Dijkstra's Algorithm in Python


import sys

# Providing the graph
vertices = [[0, 0, 1, 1, 0, 0, 0],
            [0, 0, 1, 0, 0, 1, 0],
            [1, 1, 0, 1, 1, 0, 0],
            [1, 0, 1, 0, 0, 0, 1],
            [0, 0, 1, 0, 0, 1, 0],
            [0, 1, 0, 0, 1, 0, 1],
            [0, 0, 0, 1, 0, 1, 0]]

edges = [[0, 0, 1, 2, 0, 0, 0],
         [0, 0, 2, 0, 0, 3, 0],
         [1, 2, 0, 1, 3, 0, 0],
         [2, 0, 1, 0, 0, 0, 1],
         [0, 0, 3, 0, 0, 2, 0],
         [0, 3, 0, 0, 2, 0, 1],
         [0, 0, 0, 1, 0, 1, 0]]

# Find which vertex is to be visited next
def to_be_visited():
    global visited_and_distance
    v = -10
    for index in range(num_of_vertices):
        if visited_and_distance[index][0] == 0 \
            and (v < 0 or visited_and_distance[index][1] <=
                 visited_and_distance[v][1]):
            v = index
    return v


num_of_vertices = len(vertices[0])

visited_and_distance = [[0, 0]]
for i in range(num_of_vertices-1):
    visited_and_distance.append([0, sys.maxsize])

for vertex in range(num_of_vertices):

    # Find next vertex to be visited
    to_visit = to_be_visited()
    for neighbor_index in range(num_of_vertices):

        # Updating new distances
        if vertices[to_visit][neighbor_index] == 1 and \
                visited_and_distance[neighbor_index][0] == 0:
            new_distance = visited_and_distance[to_visit][1] \
                + edges[to_visit][neighbor_index]
            if visited_and_distance[neighbor_index][1] > new_distance:
                visited_and_distance[neighbor_index][1] = new_distance
        
        visited_and_distance[to_visit][0] = 1

i = 0

# Printing the distance
for distance in visited_and_distance:
    print(""Distance of "", chr(ord('a') + i),
          "" from source vertex: "", distance[1])
    i = i + 1"
Dijkstra's Algorithm,"It differs from the minimum spanning tree because the shortest distance between two vertices might not include all the vertices of the graph. Dijkstra's Algorithm works on the basis that any subpath B -> D of the shortest path A -> D between vertices A and D is also the shortest path between vertices B and D. Djikstra used this property in the opposite direction i.e we overestimate the distance of each vertex from the starting vertex. Then we visit each node and its neighbors to find the shortest subpath to those neighbors. The algorithm uses a greedy approach in the sense that we find the next best solution hoping that the end result is the best solution for the whole problem. It is easier to start with an example and then think about the algorithm. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length.  Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. A minimum priority queue can be used to efficiently receive the vertex with least path distance. The implementation of Dijkstra's Algorithm in C++ is given below. The complexity of the code can be improved, but the abstractions are convenient to relate the code with the algorithm. Time Complexity: O(E Log V) where, E is the number of edges and V is the number of vertices. Space Complexity: O(V) To find the shortest path
	In social networking applications
	In a telephone network
	To find the locations in the map To find the shortest path In social networking applications In a telephone network To find the locations in the map","// Dijkstra's Algorithm in Java

public class Dijkstra {

  public static void dijkstra(int[][] graph, int source) {
    int count = graph.length;
    boolean[] visitedVertex = new boolean[count];
    int[] distance = new int[count];
    for (int i = 0; i < count; i++) {
      visitedVertex[i] = false;
      distance[i] = Integer.MAX_VALUE;
    }

    // Distance of self loop is zero
    distance[source] = 0;
    for (int i = 0; i < count; i++) {

      // Update the distance between neighbouring vertex and source vertex
      int u = findMinDistance(distance, visitedVertex);
      visitedVertex[u] = true;

      // Update all the neighbouring vertex distances
      for (int v = 0; v < count; v++) {
        if (!visitedVertex[v] && graph[u][v] != 0 && (distance[u] + graph[u][v] < distance[v])) {
          distance[v] = distance[u] + graph[u][v];
        }
      }
    }
    for (int i = 0; i < distance.length; i++) {
      System.out.println(String.format(""Distance from %s to %s is %s"", source, i, distance[i]));
    }

  }

  // Finding the minimum distance
  private static int findMinDistance(int[] distance, boolean[] visitedVertex) {
    int minDistance = Integer.MAX_VALUE;
    int minDistanceVertex = -1;
    for (int i = 0; i < distance.length; i++) {
      if (!visitedVertex[i] && distance[i] < minDistance) {
        minDistance = distance[i];
        minDistanceVertex = i;
      }
    }
    return minDistanceVertex;
  }

  public static void main(String[] args) {
    int graph[][] = new int[][] { { 0, 0, 1, 2, 0, 0, 0 }, { 0, 0, 2, 0, 0, 3, 0 }, { 1, 2, 0, 1, 3, 0, 0 },
        { 2, 0, 1, 0, 0, 0, 1 }, { 0, 0, 3, 0, 0, 2, 0 }, { 0, 3, 0, 0, 2, 0, 1 }, { 0, 0, 0, 1, 0, 1, 0 } };
    Dijkstra T = new Dijkstra();
    T.dijkstra(graph, 0);
  }
}"
Dijkstra's Algorithm,"It differs from the minimum spanning tree because the shortest distance between two vertices might not include all the vertices of the graph. Dijkstra's Algorithm works on the basis that any subpath B -> D of the shortest path A -> D between vertices A and D is also the shortest path between vertices B and D. Djikstra used this property in the opposite direction i.e we overestimate the distance of each vertex from the starting vertex. Then we visit each node and its neighbors to find the shortest subpath to those neighbors. The algorithm uses a greedy approach in the sense that we find the next best solution hoping that the end result is the best solution for the whole problem. It is easier to start with an example and then think about the algorithm. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length.  Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. A minimum priority queue can be used to efficiently receive the vertex with least path distance. The implementation of Dijkstra's Algorithm in C++ is given below. The complexity of the code can be improved, but the abstractions are convenient to relate the code with the algorithm. Time Complexity: O(E Log V) where, E is the number of edges and V is the number of vertices. Space Complexity: O(V) To find the shortest path
	In social networking applications
	In a telephone network
	To find the locations in the map To find the shortest path In social networking applications In a telephone network To find the locations in the map","// Dijkstra's Algorithm in C

#include <stdio.h>
#define INFINITY 9999
#define MAX 10

void Dijkstra(int Graph[MAX][MAX], int n, int start);

void Dijkstra(int Graph[MAX][MAX], int n, int start) {
  int cost[MAX][MAX], distance[MAX], pred[MAX];
  int visited[MAX], count, mindistance, nextnode, i, j;

  // Creating cost matrix
  for (i = 0; i < n; i++)
    for (j = 0; j < n; j++)
      if (Graph[i][j] == 0)
        cost[i][j] = INFINITY;
      else
        cost[i][j] = Graph[i][j];

  for (i = 0; i < n; i++) {
    distance[i] = cost[start][i];
    pred[i] = start;
    visited[i] = 0;
  }

  distance[start] = 0;
  visited[start] = 1;
  count = 1;

  while (count < n - 1) {
    mindistance = INFINITY;

    for (i = 0; i < n; i++)
      if (distance[i] < mindistance && !visited[i]) {
        mindistance = distance[i];
        nextnode = i;
      }

    visited[nextnode] = 1;
    for (i = 0; i < n; i++)
      if (!visited[i])
        if (mindistance + cost[nextnode][i] < distance[i]) {
          distance[i] = mindistance + cost[nextnode][i];
          pred[i] = nextnode;
        }
    count++;
  }

  // Printing the distance
  for (i = 0; i < n; i++)
    if (i != start) {
      printf(""\nDistance from source to %d: %d"", i, distance[i]);
    }
}
int main() {
  int Graph[MAX][MAX], i, j, n, u;
  n = 7;

  Graph[0][0] = 0;
  Graph[0][1] = 0;
  Graph[0][2] = 1;
  Graph[0][3] = 2;
  Graph[0][4] = 0;
  Graph[0][5] = 0;
  Graph[0][6] = 0;

  Graph[1][0] = 0;
  Graph[1][1] = 0;
  Graph[1][2] = 2;
  Graph[1][3] = 0;
  Graph[1][4] = 0;
  Graph[1][5] = 3;
  Graph[1][6] = 0;

  Graph[2][0] = 1;
  Graph[2][1] = 2;
  Graph[2][2] = 0;
  Graph[2][3] = 1;
  Graph[2][4] = 3;
  Graph[2][5] = 0;
  Graph[2][6] = 0;

  Graph[3][0] = 2;
  Graph[3][1] = 0;
  Graph[3][2] = 1;
  Graph[3][3] = 0;
  Graph[3][4] = 0;
  Graph[3][5] = 0;
  Graph[3][6] = 1;

  Graph[4][0] = 0;
  Graph[4][1] = 0;
  Graph[4][2] = 3;
  Graph[4][3] = 0;
  Graph[4][4] = 0;
  Graph[4][5] = 2;
  Graph[4][6] = 0;

  Graph[5][0] = 0;
  Graph[5][1] = 3;
  Graph[5][2] = 0;
  Graph[5][3] = 0;
  Graph[5][4] = 2;
  Graph[5][5] = 0;
  Graph[5][6] = 1;

  Graph[6][0] = 0;
  Graph[6][1] = 0;
  Graph[6][2] = 0;
  Graph[6][3] = 1;
  Graph[6][4] = 0;
  Graph[6][5] = 1;
  Graph[6][6] = 0;

  u = 0;
  Dijkstra(Graph, n, u);

  return 0;
}"
Dijkstra's Algorithm,"It differs from the minimum spanning tree because the shortest distance between two vertices might not include all the vertices of the graph. Dijkstra's Algorithm works on the basis that any subpath B -> D of the shortest path A -> D between vertices A and D is also the shortest path between vertices B and D. Djikstra used this property in the opposite direction i.e we overestimate the distance of each vertex from the starting vertex. Then we visit each node and its neighbors to find the shortest subpath to those neighbors. The algorithm uses a greedy approach in the sense that we find the next best solution hoping that the end result is the best solution for the whole problem. It is easier to start with an example and then think about the algorithm. We need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices. We also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length.  Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path. A minimum priority queue can be used to efficiently receive the vertex with least path distance. The implementation of Dijkstra's Algorithm in C++ is given below. The complexity of the code can be improved, but the abstractions are convenient to relate the code with the algorithm. Time Complexity: O(E Log V) where, E is the number of edges and V is the number of vertices. Space Complexity: O(V) To find the shortest path
	In social networking applications
	In a telephone network
	To find the locations in the map To find the shortest path In social networking applications In a telephone network To find the locations in the map","// Dijkstra's Algorithm in C++

#include <iostream>
#include <vector>

#define INT_MAX 10000000

using namespace std;

void DijkstrasTest();

int main() {
  DijkstrasTest();
  return 0;
}

class Node;
class Edge;

void Dijkstras();
vector<Node*>* AdjacentRemainingNodes(Node* node);
Node* ExtractSmallest(vector<Node*>& nodes);
int Distance(Node* node1, Node* node2);
bool Contains(vector<Node*>& nodes, Node* node);
void PrintShortestRouteTo(Node* destination);

vector<Node*> nodes;
vector<Edge*> edges;

class Node {
   public:
  Node(char id)
    : id(id), previous(NULL), distanceFromStart(INT_MAX) {
    nodes.push_back(this);
  }

   public:
  char id;
  Node* previous;
  int distanceFromStart;
};

class Edge {
   public:
  Edge(Node* node1, Node* node2, int distance)
    : node1(node1), node2(node2), distance(distance) {
    edges.push_back(this);
  }
  bool Connects(Node* node1, Node* node2) {
    return (
      (node1 == this->node1 &&
       node2 == this->node2) ||
      (node1 == this->node2 &&
       node2 == this->node1));
  }

   public:
  Node* node1;
  Node* node2;
  int distance;
};

///////////////////
void DijkstrasTest() {
  Node* a = new Node('a');
  Node* b = new Node('b');
  Node* c = new Node('c');
  Node* d = new Node('d');
  Node* e = new Node('e');
  Node* f = new Node('f');
  Node* g = new Node('g');

  Edge* e1 = new Edge(a, c, 1);
  Edge* e2 = new Edge(a, d, 2);
  Edge* e3 = new Edge(b, c, 2);
  Edge* e4 = new Edge(c, d, 1);
  Edge* e5 = new Edge(b, f, 3);
  Edge* e6 = new Edge(c, e, 3);
  Edge* e7 = new Edge(e, f, 2);
  Edge* e8 = new Edge(d, g, 1);
  Edge* e9 = new Edge(g, f, 1);

  a->distanceFromStart = 0;  // set start node
  Dijkstras();
  PrintShortestRouteTo(f);
}

///////////////////

void Dijkstras() {
  while (nodes.size() > 0) {
    Node* smallest = ExtractSmallest(nodes);
    vector<Node*>* adjacentNodes =
      AdjacentRemainingNodes(smallest);

    const int size = adjacentNodes->size();
    for (int i = 0; i < size; ++i) {
      Node* adjacent = adjacentNodes->at(i);
      int distance = Distance(smallest, adjacent) +
               smallest->distanceFromStart;

      if (distance < adjacent->distanceFromStart) {
        adjacent->distanceFromStart = distance;
        adjacent->previous = smallest;
      }
    }
    delete adjacentNodes;
  }
}

// Find the node with the smallest distance,
// remove it, and return it.
Node* ExtractSmallest(vector<Node*>& nodes) {
  int size = nodes.size();
  if (size == 0) return NULL;
  int smallestPosition = 0;
  Node* smallest = nodes.at(0);
  for (int i = 1; i < size; ++i) {
    Node* current = nodes.at(i);
    if (current->distanceFromStart <
      smallest->distanceFromStart) {
      smallest = current;
      smallestPosition = i;
    }
  }
  nodes.erase(nodes.begin() + smallestPosition);
  return smallest;
}

// Return all nodes adjacent to 'node' which are still
// in the 'nodes' collection.
vector<Node*>* AdjacentRemainingNodes(Node* node) {
  vector<Node*>* adjacentNodes = new vector<Node*>();
  const int size = edges.size();
  for (int i = 0; i < size; ++i) {
    Edge* edge = edges.at(i);
    Node* adjacent = NULL;
    if (edge->node1 == node) {
      adjacent = edge->node2;
    } else if (edge->node2 == node) {
      adjacent = edge->node1;
    }
    if (adjacent && Contains(nodes, adjacent)) {
      adjacentNodes->push_back(adjacent);
    }
  }
  return adjacentNodes;
}

// Return distance between two connected nodes
int Distance(Node* node1, Node* node2) {
  const int size = edges.size();
  for (int i = 0; i < size; ++i) {
    Edge* edge = edges.at(i);
    if (edge->Connects(node1, node2)) {
      return edge->distance;
    }
  }
  return -1;  // should never happen
}

// Does the 'nodes' vector contain 'node'
bool Contains(vector<Node*>& nodes, Node* node) {
  const int size = nodes.size();
  for (int i = 0; i < size; ++i) {
    if (node == nodes.at(i)) {
      return true;
    }
  }
  return false;
}

///////////////////

void PrintShortestRouteTo(Node* destination) {
  Node* previous = destination;
  cout << ""Distance from start: ""
     << destination->distanceFromStart << endl;
  while (previous) {
    cout << previous->id << "" "";
    previous = previous->previous;
  }
  cout << endl;
}

// these two not needed
vector<Edge*>* AdjacentEdges(vector<Edge*>& Edges, Node* node);
void RemoveEdge(vector<Edge*>& Edges, Edge* edge);

vector<Edge*>* AdjacentEdges(vector<Edge*>& edges, Node* node) {
  vector<Edge*>* adjacentEdges = new vector<Edge*>();

  const int size = edges.size();
  for (int i = 0; i < size; ++i) {
    Edge* edge = edges.at(i);
    if (edge->node1 == node) {
      cout << ""adjacent: "" << edge->node2->id << endl;
      adjacentEdges->push_back(edge);
    } else if (edge->node2 == node) {
      cout << ""adjacent: "" << edge->node1->id << endl;
      adjacentEdges->push_back(edge);
    }
  }
  return adjacentEdges;
}

void RemoveEdge(vector<Edge*>& edges, Edge* edge) {
  vector<Edge*>::iterator it;
  for (it = edges.begin(); it < edges.end(); ++it) {
    if (*it == edge) {
      edges.erase(it);
      return;
    }
  }
}"
Kruskal's Algorithm,"Kruskal's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from the edges with the lowest weight and keep adding edges until we reach our goal. The steps for implementing Kruskal's algorithm are as follows: Sort all the edges from low weight to high Take the edge with the lowest weight and add it to the spanning tree. If adding the edge created a cycle, then reject this edge. Keep adding edges until we reach all vertices. Any minimum spanning tree algorithm revolves around checking if adding an edge creates a loop or not.  The most common way to find this out is an algorithm called Union FInd. The Union-Find algorithm divides the vertices into clusters and allows us to check if two vertices belong to the same cluster or not and hence decide whether adding an edge creates a cycle. Prim's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from an edge, Prim's algorithm starts from a vertex and keeps adding lowest-weight edges which aren't in the tree, until all vertices have been covered. The time complexity Of Kruskal's Algorithm is: O(E log E). In order to layout electrical wiring
	In computer network (LAN connection) In order to layout electrical wiring In computer network (LAN connection)","# Kruskal's algorithm in Python


class Graph:
    def __init__(self, vertices):
        self.V = vertices
        self.graph = []

    def add_edge(self, u, v, w):
        self.graph.append([u, v, w])

    # Search function

    def find(self, parent, i):
        if parent[i] == i:
            return i
        return self.find(parent, parent[i])

    def apply_union(self, parent, rank, x, y):
        xroot = self.find(parent, x)
        yroot = self.find(parent, y)
        if rank[xroot] < rank[yroot]:
            parent[xroot] = yroot
        elif rank[xroot] > rank[yroot]:
            parent[yroot] = xroot
        else:
            parent[yroot] = xroot
            rank[xroot] += 1

    #  Applying Kruskal algorithm
    def kruskal_algo(self):
        result = []
        i, e = 0, 0
        self.graph = sorted(self.graph, key=lambda item: item[2])
        parent = []
        rank = []
        for node in range(self.V):
            parent.append(node)
            rank.append(0)
        while e < self.V - 1:
            u, v, w = self.graph[i]
            i = i + 1
            x = self.find(parent, u)
            y = self.find(parent, v)
            if x != y:
                e = e + 1
                result.append([u, v, w])
                self.apply_union(parent, rank, x, y)
        for u, v, weight in result:
            print(""%d - %d: %d"" % (u, v, weight))


g = Graph(6)
g.add_edge(0, 1, 4)
g.add_edge(0, 2, 4)
g.add_edge(1, 2, 2)
g.add_edge(1, 0, 4)
g.add_edge(2, 0, 4)
g.add_edge(2, 1, 2)
g.add_edge(2, 3, 3)
g.add_edge(2, 5, 2)
g.add_edge(2, 4, 4)
g.add_edge(3, 2, 3)
g.add_edge(3, 4, 3)
g.add_edge(4, 2, 4)
g.add_edge(4, 3, 3)
g.add_edge(5, 2, 2)
g.add_edge(5, 4, 3)
g.kruskal_algo()"
Kruskal's Algorithm,"Kruskal's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from the edges with the lowest weight and keep adding edges until we reach our goal. The steps for implementing Kruskal's algorithm are as follows: Sort all the edges from low weight to high Take the edge with the lowest weight and add it to the spanning tree. If adding the edge created a cycle, then reject this edge. Keep adding edges until we reach all vertices. Any minimum spanning tree algorithm revolves around checking if adding an edge creates a loop or not.  The most common way to find this out is an algorithm called Union FInd. The Union-Find algorithm divides the vertices into clusters and allows us to check if two vertices belong to the same cluster or not and hence decide whether adding an edge creates a cycle. Prim's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from an edge, Prim's algorithm starts from a vertex and keeps adding lowest-weight edges which aren't in the tree, until all vertices have been covered. The time complexity Of Kruskal's Algorithm is: O(E log E). In order to layout electrical wiring
	In computer network (LAN connection) In order to layout electrical wiring In computer network (LAN connection)","// Kruskal's algorithm in Java

import java.util.*;

class Graph {
  class Edge implements Comparable<Edge> {
    int src, dest, weight;

    public int compareTo(Edge compareEdge) {
      return this.weight - compareEdge.weight;
    }
  };

  // Union
  class subset {
    int parent, rank;
  };

  int vertices, edges;
  Edge edge[];

  // Graph creation
  Graph(int v, int e) {
    vertices = v;
    edges = e;
    edge = new Edge[edges];
    for (int i = 0; i < e; ++i)
      edge[i] = new Edge();
  }

  int find(subset subsets[], int i) {
    if (subsets[i].parent != i)
      subsets[i].parent = find(subsets, subsets[i].parent);
    return subsets[i].parent;
  }

  void Union(subset subsets[], int x, int y) {
    int xroot = find(subsets, x);
    int yroot = find(subsets, y);

    if (subsets[xroot].rank < subsets[yroot].rank)
      subsets[xroot].parent = yroot;
    else if (subsets[xroot].rank > subsets[yroot].rank)
      subsets[yroot].parent = xroot;
    else {
      subsets[yroot].parent = xroot;
      subsets[xroot].rank++;
    }
  }

  // Applying Krushkal Algorithm
  void KruskalAlgo() {
    Edge result[] = new Edge[vertices];
    int e = 0;
    int i = 0;
    for (i = 0; i < vertices; ++i)
      result[i] = new Edge();

    // Sorting the edges
    Arrays.sort(edge);
    subset subsets[] = new subset[vertices];
    for (i = 0; i < vertices; ++i)
      subsets[i] = new subset();

    for (int v = 0; v < vertices; ++v) {
      subsets[v].parent = v;
      subsets[v].rank = 0;
    }
    i = 0;
    while (e < vertices - 1) {
      Edge next_edge = new Edge();
      next_edge = edge[i++];
      int x = find(subsets, next_edge.src);
      int y = find(subsets, next_edge.dest);
      if (x != y) {
        result[e++] = next_edge;
        Union(subsets, x, y);
      }
    }
    for (i = 0; i < e; ++i)
      System.out.println(result[i].src + "" - "" + result[i].dest + "": "" + result[i].weight);
  }

  public static void main(String[] args) {
    int vertices = 6; // Number of vertices
    int edges = 8; // Number of edges
    Graph G = new Graph(vertices, edges);

    G.edge[0].src = 0;
    G.edge[0].dest = 1;
    G.edge[0].weight = 4;

    G.edge[1].src = 0;
    G.edge[1].dest = 2;
    G.edge[1].weight = 4;

    G.edge[2].src = 1;
    G.edge[2].dest = 2;
    G.edge[2].weight = 2;

    G.edge[3].src = 2;
    G.edge[3].dest = 3;
    G.edge[3].weight = 3;

    G.edge[4].src = 2;
    G.edge[4].dest = 5;
    G.edge[4].weight = 2;

    G.edge[5].src = 2;
    G.edge[5].dest = 4;
    G.edge[5].weight = 4;

    G.edge[6].src = 3;
    G.edge[6].dest = 4;
    G.edge[6].weight = 3;

    G.edge[7].src = 5;
    G.edge[7].dest = 4;
    G.edge[7].weight = 3;
    G.KruskalAlgo();
  }
}"
Kruskal's Algorithm,"Kruskal's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from the edges with the lowest weight and keep adding edges until we reach our goal. The steps for implementing Kruskal's algorithm are as follows: Sort all the edges from low weight to high Take the edge with the lowest weight and add it to the spanning tree. If adding the edge created a cycle, then reject this edge. Keep adding edges until we reach all vertices. Any minimum spanning tree algorithm revolves around checking if adding an edge creates a loop or not.  The most common way to find this out is an algorithm called Union FInd. The Union-Find algorithm divides the vertices into clusters and allows us to check if two vertices belong to the same cluster or not and hence decide whether adding an edge creates a cycle. Prim's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from an edge, Prim's algorithm starts from a vertex and keeps adding lowest-weight edges which aren't in the tree, until all vertices have been covered. The time complexity Of Kruskal's Algorithm is: O(E log E). In order to layout electrical wiring
	In computer network (LAN connection) In order to layout electrical wiring In computer network (LAN connection)","// Kruskal's algorithm in C

#include <stdio.h>

#define MAX 30

typedef struct edge {
  int u, v, w;
} edge;

typedef struct edge_list {
  edge data[MAX];
  int n;
} edge_list;

edge_list elist;

int Graph[MAX][MAX], n;
edge_list spanlist;

void kruskalAlgo();
int find(int belongs[], int vertexno);
void applyUnion(int belongs[], int c1, int c2);
void sort();
void print();

// Applying Krushkal Algo
void kruskalAlgo() {
  int belongs[MAX], i, j, cno1, cno2;
  elist.n = 0;

  for (i = 1; i < n; i++)
    for (j = 0; j < i; j++) {
      if (Graph[i][j] != 0) {
        elist.data[elist.n].u = i;
        elist.data[elist.n].v = j;
        elist.data[elist.n].w = Graph[i][j];
        elist.n++;
      }
    }

  sort();

  for (i = 0; i < n; i++)
    belongs[i] = i;

  spanlist.n = 0;

  for (i = 0; i < elist.n; i++) {
    cno1 = find(belongs, elist.data[i].u);
    cno2 = find(belongs, elist.data[i].v);

    if (cno1 != cno2) {
      spanlist.data[spanlist.n] = elist.data[i];
      spanlist.n = spanlist.n + 1;
      applyUnion(belongs, cno1, cno2);
    }
  }
}

int find(int belongs[], int vertexno) {
  return (belongs[vertexno]);
}

void applyUnion(int belongs[], int c1, int c2) {
  int i;

  for (i = 0; i < n; i++)
    if (belongs[i] == c2)
      belongs[i] = c1;
}

// Sorting algo
void sort() {
  int i, j;
  edge temp;

  for (i = 1; i < elist.n; i++)
    for (j = 0; j < elist.n - 1; j++)
      if (elist.data[j].w > elist.data[j + 1].w) {
        temp = elist.data[j];
        elist.data[j] = elist.data[j + 1];
        elist.data[j + 1] = temp;
      }
}

// Printing the result
void print() {
  int i, cost = 0;

  for (i = 0; i < spanlist.n; i++) {
    printf(""\n%d - %d : %d"", spanlist.data[i].u, spanlist.data[i].v, spanlist.data[i].w);
    cost = cost + spanlist.data[i].w;
  }

  printf(""\nSpanning tree cost: %d"", cost);
}

int main() {
  int i, j, total_cost;

  n = 6;

  Graph[0][0] = 0;
  Graph[0][1] = 4;
  Graph[0][2] = 4;
  Graph[0][3] = 0;
  Graph[0][4] = 0;
  Graph[0][5] = 0;
  Graph[0][6] = 0;

  Graph[1][0] = 4;
  Graph[1][1] = 0;
  Graph[1][2] = 2;
  Graph[1][3] = 0;
  Graph[1][4] = 0;
  Graph[1][5] = 0;
  Graph[1][6] = 0;

  Graph[2][0] = 4;
  Graph[2][1] = 2;
  Graph[2][2] = 0;
  Graph[2][3] = 3;
  Graph[2][4] = 4;
  Graph[2][5] = 0;
  Graph[2][6] = 0;

  Graph[3][0] = 0;
  Graph[3][1] = 0;
  Graph[3][2] = 3;
  Graph[3][3] = 0;
  Graph[3][4] = 3;
  Graph[3][5] = 0;
  Graph[3][6] = 0;

  Graph[4][0] = 0;
  Graph[4][1] = 0;
  Graph[4][2] = 4;
  Graph[4][3] = 3;
  Graph[4][4] = 0;
  Graph[4][5] = 0;
  Graph[4][6] = 0;

  Graph[5][0] = 0;
  Graph[5][1] = 0;
  Graph[5][2] = 2;
  Graph[5][3] = 0;
  Graph[5][4] = 3;
  Graph[5][5] = 0;
  Graph[5][6] = 0;

  kruskalAlgo();
  print();
}"
Kruskal's Algorithm,"Kruskal's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from the edges with the lowest weight and keep adding edges until we reach our goal. The steps for implementing Kruskal's algorithm are as follows: Sort all the edges from low weight to high Take the edge with the lowest weight and add it to the spanning tree. If adding the edge created a cycle, then reject this edge. Keep adding edges until we reach all vertices. Any minimum spanning tree algorithm revolves around checking if adding an edge creates a loop or not.  The most common way to find this out is an algorithm called Union FInd. The Union-Find algorithm divides the vertices into clusters and allows us to check if two vertices belong to the same cluster or not and hence decide whether adding an edge creates a cycle. Prim's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from an edge, Prim's algorithm starts from a vertex and keeps adding lowest-weight edges which aren't in the tree, until all vertices have been covered. The time complexity Of Kruskal's Algorithm is: O(E log E). In order to layout electrical wiring
	In computer network (LAN connection) In order to layout electrical wiring In computer network (LAN connection)","// Kruskal's algorithm in C++

#include <algorithm>
#include <iostream>
#include <vector>
using namespace std;

#define edge pair<int, int>

class Graph {
   private:
  vector<pair<int, edge> > G;  // graph
  vector<pair<int, edge> > T;  // mst
  int *parent;
  int V;  // number of vertices/nodes in graph
   public:
  Graph(int V);
  void AddWeightedEdge(int u, int v, int w);
  int find_set(int i);
  void union_set(int u, int v);
  void kruskal();
  void print();
};
Graph::Graph(int V) {
  parent = new int[V];

  //i 0 1 2 3 4 5
  //parent[i] 0 1 2 3 4 5
  for (int i = 0; i < V; i++)
    parent[i] = i;

  G.clear();
  T.clear();
}
void Graph::AddWeightedEdge(int u, int v, int w) {
  G.push_back(make_pair(w, edge(u, v)));
}
int Graph::find_set(int i) {
  // If i is the parent of itself
  if (i == parent[i])
    return i;
  else
    // Else if i is not the parent of itself
    // Then i is not the representative of his set,
    // so we recursively call Find on its parent
    return find_set(parent[i]);
}

void Graph::union_set(int u, int v) {
  parent[u] = parent[v];
}
void Graph::kruskal() {
  int i, uRep, vRep;
  sort(G.begin(), G.end());  // increasing weight
  for (i = 0; i < G.size(); i++) {
    uRep = find_set(G[i].second.first);
    vRep = find_set(G[i].second.second);
    if (uRep != vRep) {
      T.push_back(G[i]);  // add to tree
      union_set(uRep, vRep);
    }
  }
}
void Graph::print() {
  cout << ""Edge :""
     << "" Weight"" << endl;
  for (int i = 0; i < T.size(); i++) {
    cout << T[i].second.first << "" - "" << T[i].second.second << "" : ""
       << T[i].first;
    cout << endl;
  }
}
int main() {
  Graph g(6);
  g.AddWeightedEdge(0, 1, 4);
  g.AddWeightedEdge(0, 2, 4);
  g.AddWeightedEdge(1, 2, 2);
  g.AddWeightedEdge(1, 0, 4);
  g.AddWeightedEdge(2, 0, 4);
  g.AddWeightedEdge(2, 1, 2);
  g.AddWeightedEdge(2, 3, 3);
  g.AddWeightedEdge(2, 5, 2);
  g.AddWeightedEdge(2, 4, 4);
  g.AddWeightedEdge(3, 2, 3);
  g.AddWeightedEdge(3, 4, 3);
  g.AddWeightedEdge(4, 2, 4);
  g.AddWeightedEdge(4, 3, 3);
  g.AddWeightedEdge(5, 2, 2);
  g.AddWeightedEdge(5, 4, 3);
  g.kruskal();
  g.print();
  return 0;
}"
Prim's Algorithm,"Prim's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from one vertex and keep adding edges with the lowest weight until we reach our goal. The steps for implementing Prim's algorithm are as follows: Initialize the minimum spanning tree with a vertex chosen at random. Find all the edges that connect the tree to new vertices, find the minimum and add it to the tree Keep repeating step 2 until we get a minimum spanning tree The pseudocode for prim's algorithm shows how we create two sets of vertices U and V-U. U contains the list of vertices that have been visited and V-U the list of vertices that haven't. One by one, we move vertices from set V-U to set U by connecting the least weight edge.  Although adjacency matrix representation of graphs is used, this algorithm can also be implemented using Adjacency List to improve its efficiency. Kruskal's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from a vertex, Kruskal's algorithm sorts all the edges from low weight to high and keeps adding the lowest edges, ignoring those edges that create a cycle. The time complexity of Prim's algorithm is O(E log V). Laying cables of electrical wiring
	In network designed
	To make protocols in network cycles Laying cables of electrical wiring In network designed To make protocols in network cycles","# Prim's Algorithm in Python


INF = 9999999
# number of vertices in graph
V = 5
# create a 2d array of size 5x5
# for adjacency matrix to represent graph
G = [[0, 9, 75, 0, 0],
     [9, 0, 95, 19, 42],
     [75, 95, 0, 51, 66],
     [0, 19, 51, 0, 31],
     [0, 42, 66, 31, 0]]
# create a array to track selected vertex
# selected will become true otherwise false
selected = [0, 0, 0, 0, 0]
# set number of edge to 0
no_edge = 0
# the number of egde in minimum spanning tree will be
# always less than(V - 1), where V is number of vertices in
# graph
# choose 0th vertex and make it true
selected[0] = True
# print for edge and weight
print(""Edge : Weight\n"")
while (no_edge < V - 1):
    # For every vertex in the set S, find the all adjacent vertices
    #, calculate the distance from the vertex selected at step 1.
    # if the vertex is already in the set S, discard it otherwise
    # choose another vertex nearest to selected vertex  at step 1.
    minimum = INF
    x = 0
    y = 0
    for i in range(V):
        if selected[i]:
            for j in range(V):
                if ((not selected[j]) and G[i][j]):  
                    # not in selected and there is an edge
                    if minimum > G[i][j]:
                        minimum = G[i][j]
                        x = i
                        y = j
    print(str(x) + ""-"" + str(y) + "":"" + str(G[x][y]))
    selected[y] = True
    no_edge += 1"
Prim's Algorithm,"Prim's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from one vertex and keep adding edges with the lowest weight until we reach our goal. The steps for implementing Prim's algorithm are as follows: Initialize the minimum spanning tree with a vertex chosen at random. Find all the edges that connect the tree to new vertices, find the minimum and add it to the tree Keep repeating step 2 until we get a minimum spanning tree The pseudocode for prim's algorithm shows how we create two sets of vertices U and V-U. U contains the list of vertices that have been visited and V-U the list of vertices that haven't. One by one, we move vertices from set V-U to set U by connecting the least weight edge.  Although adjacency matrix representation of graphs is used, this algorithm can also be implemented using Adjacency List to improve its efficiency. Kruskal's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from a vertex, Kruskal's algorithm sorts all the edges from low weight to high and keeps adding the lowest edges, ignoring those edges that create a cycle. The time complexity of Prim's algorithm is O(E log V). Laying cables of electrical wiring
	In network designed
	To make protocols in network cycles Laying cables of electrical wiring In network designed To make protocols in network cycles","// Prim's Algorithm in Java

import java.util.Arrays;

class PGraph {

  public void Prim(int G[][], int V) {

    int INF = 9999999;

    int no_edge; // number of edge

    // create a array to track selected vertex
    // selected will become true otherwise false
    boolean[] selected = new boolean[V];

    // set selected false initially
    Arrays.fill(selected, false);

    // set number of edge to 0
    no_edge = 0;

    // the number of egde in minimum spanning tree will be
    // always less than (V -1), where V is number of vertices in
    // graph

    // choose 0th vertex and make it true
    selected[0] = true;

    // print for edge and weight
    System.out.println(""Edge : Weight"");

    while (no_edge < V - 1) {
      // For every vertex in the set S, find the all adjacent vertices
      // , calculate the distance from the vertex selected at step 1.
      // if the vertex is already in the set S, discard it otherwise
      // choose another vertex nearest to selected vertex at step 1.

      int min = INF;
      int x = 0; // row number
      int y = 0; // col number

      for (int i = 0; i < V; i++) {
        if (selected[i] == true) {
          for (int j = 0; j < V; j++) {
            // not in selected and there is an edge
            if (!selected[j] && G[i][j] != 0) {
              if (min > G[i][j]) {
                min = G[i][j];
                x = i;
                y = j;
              }
            }
          }
        }
      }
      System.out.println(x + "" - "" + y + "" :  "" + G[x][y]);
      selected[y] = true;
      no_edge++;
    }
  }

  public static void main(String[] args) {
    PGraph g = new PGraph();

    // number of vertices in grapj
    int V = 5;

    // create a 2d array of size 5x5
    // for adjacency matrix to represent graph
    int[][] G = { { 0, 9, 75, 0, 0 }, { 9, 0, 95, 19, 42 }, { 75, 95, 0, 51, 66 }, { 0, 19, 51, 0, 31 },
        { 0, 42, 66, 31, 0 } };

    g.Prim(G, V);
  }
}"
Prim's Algorithm,"Prim's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from one vertex and keep adding edges with the lowest weight until we reach our goal. The steps for implementing Prim's algorithm are as follows: Initialize the minimum spanning tree with a vertex chosen at random. Find all the edges that connect the tree to new vertices, find the minimum and add it to the tree Keep repeating step 2 until we get a minimum spanning tree The pseudocode for prim's algorithm shows how we create two sets of vertices U and V-U. U contains the list of vertices that have been visited and V-U the list of vertices that haven't. One by one, we move vertices from set V-U to set U by connecting the least weight edge.  Although adjacency matrix representation of graphs is used, this algorithm can also be implemented using Adjacency List to improve its efficiency. Kruskal's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from a vertex, Kruskal's algorithm sorts all the edges from low weight to high and keeps adding the lowest edges, ignoring those edges that create a cycle. The time complexity of Prim's algorithm is O(E log V). Laying cables of electrical wiring
	In network designed
	To make protocols in network cycles Laying cables of electrical wiring In network designed To make protocols in network cycles","// Prim's Algorithm in C

#include<stdio.h>
#include<stdbool.h> 

#define INF 9999999

// number of vertices in graph
#define V 5

// create a 2d array of size 5x5
//for adjacency matrix to represent graph
int G[V][V] = {
  {0, 9, 75, 0, 0},
  {9, 0, 95, 19, 42},
  {75, 95, 0, 51, 66},
  {0, 19, 51, 0, 31},
  {0, 42, 66, 31, 0}};

int main() {
  int no_edge;  // number of edge

  // create a array to track selected vertex
  // selected will become true otherwise false
  int selected[V];

  // set selected false initially
  memset(selected, false, sizeof(selected));
  
  // set number of edge to 0
  no_edge = 0;

  // the number of egde in minimum spanning tree will be
  // always less than (V -1), where V is number of vertices in
  //graph

  // choose 0th vertex and make it true
  selected[0] = true;

  int x;  //  row number
  int y;  //  col number

  // print for edge and weight
  printf(""Edge : Weight\n"");

  while (no_edge < V - 1) {
    //For every vertex in the set S, find the all adjacent vertices
    // , calculate the distance from the vertex selected at step 1.
    // if the vertex is already in the set S, discard it otherwise
    //choose another vertex nearest to selected vertex  at step 1.

    int min = INF;
    x = 0;
    y = 0;

    for (int i = 0; i < V; i++) {
      if (selected[i]) {
        for (int j = 0; j < V; j++) {
          if (!selected[j] && G[i][j]) {  // not in selected and there is an edge
            if (min > G[i][j]) {
              min = G[i][j];
              x = i;
              y = j;
            }
          }
        }
      }
    }
    printf(""%d - %d : %d\n"", x, y, G[x][y]);
    selected[y] = true;
    no_edge++;
  }

  return 0;
}"
Prim's Algorithm,"Prim's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which form a tree that includes every vertex
	has the minimum sum of weights among all the trees that can be formed from the graph form a tree that includes every vertex has the minimum sum of weights among all the trees that can be formed from the graph It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum. We start from one vertex and keep adding edges with the lowest weight until we reach our goal. The steps for implementing Prim's algorithm are as follows: Initialize the minimum spanning tree with a vertex chosen at random. Find all the edges that connect the tree to new vertices, find the minimum and add it to the tree Keep repeating step 2 until we get a minimum spanning tree The pseudocode for prim's algorithm shows how we create two sets of vertices U and V-U. U contains the list of vertices that have been visited and V-U the list of vertices that haven't. One by one, we move vertices from set V-U to set U by connecting the least weight edge.  Although adjacency matrix representation of graphs is used, this algorithm can also be implemented using Adjacency List to improve its efficiency. Kruskal's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. Instead of starting from a vertex, Kruskal's algorithm sorts all the edges from low weight to high and keeps adding the lowest edges, ignoring those edges that create a cycle. The time complexity of Prim's algorithm is O(E log V). Laying cables of electrical wiring
	In network designed
	To make protocols in network cycles Laying cables of electrical wiring In network designed To make protocols in network cycles","// Prim's Algorithm in C++

#include <cstring>
#include <iostream>
using namespace std;

#define INF 9999999

// number of vertices in grapj
#define V 5

// create a 2d array of size 5x5
//for adjacency matrix to represent graph

int G[V][V] = {
  {0, 9, 75, 0, 0},
  {9, 0, 95, 19, 42},
  {75, 95, 0, 51, 66},
  {0, 19, 51, 0, 31},
  {0, 42, 66, 31, 0}};

int main() {
  int no_edge;  // number of edge

  // create a array to track selected vertex
  // selected will become true otherwise false
  int selected[V];

  // set selected false initially
  memset(selected, false, sizeof(selected));

  // set number of edge to 0
  no_edge = 0;

  // the number of egde in minimum spanning tree will be
  // always less than (V -1), where V is number of vertices in
  //graph

  // choose 0th vertex and make it true
  selected[0] = true;

  int x;  //  row number
  int y;  //  col number

  // print for edge and weight
  cout << ""Edge""
     << "" : ""
     << ""Weight"";
  cout << endl;
  while (no_edge < V - 1) {
    //For every vertex in the set S, find the all adjacent vertices
    // , calculate the distance from the vertex selected at step 1.
    // if the vertex is already in the set S, discard it otherwise
    //choose another vertex nearest to selected vertex  at step 1.

    int min = INF;
    x = 0;
    y = 0;

    for (int i = 0; i < V; i++) {
      if (selected[i]) {
        for (int j = 0; j < V; j++) {
          if (!selected[j] && G[i][j]) {  // not in selected and there is an edge
            if (min > G[i][j]) {
              min = G[i][j];
              x = i;
              y = j;
            }
          }
        }
      }
    }
    cout << x << "" - "" << y << "" :  "" << G[x][y];
    cout << endl;
    selected[y] = true;
    no_edge++;
  }

  return 0;
}"
Huffman Coding Algorithm,"Huffman Coding is a technique of compressing data to reduce its size without losing any of the details. It was first developed by David Huffman. Huffman Coding is generally useful to compress the data in which there are frequently occurring characters. Suppose the string below is to be sent over a network. Each character occupies 8 bits. There are a total of 15 characters in the above string. Thus, a total of 8 * 15 = 120 bits are required to send this string. Using the Huffman Coding technique, we can compress the string to a smaller size. Huffman coding first creates a tree using the frequencies of the character and then generates code for each character.  Once the data is encoded, it has to be decoded. Decoding is done using the same tree. Huffman Coding prevents any ambiguity in the decoding process using the concept of prefix code ie. a code associated with a character should not be present in the prefix of any other code. The tree created above helps in maintaining the property. Huffman coding is done with the help of the following steps. Calculate the frequency of each character in the string.
		
			Frequency of string Sort the characters in increasing order of the frequency. These are stored in a priority queue Q.
		
			Characters sorted according to the frequency Make each unique character as a leaf node. Create an empty node z. Assign the minimum frequency to the left child of z and assign the second minimum frequency to the right child of z. Set the value of the z as the sum of the above two minimum frequencies.
		
			Getting the sum of the least numbers Remove these two minimum frequencies from Q and add the sum into the list of frequencies (* denote the internal nodes in the figure above). Insert node z into the tree. Repeat steps 3 to 5 for all the characters.
		
			Repeat steps 3 to 5 for all the characters.
		
		 

		
			Repeat steps 3 to 5 for all the characters. For each non-leaf node, assign 0 to the left edge and 1 to the right edge.
		
			Assign 0 to the left edge and 1 to the right edge For sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below.   Without encoding, the total size of the string was 120 bits. After encoding the size is reduced to 32 + 15 + 28 = 75. For decoding the code, we can take the code and traverse through the tree to find the character. Let 101 is to be decoded, we can traverse from the root as in the figure below. The time complexity for encoding each unique character based on its frequency is O(nlog n). Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). Thus the overall complexity is O(nlog n). Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc.
	For text and fax transmissions. Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc. For text and fax transmissions.","# Huffman Coding in python

string = 'BCAADDDCCACACAC'


# Creating tree nodes
class NodeTree(object):

    def __init__(self, left=None, right=None):
        self.left = left
        self.right = right

    def children(self):
        return (self.left, self.right)

    def nodes(self):
        return (self.left, self.right)

    def __str__(self):
        return '%s_%s' % (self.left, self.right)


# Main function implementing huffman coding
def huffman_code_tree(node, left=True, binString=''):
    if type(node) is str:
        return {node: binString}
    (l, r) = node.children()
    d = dict()
    d.update(huffman_code_tree(l, True, binString + '0'))
    d.update(huffman_code_tree(r, False, binString + '1'))
    return d


# Calculating frequency
freq = {}
for c in string:
    if c in freq:
        freq[c] += 1
    else:
        freq[c] = 1

freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)

nodes = freq

while len(nodes) > 1:
    (key1, c1) = nodes[-1]
    (key2, c2) = nodes[-2]
    nodes = nodes[:-2]
    node = NodeTree(key1, key2)
    nodes.append((node, c1 + c2))

    nodes = sorted(nodes, key=lambda x: x[1], reverse=True)

huffmanCode = huffman_code_tree(nodes[0][0])

print(' Char | Huffman code ')
print('----------------------')
for (char, frequency) in freq:
    print(' %-4r |%12s' % (char, huffmanCode[char]))"
Huffman Coding Algorithm,"Huffman Coding is a technique of compressing data to reduce its size without losing any of the details. It was first developed by David Huffman. Huffman Coding is generally useful to compress the data in which there are frequently occurring characters. Suppose the string below is to be sent over a network. Each character occupies 8 bits. There are a total of 15 characters in the above string. Thus, a total of 8 * 15 = 120 bits are required to send this string. Using the Huffman Coding technique, we can compress the string to a smaller size. Huffman coding first creates a tree using the frequencies of the character and then generates code for each character.  Once the data is encoded, it has to be decoded. Decoding is done using the same tree. Huffman Coding prevents any ambiguity in the decoding process using the concept of prefix code ie. a code associated with a character should not be present in the prefix of any other code. The tree created above helps in maintaining the property. Huffman coding is done with the help of the following steps. Calculate the frequency of each character in the string.
		
			Frequency of string Sort the characters in increasing order of the frequency. These are stored in a priority queue Q.
		
			Characters sorted according to the frequency Make each unique character as a leaf node. Create an empty node z. Assign the minimum frequency to the left child of z and assign the second minimum frequency to the right child of z. Set the value of the z as the sum of the above two minimum frequencies.
		
			Getting the sum of the least numbers Remove these two minimum frequencies from Q and add the sum into the list of frequencies (* denote the internal nodes in the figure above). Insert node z into the tree. Repeat steps 3 to 5 for all the characters.
		
			Repeat steps 3 to 5 for all the characters.
		
		 

		
			Repeat steps 3 to 5 for all the characters. For each non-leaf node, assign 0 to the left edge and 1 to the right edge.
		
			Assign 0 to the left edge and 1 to the right edge For sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below.   Without encoding, the total size of the string was 120 bits. After encoding the size is reduced to 32 + 15 + 28 = 75. For decoding the code, we can take the code and traverse through the tree to find the character. Let 101 is to be decoded, we can traverse from the root as in the figure below. The time complexity for encoding each unique character based on its frequency is O(nlog n). Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). Thus the overall complexity is O(nlog n). Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc.
	For text and fax transmissions. Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc. For text and fax transmissions.","// Huffman Coding in Java

import java.util.PriorityQueue;
import java.util.Comparator;

class HuffmanNode {
  int item;
  char c;
  HuffmanNode left;
  HuffmanNode right;
}

// For comparing the nodes
class ImplementComparator implements Comparator<HuffmanNode> {
  public int compare(HuffmanNode x, HuffmanNode y) {
    return x.item - y.item;
  }
}

// IMplementing the huffman algorithm
public class Huffman {
  public static void printCode(HuffmanNode root, String s) {
    if (root.left == null && root.right == null && Character.isLetter(root.c)) {

      System.out.println(root.c + ""   |  "" + s);

      return;
    }
    printCode(root.left, s + ""0"");
    printCode(root.right, s + ""1"");
  }

  public static void main(String[] args) {

    int n = 4;
    char[] charArray = { 'A', 'B', 'C', 'D' };
    int[] charfreq = { 5, 1, 6, 3 };

    PriorityQueue<HuffmanNode> q = new PriorityQueue<HuffmanNode>(n, new ImplementComparator());

    for (int i = 0; i < n; i++) {
      HuffmanNode hn = new HuffmanNode();

      hn.c = charArray[i];
      hn.item = charfreq[i];

      hn.left = null;
      hn.right = null;

      q.add(hn);
    }

    HuffmanNode root = null;

    while (q.size() > 1) {

      HuffmanNode x = q.peek();
      q.poll();

      HuffmanNode y = q.peek();
      q.poll();

      HuffmanNode f = new HuffmanNode();

      f.item = x.item + y.item;
      f.c = '-';
      f.left = x;
      f.right = y;
      root = f;

      q.add(f);
    }
    System.out.println("" Char | Huffman code "");
    System.out.println(""--------------------"");
    printCode(root, """");
  }
}"
Huffman Coding Algorithm,"Huffman Coding is a technique of compressing data to reduce its size without losing any of the details. It was first developed by David Huffman. Huffman Coding is generally useful to compress the data in which there are frequently occurring characters. Suppose the string below is to be sent over a network. Each character occupies 8 bits. There are a total of 15 characters in the above string. Thus, a total of 8 * 15 = 120 bits are required to send this string. Using the Huffman Coding technique, we can compress the string to a smaller size. Huffman coding first creates a tree using the frequencies of the character and then generates code for each character.  Once the data is encoded, it has to be decoded. Decoding is done using the same tree. Huffman Coding prevents any ambiguity in the decoding process using the concept of prefix code ie. a code associated with a character should not be present in the prefix of any other code. The tree created above helps in maintaining the property. Huffman coding is done with the help of the following steps. Calculate the frequency of each character in the string.
		
			Frequency of string Sort the characters in increasing order of the frequency. These are stored in a priority queue Q.
		
			Characters sorted according to the frequency Make each unique character as a leaf node. Create an empty node z. Assign the minimum frequency to the left child of z and assign the second minimum frequency to the right child of z. Set the value of the z as the sum of the above two minimum frequencies.
		
			Getting the sum of the least numbers Remove these two minimum frequencies from Q and add the sum into the list of frequencies (* denote the internal nodes in the figure above). Insert node z into the tree. Repeat steps 3 to 5 for all the characters.
		
			Repeat steps 3 to 5 for all the characters.
		
		 

		
			Repeat steps 3 to 5 for all the characters. For each non-leaf node, assign 0 to the left edge and 1 to the right edge.
		
			Assign 0 to the left edge and 1 to the right edge For sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below.   Without encoding, the total size of the string was 120 bits. After encoding the size is reduced to 32 + 15 + 28 = 75. For decoding the code, we can take the code and traverse through the tree to find the character. Let 101 is to be decoded, we can traverse from the root as in the figure below. The time complexity for encoding each unique character based on its frequency is O(nlog n). Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). Thus the overall complexity is O(nlog n). Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc.
	For text and fax transmissions. Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc. For text and fax transmissions.","// Huffman Coding in C

#include <stdio.h>
#include <stdlib.h>

#define MAX_TREE_HT 50

struct MinHNode {
  char item;
  unsigned freq;
  struct MinHNode *left, *right;
};

struct MinHeap {
  unsigned size;
  unsigned capacity;
  struct MinHNode **array;
};

// Create nodes
struct MinHNode *newNode(char item, unsigned freq) {
  struct MinHNode *temp = (struct MinHNode *)malloc(sizeof(struct MinHNode));

  temp->left = temp->right = NULL;
  temp->item = item;
  temp->freq = freq;

  return temp;
}

// Create min heap
struct MinHeap *createMinH(unsigned capacity) {
  struct MinHeap *minHeap = (struct MinHeap *)malloc(sizeof(struct MinHeap));

  minHeap->size = 0;

  minHeap->capacity = capacity;

  minHeap->array = (struct MinHNode **)malloc(minHeap->capacity * sizeof(struct MinHNode *));
  return minHeap;
}

// Function to swap
void swapMinHNode(struct MinHNode **a, struct MinHNode **b) {
  struct MinHNode *t = *a;
  *a = *b;
  *b = t;
}

// Heapify
void minHeapify(struct MinHeap *minHeap, int idx) {
  int smallest = idx;
  int left = 2 * idx + 1;
  int right = 2 * idx + 2;

  if (left < minHeap->size && minHeap->array[left]->freq < minHeap->array[smallest]->freq)
    smallest = left;

  if (right < minHeap->size && minHeap->array[right]->freq < minHeap->array[smallest]->freq)
    smallest = right;

  if (smallest != idx) {
    swapMinHNode(&minHeap->array[smallest], &minHeap->array[idx]);
    minHeapify(minHeap, smallest);
  }
}

// Check if size if 1
int checkSizeOne(struct MinHeap *minHeap) {
  return (minHeap->size == 1);
}

// Extract min
struct MinHNode *extractMin(struct MinHeap *minHeap) {
  struct MinHNode *temp = minHeap->array[0];
  minHeap->array[0] = minHeap->array[minHeap->size - 1];

  --minHeap->size;
  minHeapify(minHeap, 0);

  return temp;
}

// Insertion function
void insertMinHeap(struct MinHeap *minHeap, struct MinHNode *minHeapNode) {
  ++minHeap->size;
  int i = minHeap->size - 1;

  while (i && minHeapNode->freq < minHeap->array[(i - 1) / 2]->freq) {
    minHeap->array[i] = minHeap->array[(i - 1) / 2];
    i = (i - 1) / 2;
  }
  minHeap->array[i] = minHeapNode;
}

void buildMinHeap(struct MinHeap *minHeap) {
  int n = minHeap->size - 1;
  int i;

  for (i = (n - 1) / 2; i >= 0; --i)
    minHeapify(minHeap, i);
}

int isLeaf(struct MinHNode *root) {
  return !(root->left) && !(root->right);
}

struct MinHeap *createAndBuildMinHeap(char item[], int freq[], int size) {
  struct MinHeap *minHeap = createMinH(size);

  for (int i = 0; i < size; ++i)
    minHeap->array[i] = newNode(item[i], freq[i]);

  minHeap->size = size;
  buildMinHeap(minHeap);

  return minHeap;
}

struct MinHNode *buildHuffmanTree(char item[], int freq[], int size) {
  struct MinHNode *left, *right, *top;
  struct MinHeap *minHeap = createAndBuildMinHeap(item, freq, size);

  while (!checkSizeOne(minHeap)) {
    left = extractMin(minHeap);
    right = extractMin(minHeap);

    top = newNode('$', left->freq + right->freq);

    top->left = left;
    top->right = right;

    insertMinHeap(minHeap, top);
  }
  return extractMin(minHeap);
}

void printHCodes(struct MinHNode *root, int arr[], int top) {
  if (root->left) {
    arr[top] = 0;
    printHCodes(root->left, arr, top + 1);
  }
  if (root->right) {
    arr[top] = 1;
    printHCodes(root->right, arr, top + 1);
  }
  if (isLeaf(root)) {
    printf(""  %c   | "", root->item);
    printArray(arr, top);
  }
}

// Wrapper function
void HuffmanCodes(char item[], int freq[], int size) {
  struct MinHNode *root = buildHuffmanTree(item, freq, size);

  int arr[MAX_TREE_HT], top = 0;

  printHCodes(root, arr, top);
}

// Print the array
void printArray(int arr[], int n) {
  int i;
  for (i = 0; i < n; ++i)
    printf(""%d"", arr[i]);

  printf(""\n"");
}

int main() {
  char arr[] = {'A', 'B', 'C', 'D'};
  int freq[] = {5, 1, 6, 3};

  int size = sizeof(arr) / sizeof(arr[0]);

  printf("" Char | Huffman code "");
  printf(""\n--------------------\n"");

  HuffmanCodes(arr, freq, size);
}"
Huffman Coding Algorithm,"Huffman Coding is a technique of compressing data to reduce its size without losing any of the details. It was first developed by David Huffman. Huffman Coding is generally useful to compress the data in which there are frequently occurring characters. Suppose the string below is to be sent over a network. Each character occupies 8 bits. There are a total of 15 characters in the above string. Thus, a total of 8 * 15 = 120 bits are required to send this string. Using the Huffman Coding technique, we can compress the string to a smaller size. Huffman coding first creates a tree using the frequencies of the character and then generates code for each character.  Once the data is encoded, it has to be decoded. Decoding is done using the same tree. Huffman Coding prevents any ambiguity in the decoding process using the concept of prefix code ie. a code associated with a character should not be present in the prefix of any other code. The tree created above helps in maintaining the property. Huffman coding is done with the help of the following steps. Calculate the frequency of each character in the string.
		
			Frequency of string Sort the characters in increasing order of the frequency. These are stored in a priority queue Q.
		
			Characters sorted according to the frequency Make each unique character as a leaf node. Create an empty node z. Assign the minimum frequency to the left child of z and assign the second minimum frequency to the right child of z. Set the value of the z as the sum of the above two minimum frequencies.
		
			Getting the sum of the least numbers Remove these two minimum frequencies from Q and add the sum into the list of frequencies (* denote the internal nodes in the figure above). Insert node z into the tree. Repeat steps 3 to 5 for all the characters.
		
			Repeat steps 3 to 5 for all the characters.
		
		 

		
			Repeat steps 3 to 5 for all the characters. For each non-leaf node, assign 0 to the left edge and 1 to the right edge.
		
			Assign 0 to the left edge and 1 to the right edge For sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below.   Without encoding, the total size of the string was 120 bits. After encoding the size is reduced to 32 + 15 + 28 = 75. For decoding the code, we can take the code and traverse through the tree to find the character. Let 101 is to be decoded, we can traverse from the root as in the figure below. The time complexity for encoding each unique character based on its frequency is O(nlog n). Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). Thus the overall complexity is O(nlog n). Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc.
	For text and fax transmissions. Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc. For text and fax transmissions.","// Huffman Coding in C++

#include <iostream>
using namespace std;

#define MAX_TREE_HT 50

struct MinHNode {
  unsigned freq;
  char item;
  struct MinHNode *left, *right;
};

struct MinH {
  unsigned size;
  unsigned capacity;
  struct MinHNode **array;
};

// Creating Huffman tree node
struct MinHNode *newNode(char item, unsigned freq) {
  struct MinHNode *temp = (struct MinHNode *)malloc(sizeof(struct MinHNode));

  temp->left = temp->right = NULL;
  temp->item = item;
  temp->freq = freq;

  return temp;
}

// Create min heap using given capacity
struct MinH *createMinH(unsigned capacity) {
  struct MinH *minHeap = (struct MinH *)malloc(sizeof(struct MinH));
  minHeap->size = 0;
  minHeap->capacity = capacity;
  minHeap->array = (struct MinHNode **)malloc(minHeap->capacity * sizeof(struct MinHNode *));
  return minHeap;
}

// Print the array
void printArray(int arr[], int n) {
  int i;
  for (i = 0; i < n; ++i)
    cout << arr[i];

  cout << ""\n"";
}

// Swap function
void swapMinHNode(struct MinHNode **a, struct MinHNode **b) {
  struct MinHNode *t = *a;
  *a = *b;
  *b = t;
}

// Heapify
void minHeapify(struct MinH *minHeap, int idx) {
  int smallest = idx;
  int left = 2 * idx + 1;
  int right = 2 * idx + 2;

  if (left < minHeap->size && minHeap->array[left]->freq < minHeap->array[smallest]->freq)
    smallest = left;

  if (right < minHeap->size && minHeap->array[right]->freq < minHeap->array[smallest]->freq)
    smallest = right;

  if (smallest != idx) {
    swapMinHNode(&minHeap->array[smallest],
           &minHeap->array[idx]);
    minHeapify(minHeap, smallest);
  }
}

// Check if size if 1
int checkSizeOne(struct MinH *minHeap) {
  return (minHeap->size == 1);
}

// Extract the min
struct MinHNode *extractMin(struct MinH *minHeap) {
  struct MinHNode *temp = minHeap->array[0];
  minHeap->array[0] = minHeap->array[minHeap->size - 1];

  --minHeap->size;
  minHeapify(minHeap, 0);

  return temp;
}

// Insertion
void insertMinHeap(struct MinH *minHeap, struct MinHNode *minHeapNode) {
  ++minHeap->size;
  int i = minHeap->size - 1;

  while (i && minHeapNode->freq < minHeap->array[(i - 1) / 2]->freq) {
    minHeap->array[i] = minHeap->array[(i - 1) / 2];
    i = (i - 1) / 2;
  }

  minHeap->array[i] = minHeapNode;
}

// BUild min heap
void buildMinHeap(struct MinH *minHeap) {
  int n = minHeap->size - 1;
  int i;

  for (i = (n - 1) / 2; i >= 0; --i)
    minHeapify(minHeap, i);
}

int isLeaf(struct MinHNode *root) {
  return !(root->left) && !(root->right);
}

struct MinH *createAndBuildMinHeap(char item[], int freq[], int size) {
  struct MinH *minHeap = createMinH(size);

  for (int i = 0; i < size; ++i)
    minHeap->array[i] = newNode(item[i], freq[i]);

  minHeap->size = size;
  buildMinHeap(minHeap);

  return minHeap;
}

struct MinHNode *buildHfTree(char item[], int freq[], int size) {
  struct MinHNode *left, *right, *top;
  struct MinH *minHeap = createAndBuildMinHeap(item, freq, size);

  while (!checkSizeOne(minHeap)) {
    left = extractMin(minHeap);
    right = extractMin(minHeap);

    top = newNode('$', left->freq + right->freq);

    top->left = left;
    top->right = right;

    insertMinHeap(minHeap, top);
  }
  return extractMin(minHeap);
}
void printHCodes(struct MinHNode *root, int arr[], int top) {
  if (root->left) {
    arr[top] = 0;
    printHCodes(root->left, arr, top + 1);
  }

  if (root->right) {
    arr[top] = 1;
    printHCodes(root->right, arr, top + 1);
  }
  if (isLeaf(root)) {
    cout << root->item << ""  | "";
    printArray(arr, top);
  }
}

// Wrapper function
void HuffmanCodes(char item[], int freq[], int size) {
  struct MinHNode *root = buildHfTree(item, freq, size);

  int arr[MAX_TREE_HT], top = 0;

  printHCodes(root, arr, top);
}

int main() {
  char arr[] = {'A', 'B', 'C', 'D'};
  int freq[] = {5, 1, 6, 3};

  int size = sizeof(arr) / sizeof(arr[0]);

  cout << ""Char | Huffman code "";
  cout << ""\n----------------------\n"";
  HuffmanCodes(arr, freq, size);
}"
Floyd-Warshall Algorithm,"Floyd-Warshall Algorithm is an algorithm for finding the shortest path between all the pairs of vertices in a weighted graph. This algorithm works for both the directed and undirected weighted graphs. But, it does not work for the graphs with negative cycles (where the sum of the edges in a cycle is negative). A weighted graph is a graph in which each edge has a numerical value associated with it. Floyd-Warhshall algorithm is also called as Floyd's algorithm, Roy-Floyd algorithm, Roy-Warshall algorithm, or WFI algorithm. This algorithm follows the dynamic programming approach to find the shortest paths. Let the given graph be:  Follow the steps below to find the shortest path between all the pairs of vertices. Create a matrix A0 of dimension n*n where n is the number of vertices. The row and the column are indexed as i and j respectively. i and j are the vertices of the graph.
		
		Each cell A[i][j] is filled with the distance from the ith vertex to the jth vertex. If there is no path from ith vertex to jth vertex, the cell is left as infinity.

		
			Fill each cell with the distance between ith and jth vertex Now, create a matrix A1 using matrix A0. The elements in the first column and the first row are left as they are. The remaining cells are filled in the following way.
		
		Let k be the intermediate vertex in the shortest path from source to destination. In this step, k is the first vertex. A[i][j] is filled with (A[i][k] + A[k][j]) if (A[i][j] > A[i][k] + A[k][j]).
		
		That is, if the direct distance from the source to the destination is greater than the path through the vertex k, then the cell is filled with A[i][k] + A[k][j].
		
		In this step, k is vertex 1. We calculate the distance from source vertex to destination vertex through this vertex k.
		
			Calculate the distance from the source vertex to destination vertex through this vertex k
		
		
		For example: For A1[2, 4], the direct distance from vertex 2 to 4 is 4 and the sum of the distance from vertex 2 to 4 through vertex (ie. from vertex 2 to 1 and from vertex 1 to 4) is 7. Since 4 < 7, A0[2, 4] is filled with 4. Similarly, A2 is created using A1. The elements in the second column and the second row are left as they are.
		
		In this step, k is the second vertex (i.e. vertex 2). The remaining steps are the same as in step 2.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 2 Similarly, A3 and A4 is also created.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 3
		
		 

		
			Calculate the distance from the source vertex to destination vertex through this vertex 4 A4 gives the shortest path between each pair of vertices. There are three loops. Each loop has constant complexities. So, the time complexity of the Floyd-Warshall algorithm is O(n3). The space complexity of the Floyd-Warshall algorithm is O(n2). To find the shortest path is a directed graph
	To find the transitive closure of directed graphs
	To find the Inversion of real matrices
	For testing whether an undirected graph is bipartite To find the shortest path is a directed graph To find the transitive closure of directed graphs To find the Inversion of real matrices For testing whether an undirected graph is bipartite","# Floyd Warshall Algorithm in python


# The number of vertices
nV = 4

INF = 999


# Algorithm implementation
def floyd_warshall(G):
    distance = list(map(lambda i: list(map(lambda j: j, i)), G))

    # Adding vertices individually
    for k in range(nV):
        for i in range(nV):
            for j in range(nV):
                distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])
    print_solution(distance)


# Printing the solution
def print_solution(distance):
    for i in range(nV):
        for j in range(nV):
            if(distance[i][j] == INF):
                print(""INF"", end="" "")
            else:
                print(distance[i][j], end=""  "")
        print("" "")


G = [[0, 3, INF, 5],
         [2, 0, INF, 4],
         [INF, 1, 0, INF],
         [INF, INF, 2, 0]]
floyd_warshall(G)"
Floyd-Warshall Algorithm,"Floyd-Warshall Algorithm is an algorithm for finding the shortest path between all the pairs of vertices in a weighted graph. This algorithm works for both the directed and undirected weighted graphs. But, it does not work for the graphs with negative cycles (where the sum of the edges in a cycle is negative). A weighted graph is a graph in which each edge has a numerical value associated with it. Floyd-Warhshall algorithm is also called as Floyd's algorithm, Roy-Floyd algorithm, Roy-Warshall algorithm, or WFI algorithm. This algorithm follows the dynamic programming approach to find the shortest paths. Let the given graph be:  Follow the steps below to find the shortest path between all the pairs of vertices. Create a matrix A0 of dimension n*n where n is the number of vertices. The row and the column are indexed as i and j respectively. i and j are the vertices of the graph.
		
		Each cell A[i][j] is filled with the distance from the ith vertex to the jth vertex. If there is no path from ith vertex to jth vertex, the cell is left as infinity.

		
			Fill each cell with the distance between ith and jth vertex Now, create a matrix A1 using matrix A0. The elements in the first column and the first row are left as they are. The remaining cells are filled in the following way.
		
		Let k be the intermediate vertex in the shortest path from source to destination. In this step, k is the first vertex. A[i][j] is filled with (A[i][k] + A[k][j]) if (A[i][j] > A[i][k] + A[k][j]).
		
		That is, if the direct distance from the source to the destination is greater than the path through the vertex k, then the cell is filled with A[i][k] + A[k][j].
		
		In this step, k is vertex 1. We calculate the distance from source vertex to destination vertex through this vertex k.
		
			Calculate the distance from the source vertex to destination vertex through this vertex k
		
		
		For example: For A1[2, 4], the direct distance from vertex 2 to 4 is 4 and the sum of the distance from vertex 2 to 4 through vertex (ie. from vertex 2 to 1 and from vertex 1 to 4) is 7. Since 4 < 7, A0[2, 4] is filled with 4. Similarly, A2 is created using A1. The elements in the second column and the second row are left as they are.
		
		In this step, k is the second vertex (i.e. vertex 2). The remaining steps are the same as in step 2.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 2 Similarly, A3 and A4 is also created.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 3
		
		 

		
			Calculate the distance from the source vertex to destination vertex through this vertex 4 A4 gives the shortest path between each pair of vertices. There are three loops. Each loop has constant complexities. So, the time complexity of the Floyd-Warshall algorithm is O(n3). The space complexity of the Floyd-Warshall algorithm is O(n2). To find the shortest path is a directed graph
	To find the transitive closure of directed graphs
	To find the Inversion of real matrices
	For testing whether an undirected graph is bipartite To find the shortest path is a directed graph To find the transitive closure of directed graphs To find the Inversion of real matrices For testing whether an undirected graph is bipartite","// Floyd Warshall Algorithm in Java

class FloydWarshall {
  final static int INF = 9999, nV = 4;

  // Implementing floyd warshall algorithm
  void floydWarshall(int graph[][]) {
    int matrix[][] = new int[nV][nV];
    int i, j, k;

    for (i = 0; i < nV; i++)
      for (j = 0; j < nV; j++)
        matrix[i][j] = graph[i][j];

    // Adding vertices individually
    for (k = 0; k < nV; k++) {
      for (i = 0; i < nV; i++) {
        for (j = 0; j < nV; j++) {
          if (matrix[i][k] + matrix[k][j] < matrix[i][j])
            matrix[i][j] = matrix[i][k] + matrix[k][j];
        }
      }
    }
    printMatrix(matrix);
  }

  void printMatrix(int matrix[][]) {
    for (int i = 0; i < nV; ++i) {
      for (int j = 0; j < nV; ++j) {
        if (matrix[i][j] == INF)
          System.out.print(""INF "");
        else
          System.out.print(matrix[i][j] + ""  "");
      }
      System.out.println();
    }
  }

  public static void main(String[] args) {
    int graph[][] = { { 0, 3, INF, 5 }, { 2, 0, INF, 4 }, { INF, 1, 0, INF }, { INF, INF, 2, 0 } };
    FloydWarshall a = new FloydWarshall();
    a.floydWarshall(graph);
  }
}"
Floyd-Warshall Algorithm,"Floyd-Warshall Algorithm is an algorithm for finding the shortest path between all the pairs of vertices in a weighted graph. This algorithm works for both the directed and undirected weighted graphs. But, it does not work for the graphs with negative cycles (where the sum of the edges in a cycle is negative). A weighted graph is a graph in which each edge has a numerical value associated with it. Floyd-Warhshall algorithm is also called as Floyd's algorithm, Roy-Floyd algorithm, Roy-Warshall algorithm, or WFI algorithm. This algorithm follows the dynamic programming approach to find the shortest paths. Let the given graph be:  Follow the steps below to find the shortest path between all the pairs of vertices. Create a matrix A0 of dimension n*n where n is the number of vertices. The row and the column are indexed as i and j respectively. i and j are the vertices of the graph.
		
		Each cell A[i][j] is filled with the distance from the ith vertex to the jth vertex. If there is no path from ith vertex to jth vertex, the cell is left as infinity.

		
			Fill each cell with the distance between ith and jth vertex Now, create a matrix A1 using matrix A0. The elements in the first column and the first row are left as they are. The remaining cells are filled in the following way.
		
		Let k be the intermediate vertex in the shortest path from source to destination. In this step, k is the first vertex. A[i][j] is filled with (A[i][k] + A[k][j]) if (A[i][j] > A[i][k] + A[k][j]).
		
		That is, if the direct distance from the source to the destination is greater than the path through the vertex k, then the cell is filled with A[i][k] + A[k][j].
		
		In this step, k is vertex 1. We calculate the distance from source vertex to destination vertex through this vertex k.
		
			Calculate the distance from the source vertex to destination vertex through this vertex k
		
		
		For example: For A1[2, 4], the direct distance from vertex 2 to 4 is 4 and the sum of the distance from vertex 2 to 4 through vertex (ie. from vertex 2 to 1 and from vertex 1 to 4) is 7. Since 4 < 7, A0[2, 4] is filled with 4. Similarly, A2 is created using A1. The elements in the second column and the second row are left as they are.
		
		In this step, k is the second vertex (i.e. vertex 2). The remaining steps are the same as in step 2.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 2 Similarly, A3 and A4 is also created.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 3
		
		 

		
			Calculate the distance from the source vertex to destination vertex through this vertex 4 A4 gives the shortest path between each pair of vertices. There are three loops. Each loop has constant complexities. So, the time complexity of the Floyd-Warshall algorithm is O(n3). The space complexity of the Floyd-Warshall algorithm is O(n2). To find the shortest path is a directed graph
	To find the transitive closure of directed graphs
	To find the Inversion of real matrices
	For testing whether an undirected graph is bipartite To find the shortest path is a directed graph To find the transitive closure of directed graphs To find the Inversion of real matrices For testing whether an undirected graph is bipartite","// Floyd-Warshall Algorithm in C

#include <stdio.h>

// defining the number of vertices
#define nV 4

#define INF 999

void printMatrix(int matrix[][nV]);

// Implementing floyd warshall algorithm
void floydWarshall(int graph[][nV]) {
  int matrix[nV][nV], i, j, k;

  for (i = 0; i < nV; i++)
    for (j = 0; j < nV; j++)
      matrix[i][j] = graph[i][j];

  // Adding vertices individually
  for (k = 0; k < nV; k++) {
    for (i = 0; i < nV; i++) {
      for (j = 0; j < nV; j++) {
        if (matrix[i][k] + matrix[k][j] < matrix[i][j])
          matrix[i][j] = matrix[i][k] + matrix[k][j];
      }
    }
  }
  printMatrix(matrix);
}

void printMatrix(int matrix[][nV]) {
  for (int i = 0; i < nV; i++) {
    for (int j = 0; j < nV; j++) {
      if (matrix[i][j] == INF)
        printf(""%4s"", ""INF"");
      else
        printf(""%4d"", matrix[i][j]);
    }
    printf(""\n"");
  }
}

int main() {
  int graph[nV][nV] = {{0, 3, INF, 5},
             {2, 0, INF, 4},
             {INF, 1, 0, INF},
             {INF, INF, 2, 0}};
  floydWarshall(graph);
}"
Floyd-Warshall Algorithm,"Floyd-Warshall Algorithm is an algorithm for finding the shortest path between all the pairs of vertices in a weighted graph. This algorithm works for both the directed and undirected weighted graphs. But, it does not work for the graphs with negative cycles (where the sum of the edges in a cycle is negative). A weighted graph is a graph in which each edge has a numerical value associated with it. Floyd-Warhshall algorithm is also called as Floyd's algorithm, Roy-Floyd algorithm, Roy-Warshall algorithm, or WFI algorithm. This algorithm follows the dynamic programming approach to find the shortest paths. Let the given graph be:  Follow the steps below to find the shortest path between all the pairs of vertices. Create a matrix A0 of dimension n*n where n is the number of vertices. The row and the column are indexed as i and j respectively. i and j are the vertices of the graph.
		
		Each cell A[i][j] is filled with the distance from the ith vertex to the jth vertex. If there is no path from ith vertex to jth vertex, the cell is left as infinity.

		
			Fill each cell with the distance between ith and jth vertex Now, create a matrix A1 using matrix A0. The elements in the first column and the first row are left as they are. The remaining cells are filled in the following way.
		
		Let k be the intermediate vertex in the shortest path from source to destination. In this step, k is the first vertex. A[i][j] is filled with (A[i][k] + A[k][j]) if (A[i][j] > A[i][k] + A[k][j]).
		
		That is, if the direct distance from the source to the destination is greater than the path through the vertex k, then the cell is filled with A[i][k] + A[k][j].
		
		In this step, k is vertex 1. We calculate the distance from source vertex to destination vertex through this vertex k.
		
			Calculate the distance from the source vertex to destination vertex through this vertex k
		
		
		For example: For A1[2, 4], the direct distance from vertex 2 to 4 is 4 and the sum of the distance from vertex 2 to 4 through vertex (ie. from vertex 2 to 1 and from vertex 1 to 4) is 7. Since 4 < 7, A0[2, 4] is filled with 4. Similarly, A2 is created using A1. The elements in the second column and the second row are left as they are.
		
		In this step, k is the second vertex (i.e. vertex 2). The remaining steps are the same as in step 2.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 2 Similarly, A3 and A4 is also created.
		
			Calculate the distance from the source vertex to destination vertex through this vertex 3
		
		 

		
			Calculate the distance from the source vertex to destination vertex through this vertex 4 A4 gives the shortest path between each pair of vertices. There are three loops. Each loop has constant complexities. So, the time complexity of the Floyd-Warshall algorithm is O(n3). The space complexity of the Floyd-Warshall algorithm is O(n2). To find the shortest path is a directed graph
	To find the transitive closure of directed graphs
	To find the Inversion of real matrices
	For testing whether an undirected graph is bipartite To find the shortest path is a directed graph To find the transitive closure of directed graphs To find the Inversion of real matrices For testing whether an undirected graph is bipartite","// Floyd-Warshall Algorithm in C++

#include <iostream>
using namespace std;

// defining the number of vertices
#define nV 4

#define INF 999

void printMatrix(int matrix[][nV]);

// Implementing floyd warshall algorithm
void floydWarshall(int graph[][nV]) {
  int matrix[nV][nV], i, j, k;

  for (i = 0; i < nV; i++)
    for (j = 0; j < nV; j++)
      matrix[i][j] = graph[i][j];

  // Adding vertices individually
  for (k = 0; k < nV; k++) {
    for (i = 0; i < nV; i++) {
      for (j = 0; j < nV; j++) {
        if (matrix[i][k] + matrix[k][j] < matrix[i][j])
          matrix[i][j] = matrix[i][k] + matrix[k][j];
      }
    }
  }
  printMatrix(matrix);
}

void printMatrix(int matrix[][nV]) {
  for (int i = 0; i < nV; i++) {
    for (int j = 0; j < nV; j++) {
      if (matrix[i][j] == INF)
        printf(""%4s"", ""INF"");
      else
        printf(""%4d"", matrix[i][j]);
    }
    printf(""\n"");
  }
}

int main() {
  int graph[nV][nV] = {{0, 3, INF, 5},
             {2, 0, INF, 4},
             {INF, 1, 0, INF},
             {INF, INF, 2, 0}};
  floydWarshall(graph);
}"
Longest Common Subsequence,"The longest common subsequence (LCS) is defined as the longest subsequence that is common to all the given sequences, provided that the elements of the subsequence are not required to occupy consecutive positions within the original sequences. If S1 and S2 are the two given sequences then, Z is the common subsequence of S1 and S2 if Z is a subsequence of both S1 and S2. Furthermore, Z must be a strictly increasing sequence of the indices of both S1 and S2. In a strictly increasing sequence, the indices of the elements chosen from the original sequences must be in ascending order in Z. If Then, {A, D, B} cannot be a subsequence of S1 as the order of the elements is not the same (ie. not strictly increasing sequence). Let us understand LCS with an example.  If Then, common subsequences are {B, C}, {C, D, A, C}, {D, A, C}, {A, A, C}, {A, C}, {C, D}, ... Among these subsequences, {C, D, A, C} is the longest common subsequence. We are going to find this longest common subsequence using dynamic programming. Before proceeding further, if you do not already know about dynamic programming, please go through dynamic programming. Let us take two sequences: The following steps are followed for finding the longest common subsequence. Create a table of dimension n+1*m+1 where n and m are the lengths of X and Y respectively. The first row and the first column are filled with zeros.

		
			Initialise a table Fill each cell of the table using the following logic. If the character correspoding to the current row and current column are matching, then fill the current cell by adding one to the diagonal element. Point an arrow to the diagonal cell. Else take the maximum value from the previous column and previous row element for filling the current cell. Point an arrow to the cell with maximum value. If they are equal, point to any of them.
		
			Fill the values Step 2 is repeated until the table is filled.
		
			Fill all the values The value in the last row and the last column is the length of the longest common subsequence.
		
			The bottom right corner is the length of the LCS In order to find the longest common subsequence, start from the last element and follow the direction of the arrow. The elements corresponding to () symbol form the longest common subsequence.
		
			Create a path according to the arrows Thus, the longest common subsequence is CA. How is a dynamic programming algorithm more efficient than the recursive algorithm while solving an LCS problem? The method of dynamic programming reduces the number of function calls. It stores the result of each function call so that it can be used in future calls without the need for redundant calls. In the above dynamic algorithm, the results obtained from each comparison between elements of X and the elements of Y are stored in a table so that they can be used in future computations. So, the time taken by a dynamic approach is the time taken to fill the table (ie. O(mn)). Whereas, the recursion algorithm has the complexity of 2max(m, n). in compressing genome resequencing data to authenticate users within their mobile phone through in-air signatures","# The longest common subsequence in Python


# Function to find lcs_algo
def lcs_algo(S1, S2, m, n):
    L = [[0 for x in range(n+1)] for x in range(m+1)]

    # Building the mtrix in bottom-up way
    for i in range(m+1):
        for j in range(n+1):
            if i == 0 or j == 0:
                L[i][j] = 0
            elif S1[i-1] == S2[j-1]:
                L[i][j] = L[i-1][j-1] + 1
            else:
                L[i][j] = max(L[i-1][j], L[i][j-1])

    index = L[m][n]

    lcs_algo = [""""] * (index+1)
    lcs_algo[index] = """"

    i = m
    j = n
    while i > 0 and j > 0:

        if S1[i-1] == S2[j-1]:
            lcs_algo[index-1] = S1[i-1]
            i -= 1
            j -= 1
            index -= 1

        elif L[i-1][j] > L[i][j-1]:
            i -= 1
        else:
            j -= 1
            
    # Printing the sub sequences
    print(""S1 : "" + S1 + ""\nS2 : "" + S2)
    print(""LCS: "" + """".join(lcs_algo))


S1 = ""ACADB""
S2 = ""CBDA""
m = len(S1)
n = len(S2)
lcs_algo(S1, S2, m, n)"
Longest Common Subsequence,"The longest common subsequence (LCS) is defined as the longest subsequence that is common to all the given sequences, provided that the elements of the subsequence are not required to occupy consecutive positions within the original sequences. If S1 and S2 are the two given sequences then, Z is the common subsequence of S1 and S2 if Z is a subsequence of both S1 and S2. Furthermore, Z must be a strictly increasing sequence of the indices of both S1 and S2. In a strictly increasing sequence, the indices of the elements chosen from the original sequences must be in ascending order in Z. If Then, {A, D, B} cannot be a subsequence of S1 as the order of the elements is not the same (ie. not strictly increasing sequence). Let us understand LCS with an example.  If Then, common subsequences are {B, C}, {C, D, A, C}, {D, A, C}, {A, A, C}, {A, C}, {C, D}, ... Among these subsequences, {C, D, A, C} is the longest common subsequence. We are going to find this longest common subsequence using dynamic programming. Before proceeding further, if you do not already know about dynamic programming, please go through dynamic programming. Let us take two sequences: The following steps are followed for finding the longest common subsequence. Create a table of dimension n+1*m+1 where n and m are the lengths of X and Y respectively. The first row and the first column are filled with zeros.

		
			Initialise a table Fill each cell of the table using the following logic. If the character correspoding to the current row and current column are matching, then fill the current cell by adding one to the diagonal element. Point an arrow to the diagonal cell. Else take the maximum value from the previous column and previous row element for filling the current cell. Point an arrow to the cell with maximum value. If they are equal, point to any of them.
		
			Fill the values Step 2 is repeated until the table is filled.
		
			Fill all the values The value in the last row and the last column is the length of the longest common subsequence.
		
			The bottom right corner is the length of the LCS In order to find the longest common subsequence, start from the last element and follow the direction of the arrow. The elements corresponding to () symbol form the longest common subsequence.
		
			Create a path according to the arrows Thus, the longest common subsequence is CA. How is a dynamic programming algorithm more efficient than the recursive algorithm while solving an LCS problem? The method of dynamic programming reduces the number of function calls. It stores the result of each function call so that it can be used in future calls without the need for redundant calls. In the above dynamic algorithm, the results obtained from each comparison between elements of X and the elements of Y are stored in a table so that they can be used in future computations. So, the time taken by a dynamic approach is the time taken to fill the table (ie. O(mn)). Whereas, the recursion algorithm has the complexity of 2max(m, n). in compressing genome resequencing data to authenticate users within their mobile phone through in-air signatures","// The longest common subsequence in Java

class LCS_ALGO {
  static void lcs(String S1, String S2, int m, int n) {
    int[][] LCS_table = new int[m + 1][n + 1];

    // Building the mtrix in bottom-up way
    for (int i = 0; i <= m; i++) {
      for (int j = 0; j <= n; j++) {
        if (i == 0 || j == 0)
          LCS_table[i][j] = 0;
        else if (S1.charAt(i - 1) == S2.charAt(j - 1))
          LCS_table[i][j] = LCS_table[i - 1][j - 1] + 1;
        else
          LCS_table[i][j] = Math.max(LCS_table[i - 1][j], LCS_table[i][j - 1]);
      }
    }

    int index = LCS_table[m][n];
    int temp = index;

    char[] lcs = new char[index + 1];
    lcs[index] = '\0';

    int i = m, j = n;
    while (i > 0 && j > 0) {
      if (S1.charAt(i - 1) == S2.charAt(j - 1)) {
        lcs[index - 1] = S1.charAt(i - 1);

        i--;
        j--;
        index--;
      }

      else if (LCS_table[i - 1][j] > LCS_table[i][j - 1])
        i--;
      else
        j--;
    }

    // Printing the sub sequences
    System.out.print(""S1 : "" + S1 + ""\nS2 : "" + S2 + ""\nLCS: "");
    for (int k = 0; k <= temp; k++)
      System.out.print(lcs[k]);
    System.out.println("""");
  }

  public static void main(String[] args) {
    String S1 = ""ACADB"";
    String S2 = ""CBDA"";
    int m = S1.length();
    int n = S2.length();
    lcs(S1, S2, m, n);
  }
}"
Longest Common Subsequence,"The longest common subsequence (LCS) is defined as the longest subsequence that is common to all the given sequences, provided that the elements of the subsequence are not required to occupy consecutive positions within the original sequences. If S1 and S2 are the two given sequences then, Z is the common subsequence of S1 and S2 if Z is a subsequence of both S1 and S2. Furthermore, Z must be a strictly increasing sequence of the indices of both S1 and S2. In a strictly increasing sequence, the indices of the elements chosen from the original sequences must be in ascending order in Z. If Then, {A, D, B} cannot be a subsequence of S1 as the order of the elements is not the same (ie. not strictly increasing sequence). Let us understand LCS with an example.  If Then, common subsequences are {B, C}, {C, D, A, C}, {D, A, C}, {A, A, C}, {A, C}, {C, D}, ... Among these subsequences, {C, D, A, C} is the longest common subsequence. We are going to find this longest common subsequence using dynamic programming. Before proceeding further, if you do not already know about dynamic programming, please go through dynamic programming. Let us take two sequences: The following steps are followed for finding the longest common subsequence. Create a table of dimension n+1*m+1 where n and m are the lengths of X and Y respectively. The first row and the first column are filled with zeros.

		
			Initialise a table Fill each cell of the table using the following logic. If the character correspoding to the current row and current column are matching, then fill the current cell by adding one to the diagonal element. Point an arrow to the diagonal cell. Else take the maximum value from the previous column and previous row element for filling the current cell. Point an arrow to the cell with maximum value. If they are equal, point to any of them.
		
			Fill the values Step 2 is repeated until the table is filled.
		
			Fill all the values The value in the last row and the last column is the length of the longest common subsequence.
		
			The bottom right corner is the length of the LCS In order to find the longest common subsequence, start from the last element and follow the direction of the arrow. The elements corresponding to () symbol form the longest common subsequence.
		
			Create a path according to the arrows Thus, the longest common subsequence is CA. How is a dynamic programming algorithm more efficient than the recursive algorithm while solving an LCS problem? The method of dynamic programming reduces the number of function calls. It stores the result of each function call so that it can be used in future calls without the need for redundant calls. In the above dynamic algorithm, the results obtained from each comparison between elements of X and the elements of Y are stored in a table so that they can be used in future computations. So, the time taken by a dynamic approach is the time taken to fill the table (ie. O(mn)). Whereas, the recursion algorithm has the complexity of 2max(m, n). in compressing genome resequencing data to authenticate users within their mobile phone through in-air signatures","// The longest common subsequence in C

#include <stdio.h>
#include <string.h>

int i, j, m, n, LCS_table[20][20];
char S1[20] = ""ACADB"", S2[20] = ""CBDA"", b[20][20];

void lcsAlgo() {
  m = strlen(S1);
  n = strlen(S2);

  // Filling 0's in the matrix
  for (i = 0; i <= m; i++)
    LCS_table[i][0] = 0;
  for (i = 0; i <= n; i++)
    LCS_table[0][i] = 0;

  // Building the mtrix in bottom-up way
  for (i = 1; i <= m; i++)
    for (j = 1; j <= n; j++) {
      if (S1[i - 1] == S2[j - 1]) {
        LCS_table[i][j] = LCS_table[i - 1][j - 1] + 1;
      } else if (LCS_table[i - 1][j] >= LCS_table[i][j - 1]) {
        LCS_table[i][j] = LCS_table[i - 1][j];
      } else {
        LCS_table[i][j] = LCS_table[i][j - 1];
      }
    }

  int index = LCS_table[m][n];
  char lcsAlgo[index + 1];
  lcsAlgo[index] = '\0';

  int i = m, j = n;
  while (i > 0 && j > 0) {
    if (S1[i - 1] == S2[j - 1]) {
      lcsAlgo[index - 1] = S1[i - 1];
      i--;
      j--;
      index--;
    }

    else if (LCS_table[i - 1][j] > LCS_table[i][j - 1])
      i--;
    else
      j--;
  }

  // Printing the sub sequences
  printf(""S1 : %s \nS2 : %s \n"", S1, S2);
  printf(""LCS: %s"", lcsAlgo);
}

int main() {
  lcsAlgo();
  printf(""\n"");
}"
Longest Common Subsequence,"The longest common subsequence (LCS) is defined as the longest subsequence that is common to all the given sequences, provided that the elements of the subsequence are not required to occupy consecutive positions within the original sequences. If S1 and S2 are the two given sequences then, Z is the common subsequence of S1 and S2 if Z is a subsequence of both S1 and S2. Furthermore, Z must be a strictly increasing sequence of the indices of both S1 and S2. In a strictly increasing sequence, the indices of the elements chosen from the original sequences must be in ascending order in Z. If Then, {A, D, B} cannot be a subsequence of S1 as the order of the elements is not the same (ie. not strictly increasing sequence). Let us understand LCS with an example.  If Then, common subsequences are {B, C}, {C, D, A, C}, {D, A, C}, {A, A, C}, {A, C}, {C, D}, ... Among these subsequences, {C, D, A, C} is the longest common subsequence. We are going to find this longest common subsequence using dynamic programming. Before proceeding further, if you do not already know about dynamic programming, please go through dynamic programming. Let us take two sequences: The following steps are followed for finding the longest common subsequence. Create a table of dimension n+1*m+1 where n and m are the lengths of X and Y respectively. The first row and the first column are filled with zeros.

		
			Initialise a table Fill each cell of the table using the following logic. If the character correspoding to the current row and current column are matching, then fill the current cell by adding one to the diagonal element. Point an arrow to the diagonal cell. Else take the maximum value from the previous column and previous row element for filling the current cell. Point an arrow to the cell with maximum value. If they are equal, point to any of them.
		
			Fill the values Step 2 is repeated until the table is filled.
		
			Fill all the values The value in the last row and the last column is the length of the longest common subsequence.
		
			The bottom right corner is the length of the LCS In order to find the longest common subsequence, start from the last element and follow the direction of the arrow. The elements corresponding to () symbol form the longest common subsequence.
		
			Create a path according to the arrows Thus, the longest common subsequence is CA. How is a dynamic programming algorithm more efficient than the recursive algorithm while solving an LCS problem? The method of dynamic programming reduces the number of function calls. It stores the result of each function call so that it can be used in future calls without the need for redundant calls. In the above dynamic algorithm, the results obtained from each comparison between elements of X and the elements of Y are stored in a table so that they can be used in future computations. So, the time taken by a dynamic approach is the time taken to fill the table (ie. O(mn)). Whereas, the recursion algorithm has the complexity of 2max(m, n). in compressing genome resequencing data to authenticate users within their mobile phone through in-air signatures","// The longest common subsequence in C++

#include <iostream>
using namespace std;

void lcsAlgo(char *S1, char *S2, int m, int n) {
  int LCS_table[m + 1][n + 1];


  // Building the mtrix in bottom-up way
  for (int i = 0; i <= m; i++) {
    for (int j = 0; j <= n; j++) {
      if (i == 0 || j == 0)
        LCS_table[i][j] = 0;
      else if (S1[i - 1] == S2[j - 1])
        LCS_table[i][j] = LCS_table[i - 1][j - 1] + 1;
      else
        LCS_table[i][j] = max(LCS_table[i - 1][j], LCS_table[i][j - 1]);
    }
  }

  int index = LCS_table[m][n];
  char lcsAlgo[index + 1];
  lcsAlgo[index] = '\0';

  int i = m, j = n;
  while (i > 0 && j > 0) {
    if (S1[i - 1] == S2[j - 1]) {
      lcsAlgo[index - 1] = S1[i - 1];
      i--;
      j--;
      index--;
    }

    else if (LCS_table[i - 1][j] > LCS_table[i][j - 1])
      i--;
    else
      j--;
  }
  
  // Printing the sub sequences
  cout << ""S1 : "" << S1 << ""\nS2 : "" << S2 << ""\nLCS: "" << lcsAlgo << ""\n"";
}

int main() {
  char S1[] = ""ACADB"";
  char S2[] = ""CBDA"";
  int m = strlen(S1);
  int n = strlen(S2);

  lcsAlgo(S1, S2, m, n);
}"
Rabin-Karp Algorithm,"Rabin-Karp algorithm is an algorithm used for searching/matching patterns in the text using a hash function. Unlike Naive string matching algorithm, it does not travel through every character in the initial phase rather it filters the characters that do not match and then performs the comparison. A hash function is a tool to map a larger input value to a smaller output value. This output value is called the hash value. A sequence of characters is taken and checked for the possibility of the presence of the required string. If the possibility is found then, character matching is performed. Let us understand the algorithm with the following steps: Let the text be:
		
			Text
		
		
		And the string to be searched in the above text be:
		
			Pattern Let us assign a numerical value(v)/weight for the characters we will be using in the problem. Here, we have taken first ten alphabets only (i.e. A to J).
		
			Text Weights n be the length of the pattern and m be the length of the text. Here, m = 10 and n = 3.
		Let d be the number of characters in the input set. Here, we have taken input set {A, B, C, ..., J}. So, d = 10. You can assume any suitable value for d. Let us calculate the hash value of the pattern.
		
			Hash value of text In the calculation above, choose a prime number (here, 13) in such a way that we can perform all the calculations with single-precision arithmetic.  The reason for calculating the modulus is given below. Calculate the hash value for the text-window of size m. Compare the hash value of the pattern with the hash value of the text. If they match then, character-matching is performed.
		In the above examples, the hash value of the first window (i.e. t) matches with p so, go for character matching between ABC and CDD. Since they do not match so, go for the next window. We calculate the hash value of the next window by subtracting the first term and adding the next term as shown below. In order to optimize this process, we make use of the previous hash value in the following way. For BCC, t = 12 (≠6). Therefore, go for the next window.
		After a few searches, we will get the match for the window CDA in the text.

		
			Hash value of different windows When the hash value of the pattern matches with the hash value of a window of the text but the window is not the actual pattern then it is called a spurious hit. Spurious hit increases the time complexity of the algorithm. In order to minimize spurious hit, we use modulus. It greatly reduces the spurious hit. The average case and best case complexity of Rabin-Karp algorithm is O(m + n) and the worst case complexity is O(mn). The worst-case complexity occurs when spurious hits occur a number for all the windows. For pattern matching
	For searching string in a bigger text For pattern matching For searching string in a bigger text","# Rabin-Karp algorithm in python


d = 10

def search(pattern, text, q):
    m = len(pattern)
    n = len(text)
    p = 0
    t = 0
    h = 1
    i = 0
    j = 0

    for i in range(m-1):
        h = (h*d) % q

    # Calculate hash value for pattern and text
    for i in range(m):
        p = (d*p + ord(pattern[i])) % q
        t = (d*t + ord(text[i])) % q

    # Find the match
    for i in range(n-m+1):
        if p == t:
            for j in range(m):
                if text[i+j] != pattern[j]:
                    break

            j += 1
            if j == m:
                print(""Pattern is found at position: "" + str(i+1))

        if i < n-m:
            t = (d*(t-ord(text[i])*h) + ord(text[i+m])) % q

            if t < 0:
                t = t+q


text = ""ABCCDDAEFG""
pattern = ""CDD""
q = 13
search(pattern, text, q)"
Rabin-Karp Algorithm,"Rabin-Karp algorithm is an algorithm used for searching/matching patterns in the text using a hash function. Unlike Naive string matching algorithm, it does not travel through every character in the initial phase rather it filters the characters that do not match and then performs the comparison. A hash function is a tool to map a larger input value to a smaller output value. This output value is called the hash value. A sequence of characters is taken and checked for the possibility of the presence of the required string. If the possibility is found then, character matching is performed. Let us understand the algorithm with the following steps: Let the text be:
		
			Text
		
		
		And the string to be searched in the above text be:
		
			Pattern Let us assign a numerical value(v)/weight for the characters we will be using in the problem. Here, we have taken first ten alphabets only (i.e. A to J).
		
			Text Weights n be the length of the pattern and m be the length of the text. Here, m = 10 and n = 3.
		Let d be the number of characters in the input set. Here, we have taken input set {A, B, C, ..., J}. So, d = 10. You can assume any suitable value for d. Let us calculate the hash value of the pattern.
		
			Hash value of text In the calculation above, choose a prime number (here, 13) in such a way that we can perform all the calculations with single-precision arithmetic.  The reason for calculating the modulus is given below. Calculate the hash value for the text-window of size m. Compare the hash value of the pattern with the hash value of the text. If they match then, character-matching is performed.
		In the above examples, the hash value of the first window (i.e. t) matches with p so, go for character matching between ABC and CDD. Since they do not match so, go for the next window. We calculate the hash value of the next window by subtracting the first term and adding the next term as shown below. In order to optimize this process, we make use of the previous hash value in the following way. For BCC, t = 12 (≠6). Therefore, go for the next window.
		After a few searches, we will get the match for the window CDA in the text.

		
			Hash value of different windows When the hash value of the pattern matches with the hash value of a window of the text but the window is not the actual pattern then it is called a spurious hit. Spurious hit increases the time complexity of the algorithm. In order to minimize spurious hit, we use modulus. It greatly reduces the spurious hit. The average case and best case complexity of Rabin-Karp algorithm is O(m + n) and the worst case complexity is O(mn). The worst-case complexity occurs when spurious hits occur a number for all the windows. For pattern matching
	For searching string in a bigger text For pattern matching For searching string in a bigger text","// Rabin-Karp algorithm in Java

public class RabinKarp {
  public final static int d = 10;

  static void search(String pattern, String txt, int q) {
    int m = pattern.length();
    int n = txt.length();
    int i, j;
    int p = 0;
    int t = 0;
    int h = 1;

    for (i = 0; i < m - 1; i++)
      h = (h * d) % q;

    // Calculate hash value for pattern and text
    for (i = 0; i < m; i++) {
      p = (d * p + pattern.charAt(i)) % q;
      t = (d * t + txt.charAt(i)) % q;
    }

    // Find the match
    for (i = 0; i <= n - m; i++) {
      if (p == t) {
        for (j = 0; j < m; j++) {
          if (txt.charAt(i + j) != pattern.charAt(j))
            break;
        }

        if (j == m)
          System.out.println(""Pattern is found at position: "" + (i + 1));
      }

      if (i < n - m) {
        t = (d * (t - txt.charAt(i) * h) + txt.charAt(i + m)) % q;
        if (t < 0)
          t = (t + q);
      }
    }
  }

  public static void main(String[] args) {
    String txt = ""ABCCDDAEFG"";
    String pattern = ""CDD"";
    int q = 13;
    search(pattern, txt, q);
  }
}"
Rabin-Karp Algorithm,"Rabin-Karp algorithm is an algorithm used for searching/matching patterns in the text using a hash function. Unlike Naive string matching algorithm, it does not travel through every character in the initial phase rather it filters the characters that do not match and then performs the comparison. A hash function is a tool to map a larger input value to a smaller output value. This output value is called the hash value. A sequence of characters is taken and checked for the possibility of the presence of the required string. If the possibility is found then, character matching is performed. Let us understand the algorithm with the following steps: Let the text be:
		
			Text
		
		
		And the string to be searched in the above text be:
		
			Pattern Let us assign a numerical value(v)/weight for the characters we will be using in the problem. Here, we have taken first ten alphabets only (i.e. A to J).
		
			Text Weights n be the length of the pattern and m be the length of the text. Here, m = 10 and n = 3.
		Let d be the number of characters in the input set. Here, we have taken input set {A, B, C, ..., J}. So, d = 10. You can assume any suitable value for d. Let us calculate the hash value of the pattern.
		
			Hash value of text In the calculation above, choose a prime number (here, 13) in such a way that we can perform all the calculations with single-precision arithmetic.  The reason for calculating the modulus is given below. Calculate the hash value for the text-window of size m. Compare the hash value of the pattern with the hash value of the text. If they match then, character-matching is performed.
		In the above examples, the hash value of the first window (i.e. t) matches with p so, go for character matching between ABC and CDD. Since they do not match so, go for the next window. We calculate the hash value of the next window by subtracting the first term and adding the next term as shown below. In order to optimize this process, we make use of the previous hash value in the following way. For BCC, t = 12 (≠6). Therefore, go for the next window.
		After a few searches, we will get the match for the window CDA in the text.

		
			Hash value of different windows When the hash value of the pattern matches with the hash value of a window of the text but the window is not the actual pattern then it is called a spurious hit. Spurious hit increases the time complexity of the algorithm. In order to minimize spurious hit, we use modulus. It greatly reduces the spurious hit. The average case and best case complexity of Rabin-Karp algorithm is O(m + n) and the worst case complexity is O(mn). The worst-case complexity occurs when spurious hits occur a number for all the windows. For pattern matching
	For searching string in a bigger text For pattern matching For searching string in a bigger text","// Rabin-Karp algorithm in C

#include <stdio.h>
#include <string.h>

#define d 10

void rabinKarp(char pattern[], char text[], int q) {
  int m = strlen(pattern);
  int n = strlen(text);
  int i, j;
  int p = 0;
  int t = 0;
  int h = 1;

  for (i = 0; i < m - 1; i++)
    h = (h * d) % q;

  // Calculate hash value for pattern and text
  for (i = 0; i < m; i++) {
    p = (d * p + pattern[i]) % q;
    t = (d * t + text[i]) % q;
  }

  // Find the match
  for (i = 0; i <= n - m; i++) {
    if (p == t) {
      for (j = 0; j < m; j++) {
        if (text[i + j] != pattern[j])
          break;
      }

      if (j == m)
        printf(""Pattern is found at position:  %d \n"", i + 1);
    }

    if (i < n - m) {
      t = (d * (t - text[i] * h) + text[i + m]) % q;

      if (t < 0)
        t = (t + q);
    }
  }
}

int main() {
  char text[] = ""ABCCDDAEFG"";
  char pattern[] = ""CDD"";
  int q = 13;
  rabinKarp(pattern, text, q);
}"
Rabin-Karp Algorithm,"Rabin-Karp algorithm is an algorithm used for searching/matching patterns in the text using a hash function. Unlike Naive string matching algorithm, it does not travel through every character in the initial phase rather it filters the characters that do not match and then performs the comparison. A hash function is a tool to map a larger input value to a smaller output value. This output value is called the hash value. A sequence of characters is taken and checked for the possibility of the presence of the required string. If the possibility is found then, character matching is performed. Let us understand the algorithm with the following steps: Let the text be:
		
			Text
		
		
		And the string to be searched in the above text be:
		
			Pattern Let us assign a numerical value(v)/weight for the characters we will be using in the problem. Here, we have taken first ten alphabets only (i.e. A to J).
		
			Text Weights n be the length of the pattern and m be the length of the text. Here, m = 10 and n = 3.
		Let d be the number of characters in the input set. Here, we have taken input set {A, B, C, ..., J}. So, d = 10. You can assume any suitable value for d. Let us calculate the hash value of the pattern.
		
			Hash value of text In the calculation above, choose a prime number (here, 13) in such a way that we can perform all the calculations with single-precision arithmetic.  The reason for calculating the modulus is given below. Calculate the hash value for the text-window of size m. Compare the hash value of the pattern with the hash value of the text. If they match then, character-matching is performed.
		In the above examples, the hash value of the first window (i.e. t) matches with p so, go for character matching between ABC and CDD. Since they do not match so, go for the next window. We calculate the hash value of the next window by subtracting the first term and adding the next term as shown below. In order to optimize this process, we make use of the previous hash value in the following way. For BCC, t = 12 (≠6). Therefore, go for the next window.
		After a few searches, we will get the match for the window CDA in the text.

		
			Hash value of different windows When the hash value of the pattern matches with the hash value of a window of the text but the window is not the actual pattern then it is called a spurious hit. Spurious hit increases the time complexity of the algorithm. In order to minimize spurious hit, we use modulus. It greatly reduces the spurious hit. The average case and best case complexity of Rabin-Karp algorithm is O(m + n) and the worst case complexity is O(mn). The worst-case complexity occurs when spurious hits occur a number for all the windows. For pattern matching
	For searching string in a bigger text For pattern matching For searching string in a bigger text","// Rabin-Karp algorithm in C++

#include <string.h>

#include <iostream>
using namespace std;

#define d 10

void rabinKarp(char pattern[], char text[], int q) {
  int m = strlen(pattern);
  int n = strlen(text);
  int i, j;
  int p = 0;
  int t = 0;
  int h = 1;

  for (i = 0; i < m - 1; i++)
    h = (h * d) % q;

  // Calculate hash value for pattern and text
  for (i = 0; i < m; i++) {
    p = (d * p + pattern[i]) % q;
    t = (d * t + text[i]) % q;
  }

  // Find the match
  for (i = 0; i <= n - m; i++) {
    if (p == t) {
      for (j = 0; j < m; j++) {
        if (text[i + j] != pattern[j])
          break;
      }

      if (j == m)
        cout << ""Pattern is found at position: "" << i + 1 << endl;
    }

    if (i < n - m) {
      t = (d * (t - text[i] * h) + text[i + m]) % q;

      if (t < 0)
        t = (t + q);
    }
  }
}

int main() {
  char text[] = ""ABCCDDAEFG"";
  char pattern[] = ""CDD"";
  int q = 13;
  rabinKarp(pattern, text, q);
}"
